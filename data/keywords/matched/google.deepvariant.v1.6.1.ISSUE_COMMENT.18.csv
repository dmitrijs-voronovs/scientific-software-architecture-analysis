id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/457:37,safety,depend,depends,37,"Happy to help! For your question, it depends on how low the coverage is. You can see this blog post for how coverage impacts accuracy: https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:37,testability,depend,depends,37,"Happy to help! For your question, it depends on how low the coverage is. You can see this blog post for how coverage impacts accuracy: https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:60,testability,coverag,coverage,60,"Happy to help! For your question, it depends on how low the coverage is. You can see this blog post for how coverage impacts accuracy: https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:108,testability,coverag,coverage,108,"Happy to help! For your question, it depends on how low the coverage is. You can see this blog post for how coverage impacts accuracy: https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:267,testability,coverag,coverage,267,"Happy to help! For your question, it depends on how low the coverage is. You can see this blog post for how coverage impacts accuracy: https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:9,usability,help,help,9,"Happy to help! For your question, it depends on how low the coverage is. You can see this blog post for how coverage impacts accuracy: https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:126,integrability,messag,message,126,"By the way, can you let me know what documentation page you were working from for the original command? I changed the warning message but want to make sure we're looking at the whole user flow up to that point to make sure nothing confusing is still there. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:126,interoperability,messag,message,126,"By the way, can you let me know what documentation page you were working from for the original command? I changed the warning message but want to make sure we're looking at the whole user flow up to that point to make sure nothing confusing is still there. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:37,usability,document,documentation,37,"By the way, can you let me know what documentation page you were working from for the original command? I changed the warning message but want to make sure we're looking at the whole user flow up to that point to make sure nothing confusing is still there. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:95,usability,command,command,95,"By the way, can you let me know what documentation page you were working from for the original command? I changed the warning message but want to make sure we're looking at the whole user flow up to that point to make sure nothing confusing is still there. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:183,usability,user,user,183,"By the way, can you let me know what documentation page you were working from for the original command? I changed the warning message but want to make sure we're looking at the whole user flow up to that point to make sure nothing confusing is still there. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:295,availability,error,error,295,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:583,deployability,depend,dependencies,583,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:613,deployability,instal,installed,613,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:583,integrability,depend,dependencies,583,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:583,modifiability,depend,dependencies,583,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:672,modifiability,extens,extensively,672,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:295,performance,error,error,295,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:177,safety,test,test,177,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:295,safety,error,error,295,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:583,safety,depend,dependencies,583,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:177,testability,test,test,177,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:583,testability,depend,dependencies,583,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:295,usability,error,error,295,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:526,usability,user,user,526,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:696,usability,tool,tool,696,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:737,usability,feedback,feedback,737,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true). I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:52,deployability,depend,depending,52,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:71,deployability,version,version,71,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:159,deployability,version,version,159,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1119,deployability,build,building,1119,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1408,deployability,version,version,1408,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:359,energy efficiency,model,model-case-study,359,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:457,energy efficiency,current,current,457,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1366,energy efficiency,model,model-case-study,1366,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:52,integrability,depend,depending,52,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:71,integrability,version,version,71,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:159,integrability,version,version,159,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1408,integrability,version,version,1408,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:987,interoperability,specif,specify,987,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:52,modifiability,depend,depending,52,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:71,modifiability,version,version,71,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:132,modifiability,Pac,PacBio,132,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:159,modifiability,version,version,159,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:352,modifiability,pac,pacbio-model-case-study,352,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1235,modifiability,Pac,PacBio,1235,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1359,modifiability,pac,pacbio-model-case-study,1359,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1408,modifiability,version,version,1408,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:922,reliability,doe,doesn,922,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:52,safety,depend,depending,52,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:774,safety,avoid,avoid,774,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:359,security,model,model-case-study,359,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1366,security,model,model-case-study,1366,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:52,testability,depend,depending,52,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:439,testability,simpl,simplified,439,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1108,testability,simpl,simpler,1108,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:214,usability,user,users,214,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:439,usability,simpl,simplified,439,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1108,usability,simpl,simpler,1108,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1197,usability,user,users,1197,"Hi @annabeldekker ,. one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:. https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:435,deployability,version,version,435,"@pichuan Thanks! I will look into that. . Just to point out, I was using that flag `parse_sam_aux_fields` because the warning (from v1.1.0) suggested me to do so (and my record was empty, so I was afraid my Bam wasn't being parsed). Also, the `--use_hp_information`, suggests to only take those HP flags in BAM into account, but if I understand correctly from your comment it has the `--parse_sam_aux_fields `incorporated in the newer version? . Thanks for pointing out those other issues as well. I'm sure it will be helpful in further attempts on different kind of data :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:435,integrability,version,version,435,"@pichuan Thanks! I will look into that. . Just to point out, I was using that flag `parse_sam_aux_fields` because the warning (from v1.1.0) suggested me to do so (and my record was empty, so I was afraid my Bam wasn't being parsed). Also, the `--use_hp_information`, suggests to only take those HP flags in BAM into account, but if I understand correctly from your comment it has the `--parse_sam_aux_fields `incorporated in the newer version? . Thanks for pointing out those other issues as well. I'm sure it will be helpful in further attempts on different kind of data :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:435,modifiability,version,version,435,"@pichuan Thanks! I will look into that. . Just to point out, I was using that flag `parse_sam_aux_fields` because the warning (from v1.1.0) suggested me to do so (and my record was empty, so I was afraid my Bam wasn't being parsed). Also, the `--use_hp_information`, suggests to only take those HP flags in BAM into account, but if I understand correctly from your comment it has the `--parse_sam_aux_fields `incorporated in the newer version? . Thanks for pointing out those other issues as well. I'm sure it will be helpful in further attempts on different kind of data :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:334,testability,understand,understand,334,"@pichuan Thanks! I will look into that. . Just to point out, I was using that flag `parse_sam_aux_fields` because the warning (from v1.1.0) suggested me to do so (and my record was empty, so I was afraid my Bam wasn't being parsed). Also, the `--use_hp_information`, suggests to only take those HP flags in BAM into account, but if I understand correctly from your comment it has the `--parse_sam_aux_fields `incorporated in the newer version? . Thanks for pointing out those other issues as well. I'm sure it will be helpful in further attempts on different kind of data :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:518,usability,help,helpful,518,"@pichuan Thanks! I will look into that. . Just to point out, I was using that flag `parse_sam_aux_fields` because the warning (from v1.1.0) suggested me to do so (and my record was empty, so I was afraid my Bam wasn't being parsed). Also, the `--use_hp_information`, suggests to only take those HP flags in BAM into account, but if I understand correctly from your comment it has the `--parse_sam_aux_fields `incorporated in the newer version? . Thanks for pointing out those other issues as well. I'm sure it will be helpful in further attempts on different kind of data :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:823,deployability,stage,stage,823,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1012,deployability,stage,stage,1012,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1165,deployability,stage,stage,1165,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1284,deployability,stage,stage,1284,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1406,deployability,automat,automatically,1406,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1697,deployability,log,logic,1697,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1811,deployability,updat,update,1811,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:281,energy efficiency,model,model,281,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:579,energy efficiency,model,model-case-study,579,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1841,energy efficiency,model,model-case-study,1841,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:274,modifiability,Pac,PacBio,274,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:572,modifiability,pac,pacbio-model-case-study,572,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1209,modifiability,Pac,PacBio,1209,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1834,modifiability,pac,pacbio-model-case-study,1834,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1697,safety,log,logic,1697,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1811,safety,updat,update,1811,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:281,security,model,model,281,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:579,security,model,model-case-study,579,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:740,security,control,controls,740,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1697,security,log,logic,1697,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1811,security,updat,update,1811,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1841,security,model,model-case-study,1841,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:301,testability,simpl,simplify,301,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:740,testability,control,controls,740,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1406,testability,automat,automatically,1406,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1566,testability,understand,understand,1566,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1697,testability,log,logic,1697,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:195,usability,help,help,195,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:301,usability,simpl,simplify,301,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:723,usability,command,command,723,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:906,usability,command,command,906,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1375,usability,command,command,1375,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1546,usability,command,command,1546,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:1869,usability,document,document,1869,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own. You can find the logic here: . https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/458:1187,deployability,updat,update,1187,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:112,energy efficiency,model,model,112,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:410,energy efficiency,model,model-case-study,410,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:661,energy efficiency,model,model,661,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:603,interoperability,specif,specifying,603,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:774,interoperability,specif,specifying,774,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:105,modifiability,Pac,PacBio,105,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:403,modifiability,pac,pacbio-model-case-study,403,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1159,modifiability,Pac,PacBio,1159,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1187,safety,updat,update,1187,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1220,safety,avoid,avoid,1220,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:112,security,model,model,112,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:410,security,model,model-case-study,410,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:661,security,model,model,661,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1187,security,updat,update,1187,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:132,testability,simpl,simplify,132,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:132,usability,simpl,simplify,132,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:506,usability,command,command,506,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1203,usability,document,documentation,1203,"Hi @ajsa-nukovic ,. Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:. - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`. - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`. Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:94,energy efficiency,model,model,94,"Hi @pichuan ,. Yeah, it was definitely --add_hp_channel flag that made the difference. PACBIO model works great now. Thanks a lot for your fast reply. :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:87,modifiability,PAC,PACBIO,87,"Hi @pichuan ,. Yeah, it was definitely --add_hp_channel flag that made the difference. PACBIO model works great now. Thanks a lot for your fast reply. :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:94,security,model,model,94,"Hi @pichuan ,. Yeah, it was definitely --add_hp_channel flag that made the difference. PACBIO model works great now. Thanks a lot for your fast reply. :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/459:117,usability,close,closely,117,"Hi, I am interested in benchmarking DV on exome data from a group of snakes. I have a single reference genome from a closely related species, but I would not have the trio data as was done with the Drosophila data. Do you know if anyone has been able to retrain DV using one or two genomes without pedigree info? . Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:37,energy efficiency,model,model,37,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:217,energy efficiency,model,model,217,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:227,performance,perform,performed,227,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:37,security,model,model,37,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:217,security,model,model,217,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:193,usability,mous,mouse,193,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:227,usability,perform,performed,227,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:34,availability,Heal,Health,34,"Thanks Daniel, non sure if Google Health has tried this at all with non-model genomes, but what I am thinking is trying to train my model based on GATK calls from a deeply-sequenced individual and then evaluate using the same strategy on a second deeply-sequenced individual. I don't have triads so I can't use the method GH used for mosquitoes, but does that sound like a reasonable approach to getting DV off the ground without gold-truth data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:72,energy efficiency,model,model,72,"Thanks Daniel, non sure if Google Health has tried this at all with non-model genomes, but what I am thinking is trying to train my model based on GATK calls from a deeply-sequenced individual and then evaluate using the same strategy on a second deeply-sequenced individual. I don't have triads so I can't use the method GH used for mosquitoes, but does that sound like a reasonable approach to getting DV off the ground without gold-truth data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:132,energy efficiency,model,model,132,"Thanks Daniel, non sure if Google Health has tried this at all with non-model genomes, but what I am thinking is trying to train my model based on GATK calls from a deeply-sequenced individual and then evaluate using the same strategy on a second deeply-sequenced individual. I don't have triads so I can't use the method GH used for mosquitoes, but does that sound like a reasonable approach to getting DV off the ground without gold-truth data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:350,reliability,doe,does,350,"Thanks Daniel, non sure if Google Health has tried this at all with non-model genomes, but what I am thinking is trying to train my model based on GATK calls from a deeply-sequenced individual and then evaluate using the same strategy on a second deeply-sequenced individual. I don't have triads so I can't use the method GH used for mosquitoes, but does that sound like a reasonable approach to getting DV off the ground without gold-truth data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:72,security,model,model,72,"Thanks Daniel, non sure if Google Health has tried this at all with non-model genomes, but what I am thinking is trying to train my model based on GATK calls from a deeply-sequenced individual and then evaluate using the same strategy on a second deeply-sequenced individual. I don't have triads so I can't use the method GH used for mosquitoes, but does that sound like a reasonable approach to getting DV off the ground without gold-truth data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:132,security,model,model,132,"Thanks Daniel, non sure if Google Health has tried this at all with non-model genomes, but what I am thinking is trying to train my model based on GATK calls from a deeply-sequenced individual and then evaluate using the same strategy on a second deeply-sequenced individual. I don't have triads so I can't use the method GH used for mosquitoes, but does that sound like a reasonable approach to getting DV off the ground without gold-truth data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:560,deployability,artifact,artifacts,560,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:107,energy efficiency,model,model,107,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:609,energy efficiency,model,model,609,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:768,energy efficiency,model,model,768,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:653,integrability,sub,subset,653,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:381,interoperability,standard,standard,381,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:255,performance,content,content,255,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:228,reliability,doe,does,228,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:107,security,model,model,107,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:609,security,model,model,609,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:768,security,model,model,768,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:142,usability,indicat,indication,142,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:600,usability,learn,learning,600,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues? Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)? We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:268,energy efficiency,model,model,268,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:468,energy efficiency,model,model,468,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:969,energy efficiency,model,model,969,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:1121,energy efficiency,model,model,1121,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:958,modifiability,exten,extend,958,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:563,performance,time,times,563,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:617,safety,test,test,617,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:701,safety,test,test,701,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:94,security,team,team,94,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:268,security,model,model,268,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:468,security,model,model,468,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:969,security,model,model,969,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:1121,security,model,model,1121,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:204,testability,plan,plants,204,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:617,testability,test,test,617,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:701,testability,test,test,701,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. . I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:95,interoperability,share,share,95,"Hi @kyleaoconnell22 , I'm closing this issue now. If you have any more questions or results to share, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/460:305,availability,sli,slightly,305,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:583,deployability,log,logic,583,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:751,deployability,log,logic,751,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:825,deployability,fail,failing,825,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:631,performance,network,network,631,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:869,performance,time,time,869,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:305,reliability,sli,slightly,305,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:825,reliability,fail,failing,825,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:976,reliability,doe,does,976,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:583,safety,log,logic,583,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:751,safety,log,logic,751,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:583,security,log,logic,583,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:631,security,network,network,631,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:751,security,log,logic,751,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:583,testability,log,logic,583,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:751,testability,log,logic,751,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:1041,testability,coverag,coverages,1041,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:888,usability,support,supporting,888,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:204,performance,time,time,204,"Hi Andrew, this answers all my questions! Since deepvariant is trained on bwa-mem2, I guess it's makes sense to stick to it. And it's good to know marking duplicates isn't necessary - this will save some time & electricity :) . And big thanks to all people involved in deepvariant - it's truly groundbreaking tool! . All the best, . Leszek",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/460:309,usability,tool,tool,309,"Hi Andrew, this answers all my questions! Since deepvariant is trained on bwa-mem2, I guess it's makes sense to stick to it. And it's good to know marking duplicates isn't necessary - this will save some time & electricity :) . And big thanks to all people involved in deepvariant - it's truly groundbreaking tool! . All the best, . Leszek",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460
https://github.com/google/deepvariant/issues/462:63,interoperability,share,share,63,Hello @mvelinder . Were you able to resolve the issue? Can you share a bit more context on what went wrong? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:80,testability,context,context,80,Hello @mvelinder . Were you able to resolve the issue? Can you share a bit more context on what went wrong? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:71,usability,guidanc,guidance,71,"I was not able to get this working @pichuan , if you could provide any guidance that'd still be helpful for me. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:96,usability,help,helpful,96,"I was not able to get this working @pichuan , if you could provide any guidance that'd still be helpful for me. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:243,availability,error,error,243,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:249,integrability,messag,message,249,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:249,interoperability,messag,message,249,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:219,performance,memor,memory,219,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:243,performance,error,error,243,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:713,performance,time,time,713,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:243,safety,error,error,243,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:757,safety,input,input,757,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:905,safety,input,input,905,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:954,safety,input,input,954,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:1206,safety,test,test,1206,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:1312,safety,compl,complete,1312,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:1312,security,compl,complete,1312,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:927,testability,unit,unittest,927,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:1206,testability,test,test,1206,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:219,usability,memor,memory,219,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:243,usability,error,error,243,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:527,usability,confirm,confirmed,527,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:757,usability,input,input,757,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:905,usability,input,input,905,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:954,usability,input,input,954,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. . DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:. ```. time sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=1. ```. In my test run on a **t2.medium** instance, this took: . ```. real0m23.790s. user0m0.032s. sys0m0.028s. ```. to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:117,interoperability,format,format,117,With a brand new t2.medium instance per the quickstart guide and your command I get:. ```. docker: invalid reference format. See 'docker run --help'. real	0m0.046s. user	0m0.023s. sys	0m0.028s. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:55,usability,guid,guide,55,With a brand new t2.medium instance per the quickstart guide and your command I get:. ```. docker: invalid reference format. See 'docker run --help'. real	0m0.046s. user	0m0.023s. sys	0m0.028s. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:70,usability,command,command,70,With a brand new t2.medium instance per the quickstart guide and your command I get:. ```. docker: invalid reference format. See 'docker run --help'. real	0m0.046s. user	0m0.023s. sys	0m0.028s. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:143,usability,help,help,143,With a brand new t2.medium instance per the quickstart guide and your command I get:. ```. docker: invalid reference format. See 'docker run --help'. real	0m0.046s. user	0m0.023s. sys	0m0.028s. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:165,usability,user,user,165,With a brand new t2.medium instance per the quickstart guide and your command I get:. ```. docker: invalid reference format. See 'docker run --help'. real	0m0.046s. user	0m0.023s. sys	0m0.028s. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:138,availability,echo,echo,138,"@mvelinder I think sometimes this could happen when the environment variables were not manually set right. For example, can you do:. ```. echo $BIN_VERSION. ```. (and all the others that you might have in your command) to make sure they're all set as expected?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:68,modifiability,variab,variables,68,"@mvelinder I think sometimes this could happen when the environment variables were not manually set right. For example, can you do:. ```. echo $BIN_VERSION. ```. (and all the others that you might have in your command) to make sure they're all set as expected?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:210,usability,command,command,210,"@mvelinder I think sometimes this could happen when the environment variables were not manually set right. For example, can you do:. ```. echo $BIN_VERSION. ```. (and all the others that you might have in your command) to make sure they're all set as expected?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:251,performance,time,time,251,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:110,safety,test,test,110,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:201,safety,test,testdata,201,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:296,safety,input,input,296,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:449,safety,input,input,449,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:499,safety,input,input,499,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:110,testability,test,test,110,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:201,testability,test,testdata,201,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:471,testability,unit,unittest,471,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:296,usability,input,input,296,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:449,usability,input,input,449,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:499,usability,input,input,499,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```. BIN_VERSION=""1.1.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \. 	-v ""${INPUT_DIR}"":""/input"" \. 	-v ""${OUTPUT_DIR}"":""/output"" \. 	google/deepvariant:""${BIN_VERSION}"" \. 	/opt/deepvariant/bin/run_deepvariant \. 	--model_type=WGS \. 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \. 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=/output/output.vcf.gz \. 	--output_gvcf=/output/output.g.vcf.gz \. 	--intermediate_results_dir /output/intermediate_results_dir \. 	--num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:84,deployability,resourc,resources,84,"Glad that it's working @mvelinder . For larger data, you'll likely want to use more resources. You can see this doc for more examples for different machine types and costs (estimated on GCP).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:84,energy efficiency,resourc,resources,84,"Glad that it's working @mvelinder . For larger data, you'll likely want to use more resources. You can see this doc for more examples for different machine types and costs (estimated on GCP).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:173,energy efficiency,estimat,estimated,173,"Glad that it's working @mvelinder . For larger data, you'll likely want to use more resources. You can see this doc for more examples for different machine types and costs (estimated on GCP).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:84,performance,resourc,resources,84,"Glad that it's working @mvelinder . For larger data, you'll likely want to use more resources. You can see this doc for more examples for different machine types and costs (estimated on GCP).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:84,safety,resourc,resources,84,"Glad that it's working @mvelinder . For larger data, you'll likely want to use more resources. You can see this doc for more examples for different machine types and costs (estimated on GCP).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:84,testability,resourc,resources,84,"Glad that it's working @mvelinder . For larger data, you'll likely want to use more resources. You can see this doc for more examples for different machine types and costs (estimated on GCP).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/463:331,availability,slo,slowest,331,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:357,availability,slo,slow,357,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:466,availability,slo,slow,466,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:642,availability,slo,slow,642,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:702,availability,avail,available,702,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1169,availability,cluster,cluster,1169,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1571,availability,slo,slow,1571,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1169,deployability,cluster,cluster,1169,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1562,deployability,stage,stage,1562,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1397,energy efficiency,core,cores,1397,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1592,energy efficiency,CPU,CPUs,1592,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:498,integrability,pub,publicly,498,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:693,integrability,pub,publicly,693,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:805,integrability,pub,publicly,805,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:814,interoperability,share,shared,814,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:899,interoperability,Specif,Specifically,899,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:159,performance,time,time,159,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1592,performance,CPU,CPUs,1592,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:331,reliability,slo,slowest,331,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:357,reliability,slo,slow,357,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:466,reliability,slo,slow,466,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:642,reliability,slo,slow,642,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:702,reliability,availab,available,702,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:736,reliability,diagno,diagnose,736,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1571,reliability,slo,slow,1571,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:123,safety,input,inputs,123,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:480,safety,input,input,480,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:702,safety,avail,available,702,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1101,safety,test,testdata,1101,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:592,security,ident,identify,592,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:702,security,availab,available,702,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:736,testability,diagno,diagnose,736,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1101,testability,test,testdata,1101,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:123,usability,input,inputs,123,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:389,usability,learn,learn,389,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:404,usability,user,users,404,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:480,usability,input,input,480,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1267,usability,help,help,1267,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1290,usability,support,support,1290,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1579,usability,user,users,1579,"Hi @Asppagh ,. It can be many different reasons. . Your machine setup definitely could be one of the factors. And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:. https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:. Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants? (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:69,availability,avail,available,69,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:740,availability,avail,available,740,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:871,availability,slo,slower,871,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:314,deployability,log,log,314,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:352,deployability,configurat,configuration,352,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:424,deployability,configurat,configuration,424,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:450,energy efficiency,core,core,450,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:813,energy efficiency,core,cores,813,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:60,integrability,pub,publicly,60,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:352,integrability,configur,configuration,352,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:424,integrability,configur,configuration,424,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:352,modifiability,configur,configuration,352,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:424,modifiability,configur,configuration,424,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:472,performance,memor,memory,472,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:601,performance,time,time,601,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:649,performance,time,times,649,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:721,performance,memor,memory,721,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:931,performance,memor,memory,931,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:69,reliability,availab,available,69,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:699,reliability,doe,does,699,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:740,reliability,availab,available,740,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:871,reliability,slo,slower,871,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:69,safety,avail,available,69,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:314,safety,log,log,314,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:740,safety,avail,available,740,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:69,security,availab,available,69,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:314,security,log,log,314,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:352,security,configur,configuration,352,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:424,security,configur,configuration,424,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:740,security,availab,available,740,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:314,testability,log,log,314,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:472,usability,memor,memory,472,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:721,usability,memor,memory,721,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:931,usability,memor,memory,931,"Hi @pichuan,. Thanks for the response. . As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). . https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100. Surprisingly singularity does not use the full memory, 64 GB made available to it. I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one. May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1189,availability,slo,slower,1189,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1314,availability,slo,slower,1314,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:707,deployability,build,building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-,707,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:329,energy efficiency,core,cores,329,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:408,energy efficiency,core,core,408,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:604,energy efficiency,CPU,CPU,604,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:608,energy efficiency,optim,optimization,608,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:698,energy efficiency,power,power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-,698,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:780,energy efficiency,optim,optimizations,780,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:823,energy efficiency,CPU,CPU,823,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:861,energy efficiency,cpu,cpuinfo,861,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:735,interoperability,platform,platform-how-deepVariant-uses-intels-avx-,735,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:604,performance,CPU,CPU,604,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:608,performance,optimiz,optimization,608,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:780,performance,optimiz,optimizations,780,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:823,performance,CPU,CPU,823,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:861,performance,cpu,cpuinfo,861,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:959,performance,parallel,parallelized,959,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:367,reliability,doe,does,367,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1189,reliability,slo,slower,1189,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1314,reliability,slo,slower,1314,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:432,safety,test,test,432,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1157,safety,input,input,1157,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:432,testability,test,test,432,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1157,usability,input,input,1157,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step. The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/. Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:. num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:29,availability,cluster,cluster,29,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1143,availability,sli,slightly,1143,"with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1322,availability,sli,slightly,1322,"g	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:29,deployability,cluster,cluster,29,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:460,deployability,api,apicid,460,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:481,deployability,api,apicid,481,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:599,deployability,api,apic,599,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:991,deployability,manag,management,991,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1117,deployability,log,log,1117,"hat I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1672,deployability,log,logs,1672,"ll nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1688,deployability,log,logs,1688,"constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1819,deployability,log,logs,1819,"es xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2288,deployability,log,logs,2288," for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:135,energy efficiency,core,cores,135,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:162,energy efficiency,cpu,cpu,162,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:183,energy efficiency,cpu,cpuinfo,183,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:233,energy efficiency,cpu,cpu,233,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:249,energy efficiency,model,model,249,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:262,energy efficiency,model,model,262,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:304,energy efficiency,CPU,CPU,304,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:356,energy efficiency,cpu,cpu,356,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:431,energy efficiency,core,core,431,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:445,energy efficiency,cpu,cpu,445,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:449,energy efficiency,core,cores,449,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:526,energy efficiency,cpu,cpuid,526,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:985,energy efficiency,power,power,985,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:991,energy efficiency,manag,management,991,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1286,energy efficiency,core,core,1286,"(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1299,energy efficiency,cpu,cpu,1299,"16 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1360,energy efficiency,cpu,cpu,1360,"z		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1364,energy efficiency,core,cores,1364,"2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:460,integrability,api,apicid,460,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:481,integrability,api,apicid,481,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:599,integrability,api,apic,599,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2031,integrability,buffer,buffer,2031,"all_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:460,interoperability,api,apicid,460,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:481,interoperability,api,apicid,481,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:599,interoperability,api,apic,599,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1264,interoperability,specif,specify,1264,"name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1348,interoperability,specif,specify,1348,"4d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1528,modifiability,interm,intermediate,1528,"vel	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1863,modifiability,Interm,Intermediate,1863,"at. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12',",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3152,modifiability,deco,decode,3152,"nput/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:648] 100 candidates (107 ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:162,performance,cpu,cpu,162,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:183,performance,cpu,cpuinfo,183,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:233,performance,cpu,cpu,233,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:304,performance,CPU,CPU,304,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:356,performance,cpu,cpu,356,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:377,performance,cach,cache,377,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:445,performance,cpu,cpu,445,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:526,performance,cpu,cpuid,526,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1299,performance,cpu,cpu,1299,"16 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1360,performance,cpu,cpu,1360,"z		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1988,performance,time,time,1988," management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2003,performance,parallel,parallel,2003,"t I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3929,performance,Overhead,Overhead,3929,"'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:648] 100 candidates (107 examples) [30.13s elapsed]. I0622 13:07:35.695144 47468847029248 make_examples.py:648] 202 candidates (222 examples) [6.10s elapsed]. I0622 13:07:43.305665 47468847029248 make_examples.py:648] 300 candidates (325 examples) [7.61s elapsed]. I0622 13:07:47.552723 47468847029248 make_examples.py:648] 411 candidates (450 examples) [4.25s elapsed]. I0622 13:07:50.840119 47468847029248 make_examples.py:648] 505 candidates (550 examples) [3.29s elapsed]. I0622 13:07:56.351229 47468847029248 make_examples.py:648] 607 candidates (667 examples) [5.51s elapsed]. I0622 13:08:05.931827 47468847029248 make_examples.py:648] 701 candidates (761 examples) [9.58s elapsed]. I0622 13:08:13.084163 47468847029248 make_examples.py:648] 801 candidates (865 examples) [7.15s elapsed]. I0622 13:0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:680,reliability,rdt,rdtscp,680,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1143,reliability,sli,slightly,1143,"with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1322,reliability,sli,slightly,1322,"g	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:991,safety,manag,management,991,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1117,safety,log,log,1117,"hat I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1406,safety,input,input,1406,"id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1672,safety,log,logs,1672,"ll nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1688,safety,log,logs,1688,"constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1819,safety,log,logs,1819,"es xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2154,safety,input,input,2154,"rent setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2288,safety,log,logs,2288," for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2441,safety,input,input,2441,"622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2562,safety,input,inputs,2562,"mediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2640,safety,input,input,2640,"05] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3127,safety,input,input,3127,"decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:64",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3393,safety,input,input,3393,"68847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:648] 100 candidates (107 examples) [30.13s elapsed]. I0622 13:07:35.695144 47468847029248 make_examples.py:648] 202 candidates (222 examples) [6.10s elapsed]. I0622 13:07:43.305665 47468847029248 make_examples.py:648] 300 candidates (325 examples) [7.61s elapsed]. I06",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3515,safety,input,input,3515,"68847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:648] 100 candidates (107 examples) [30.13s elapsed]. I0622 13:07:35.695144 47468847029248 make_examples.py:648] 202 candidates (222 examples) [6.10s elapsed]. I0622 13:07:43.305665 47468847029248 make_examples.py:648] 300 candidates (325 examples) [7.61s elapsed]. I0622 13:07:47.552723 47468847029248 make_examples.py:648] 411 candidates (450 examples) [4.25s elapsed]. I0622 13:07:50.8401",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3952,safety,input,inputs,3952,"22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:648] 100 candidates (107 examples) [30.13s elapsed]. I0622 13:07:35.695144 47468847029248 make_examples.py:648] 202 candidates (222 examples) [6.10s elapsed]. I0622 13:07:43.305665 47468847029248 make_examples.py:648] 300 candidates (325 examples) [7.61s elapsed]. I0622 13:07:47.552723 47468847029248 make_examples.py:648] 411 candidates (450 examples) [4.25s elapsed]. I0622 13:07:50.840119 47468847029248 make_examples.py:648] 505 candidates (550 examples) [3.29s elapsed]. I0622 13:07:56.351229 47468847029248 make_examples.py:648] 607 candidates (667 examples) [5.51s elapsed]. I0622 13:08:05.931827 47468847029248 make_examples.py:648] 701 candidates (761 examples) [9.58s elapsed]. I0622 13:08:13.084163 47468847029248 make_examples.py:648] 801 candidates (865 examples) [7.15s elapsed]. I0622 13:08:28.955715 4746884702",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:249,security,model,model,249,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:262,security,model,model,262,"Thanks @pichuan,. I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1117,security,log,log,1117,"hat I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1672,security,log,logs,1672,"ll nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1688,security,log,logs,1688,"constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1819,security,log,logs,1819,"es xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2288,security,log,logs,2288," for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1117,testability,log,log,1117,"hat I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1672,testability,log,logs,1672,"ll nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1688,testability,log,logs,1688,"constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1819,testability,log,logs,1819,"es xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2288,testability,log,logs,2288," for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1087,usability,clear,clear,1087," example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo. processor	: 0. vendor_id	: GenuineIntel. cpu family	: 6. model		: 85. model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz. stepping	: 4. microcode	: 0x200004d. cpu MHz		: 2095.078. cache size	: 16896 KB. physical id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1406,usability,input,input,1406,"id	: 0. siblings	: 1. core id		: 0. cpu cores	: 1. apicid		: 0. initial apicid	: 0. fpu		: yes. fpu_exception	: yes. cpuid level	: 13. wp		: yes. flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat. bogomips	: 4190.15. clflush size	: 64. cache_alignment	: 64. address sizes	: 40 bits physical, 48 bits virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1971,usability,command,command,1971," virtual. power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY',",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2154,usability,input,input,2154,"rent setting and different example but it shows what I said in my previous comment. In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:. input file S-001701867.markdup.bam. I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2441,usability,input,input,2441,"622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2562,usability,input,inputs,2562,"mediate_results_dir. I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2640,usability,input,input,2640,"05] Creating a directory for logs in /output/logs. I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3127,usability,input,input,3127,"decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:64",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3393,usability,input,input,3393,"68847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:648] 100 candidates (107 examples) [30.13s elapsed]. I0622 13:07:35.695144 47468847029248 make_examples.py:648] 202 candidates (222 examples) [6.10s elapsed]. I0622 13:07:43.305665 47468847029248 make_examples.py:648] 300 candidates (325 examples) [7.61s elapsed]. I06",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3515,usability,input,input,3515,"68847029248 make_examples.py:648] Preparing inputs. I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:648] 100 candidates (107 examples) [30.13s elapsed]. I0622 13:07:35.695144 47468847029248 make_examples.py:648] 202 candidates (222 examples) [6.10s elapsed]. I0622 13:07:43.305665 47468847029248 make_examples.py:648] 300 candidates (325 examples) [7.61s elapsed]. I0622 13:07:47.552723 47468847029248 make_examples.py:648] 411 candidates (450 examples) [4.25s elapsed]. I0622 13:07:50.8401",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:3952,usability,input,inputs,3952,"22', 'chrX', 'chrY', 'chrM']. I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0622 13:06:54.980819 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.028589 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader. I0622 13:06:55.547895 47468847029248 make_examples.py:648] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0622 13:06:55.548219 47468847029248 make_examples.py:648] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0622 13:06:56.858054 47468847029248 make_examples.py:648] Overhead for preparing inputs: 17 seconds. I0622 13:06:59.468765 47468847029248 make_examples.py:648] 0 candidates (0 examples) [2.61s elapsed]. I0622 13:07:29.596613 47468847029248 make_examples.py:648] 100 candidates (107 examples) [30.13s elapsed]. I0622 13:07:35.695144 47468847029248 make_examples.py:648] 202 candidates (222 examples) [6.10s elapsed]. I0622 13:07:43.305665 47468847029248 make_examples.py:648] 300 candidates (325 examples) [7.61s elapsed]. I0622 13:07:47.552723 47468847029248 make_examples.py:648] 411 candidates (450 examples) [4.25s elapsed]. I0622 13:07:50.840119 47468847029248 make_examples.py:648] 505 candidates (550 examples) [3.29s elapsed]. I0622 13:07:56.351229 47468847029248 make_examples.py:648] 607 candidates (667 examples) [5.51s elapsed]. I0622 13:08:05.931827 47468847029248 make_examples.py:648] 701 candidates (761 examples) [9.58s elapsed]. I0622 13:08:13.084163 47468847029248 make_examples.py:648] 801 candidates (865 examples) [7.15s elapsed]. I0622 13:08:28.955715 4746884702",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:937,availability,down,download,937,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4178,availability,Uptim,Uptime,4178,"6229 examples) [14.57s elapsed]. I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `htop` while `make_examples` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4576,availability,operat,operations,4576,"|||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4630,availability,operat,operations,4630,"|||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4869,availability,servic,service,4869,"|||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4877,availability,servic,service,4877,"|||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4897,availability,servic,service,4897,"|100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5058,availability,servic,service,5058,"||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5066,availability,servic,service,5066,"||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5571,availability,Restor,Restoring,5571,"operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:469,deployability,instal,installed,469,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:735,deployability,version,version,735,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:794,deployability,version,version,794,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:815,deployability,version,version,815,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1664,deployability,log,log,1664,"'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1697,deployability,log,log,1697,"cng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622 21:24:43.683619 140528910345984 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2437,deployability,log,log,2437,_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622 21:24:43.683619 140528910345984 make_examples.py:648] Task 0/8: 5700 candidates (6127 examples) [24.04s elapsed]. I0622 21:24:44.784906 139897026688768 make_examples.py:648] Task 4/8: 6002 candidates (6422 examples) [6.72s elapsed]. I0622 21:24:46.344424 139897026688768 make_examples.py:648] Task 4/8: Found 6004 candidate variants. I0622 21:24:46.344626 139897026688768 make_examples.py:648] Task 4/8: Created 6424 examples. I0622 21:24:58.252706 140528910345984 make_examples.py:648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]. I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `hto,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4869,deployability,servic,service,4869,"|||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4877,deployability,servic,service,4877,"|||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4897,deployability,servic,service,4897,"|100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5058,deployability,servic,service,5058,"||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5066,deployability,servic,service,5066,"||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5123,deployability,Version,Version,5123,"unning. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 14",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5392,deployability,log,log,5392,"/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5492,deployability,log,log,5492,"ork Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6525,deployability,observ,observe,6525,":17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6689,deployability,log,log,6689,"sed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:50,energy efficiency,cloud,cloud,50,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:237,energy efficiency,cloud,cloud-platform,237,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:319,energy efficiency,cloud,cloud,319,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:416,energy efficiency,cpu,cpu-platform,416,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4147,energy efficiency,Load,Load,4147,"648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]. I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `htop` while `make_examples` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4390,energy efficiency,core,core,4390," 6254 examples. ```. Here is a snapshot of `htop` while `make_examples` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4456,energy efficiency,optim,optimized,4456,"s` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4535,energy efficiency,CPU,CPU,4535,"||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.0090",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4739,energy efficiency,core,core,4739,"||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4785,energy efficiency,CPU,CPU,4785,"||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4789,energy efficiency,Frequenc,Frequency,4789,"|||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5173,energy efficiency,core,core,5173," . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5374,energy efficiency,optim,optimization,5374,"ensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.21",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5602,energy efficiency,model,models,5602," To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5613,energy efficiency,model,model,5613,"them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6403,energy efficiency,CPU,CPU,6403,"variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6484,energy efficiency,CPU,CPUs,6484,"ants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6544,energy efficiency,CPU,CPUs,6544,"916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 1405",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:735,integrability,version,version,735,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:794,integrability,version,version,794,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:815,integrability,version,version,815,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4869,integrability,servic,service,4869,"|||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4877,integrability,servic,service,4877,"|||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4897,integrability,servic,service,4897,"|100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5058,integrability,servic,service,5058,"||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5066,integrability,servic,service,5066,"||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5123,integrability,Version,Version,5123,"unning. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 14",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5711,integrability,batch,batches,5711,"07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5834,integrability,batch,batches,5834,"675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""referen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5954,integrability,batch,batches,5954,"t guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6074,integrability,batch,batches,6074,"76] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6194,integrability,batch,batches,6194,"ss_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:7703,integrability,Transform,Transforming,7703,"a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_variants.py:1122] Transforming call_variants_output to variants. I0622 21:30:42.625963 140597045393152 postprocess_variants.py:1149] Merging and writing variants to VCF and gVCF. I0622 21:30:42.627370 140597045393152 genomics_writer.py:176] Writing output/HG003.output.vcf.gz with NativeVcfWriter. I0622 21:30:42.628537 140597045393152 genomics_writer.py:176] Writing output/HG003.output.g.vcf.gz with NativeVcfWriter. I0622 21:31:27.396288 140597045393152 postprocess_variants.py:1169] Finished writing VCF and gVCF in 0.7461712757746378 minutes. I0622 21:31:27.415654 140597045393152 genomics_reader.py:223] Reading output/HG003.output.vcf.gz with NativeVcfReader. real 0m53.037s. user 0m47.191s. sys 0m6.700s. ```. @Asppagh Can you check whether this is inline with what you're seeing? it's expected for make_examples step to run for about 30+min on a 8core instance. If anything above differs significantly from what you're seeing. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:243,interoperability,platform,platform,243,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:420,interoperability,platform,platform,420,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4395,interoperability,platform,platform,4395,"xamples. ```. Here is a snapshot of `htop` while `make_examples` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4744,interoperability,platform,platform,4744,"|||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4931,interoperability,platform,platform,4931,"|||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:7703,interoperability,Transform,Transforming,7703,"a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_variants.py:1122] Transforming call_variants_output to variants. I0622 21:30:42.625963 140597045393152 postprocess_variants.py:1149] Merging and writing variants to VCF and gVCF. I0622 21:30:42.627370 140597045393152 genomics_writer.py:176] Writing output/HG003.output.vcf.gz with NativeVcfWriter. I0622 21:30:42.628537 140597045393152 genomics_writer.py:176] Writing output/HG003.output.g.vcf.gz with NativeVcfWriter. I0622 21:31:27.396288 140597045393152 postprocess_variants.py:1169] Finished writing VCF and gVCF in 0.7461712757746378 minutes. I0622 21:31:27.415654 140597045393152 genomics_reader.py:223] Reading output/HG003.output.vcf.gz with NativeVcfReader. real 0m53.037s. user 0m47.191s. sys 0m6.700s. ```. @Asppagh Can you check whether this is inline with what you're seeing? it's expected for make_examples step to run for about 30+min on a 8core instance. If anything above differs significantly from what you're seeing. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:735,modifiability,version,version,735,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:794,modifiability,version,version,794,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:815,modifiability,version,version,815,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4869,modifiability,servic,service,4869,"|||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4877,modifiability,servic,service,4877,"|||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4897,modifiability,servic,service,4897,"|100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5058,modifiability,servic,service,5058,"||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5066,modifiability,servic,service,5066,"||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5123,modifiability,Version,Version,5123,"unning. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 14",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5581,modifiability,paramet,parameters,5581,": AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that muc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:392,performance,disk,disk-size,392,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:416,performance,cpu,cpu-platform,416,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4147,performance,Load,Load,4147,"648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]. I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `htop` while `make_examples` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4456,performance,optimiz,optimized,4456,"s` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4490,performance,Network,Network,4490,"|||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4535,performance,CPU,CPU,4535,"||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.0090",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4555,performance,perform,performance-critical,4555,"|||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4785,performance,CPU,CPU,4785,"||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5273,performance,Tune,Tune,5273," ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5322,performance,perform,performance,5322,"iants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5374,performance,optimiz,optimization,5374,"ensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.21",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5711,performance,batch,batches,5711,"07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5834,performance,batch,batches,5834,"675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""referen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5954,performance,batch,batches,5954,"t guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6074,performance,batch,batches,6074,"76] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6194,performance,batch,batches,6194,"ss_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6403,performance,CPU,CPU,6403,"variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6484,performance,CPU,CPUs,6484,"ants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6509,performance,time,time,6509,". ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6544,performance,CPU,CPUs,6544,"916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 1405",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6777,performance,time,time,6777,"8 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_variants.py:1122] Transforming call_variants_output to variants. I0622 21:30:42.625963 1405970",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4178,reliability,Uptim,Uptime,4178,"6229 examples) [14.57s elapsed]. I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `htop` while `make_examples` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4951,reliability,doe,does,4951,"|||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5571,reliability,Restor,Restoring,5571,"operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:135,safety,test,test,135,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:188,safety,test,test-speed,188,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:766,safety,test,test-speed,766,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1374,safety,input,input,1374,"2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1430,safety,input,input,1430,"el Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1664,safety,log,log,1664,"'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1697,safety,log,log,1697,"cng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622 21:24:43.683619 140528910345984 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2437,safety,log,log,2437,_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622 21:24:43.683619 140528910345984 make_examples.py:648] Task 0/8: 5700 candidates (6127 examples) [24.04s elapsed]. I0622 21:24:44.784906 139897026688768 make_examples.py:648] Task 4/8: 6002 candidates (6422 examples) [6.72s elapsed]. I0622 21:24:46.344424 139897026688768 make_examples.py:648] Task 4/8: Found 6004 candidate variants. I0622 21:24:46.344626 139897026688768 make_examples.py:648] Task 4/8: Created 6424 examples. I0622 21:24:58.252706 140528910345984 make_examples.py:648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]. I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `hto,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5392,safety,log,log,5392,"/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5492,safety,log,log,5492,"ork Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6689,safety,log,log,6689,"sed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1664,security,log,log,1664,"'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1697,security,log,log,1697,"cng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622 21:24:43.683619 140528910345984 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2437,security,log,log,2437,_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622 21:24:43.683619 140528910345984 make_examples.py:648] Task 0/8: 5700 candidates (6127 examples) [24.04s elapsed]. I0622 21:24:44.784906 139897026688768 make_examples.py:648] Task 4/8: 6002 candidates (6422 examples) [6.72s elapsed]. I0622 21:24:46.344424 139897026688768 make_examples.py:648] Task 4/8: Found 6004 candidate variants. I0622 21:24:46.344626 139897026688768 make_examples.py:648] Task 4/8: Created 6424 examples. I0622 21:24:58.252706 140528910345984 make_examples.py:648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]. I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `hto,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4490,security,Network,Network,4490,"|||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5392,security,log,log,5392,"/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5492,security,log,log,5492,"ork Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5602,security,model,models,5602," To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5613,security,model,model,5613,"them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6689,security,log,log,6689,"sed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:8582,security,sign,significantly,8582,"a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_variants.py:1122] Transforming call_variants_output to variants. I0622 21:30:42.625963 140597045393152 postprocess_variants.py:1149] Merging and writing variants to VCF and gVCF. I0622 21:30:42.627370 140597045393152 genomics_writer.py:176] Writing output/HG003.output.vcf.gz with NativeVcfWriter. I0622 21:30:42.628537 140597045393152 genomics_writer.py:176] Writing output/HG003.output.g.vcf.gz with NativeVcfWriter. I0622 21:31:27.396288 140597045393152 postprocess_variants.py:1169] Finished writing VCF and gVCF in 0.7461712757746378 minutes. I0622 21:31:27.415654 140597045393152 genomics_reader.py:223] Reading output/HG003.output.vcf.gz with NativeVcfReader. real 0m53.037s. user 0m47.191s. sys 0m6.700s. ```. @Asppagh Can you check whether this is inline with what you're seeing? it's expected for make_examples step to run for about 30+min on a 8core instance. If anything above differs significantly from what you're seeing. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:135,testability,test,test,135,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:188,testability,test,test-speed,188,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:766,testability,test,test-speed,766,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1664,testability,log,log,1664,"'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1697,testability,log,log,1697,"cng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622 21:24:43.683619 140528910345984 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:2437,testability,log,log,2437,_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from the log:. ```. I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples. I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]. I0622 21:24:43.683619 140528910345984 make_examples.py:648] Task 0/8: 5700 candidates (6127 examples) [24.04s elapsed]. I0622 21:24:44.784906 139897026688768 make_examples.py:648] Task 4/8: 6002 candidates (6422 examples) [6.72s elapsed]. I0622 21:24:46.344424 139897026688768 make_examples.py:648] Task 4/8: Found 6004 candidate variants. I0622 21:24:46.344626 139897026688768 make_examples.py:648] Task 4/8: Created 6424 examples. I0622 21:24:58.252706 140528910345984 make_examples.py:648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]. I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `hto,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5392,testability,log,log,5392,"/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5492,testability,log,log,5492,"ork Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6525,testability,observ,observe,6525,":17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6689,testability,log,log,6689,"sed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:182,usability,USER,USER,182,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```. gcloud compute instances create ""${USER}-test-speed"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-highmem-8"" \. --zone ""us-west2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1374,usability,input,input,1374,"2-b"" \. --boot-disk-size 100G \. --min-cpu-platform ""Intel Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:1430,usability,input,input,1430,"el Skylake"". ```. On the machine, I installed Singularity:. ```. curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \. sed -e s'|github.com/sylabs|github.com/hpcng|' | \. bash -x. ```. Here's the version:. ```. pichuan@pichuan-test-speed:~$ singularity --version. singularity version 3.7.0. ```. I followed:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md. to download the data. And then:. ```. mkdir -p output. mkdir -p output/intermediate_results_dir. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type WES \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions input/idt_capture_novogene.grch38.bed \. --output_vcf output/HG003.output.vcf.gz \. --output_gvcf output/HG003.output.g.vcf.gz \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log. ```. I'll paste part of the log of each step so that you can compare. ## make_examples. make_examples speed is roughly:. ```. I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]. I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]. I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]. I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]. I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]. ```. Here are the last few lines from ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4244,usability,user,user,4244,"45984 make_examples.py:648] Task 0/8: Found 5825 candidate variants. I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples. ```. Here is a snapshot of `htop` while `make_examples` is running:. ```. 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:4555,usability,perform,performance-critical,4555,"|||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]. Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running. Swp[ 0K/0K] Load average: 8.09 7.59 4.84 . Uptime: 01:04:14. ```. make_examples took:. ```. real 32m36.929s. user 253m55.294s. sys 1m1.715s. ```. ## call_variants. At the beginning of `call_variants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5322,usability,perform,performance,5322,"iants`, I see:. ```. 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:5346,usability,confirm,confirms,5346,"021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz. 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. ```. which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6353,usability,user,user,6353,"that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:. ```. I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt. I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]. I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermedia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:6760,usability,command,command,6760," 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]. I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]. I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]. I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]. I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s. user 32m1.122s. sys 0m32.211s. ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look). I didn't take a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_variants.py:1122] Transforming call_variants_output to variants. I0622 21:30:42",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:8368,usability,user,user,8368,"a snapshot of `htop` here. ## postprocess_variants. The log here is pretty short, so I'll paste below:. ```. ***** Running the command:*****. ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003. 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760. I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes. I0622 21:30:42.625385 140597045393152 postprocess_variants.py:1122] Transforming call_variants_output to variants. I0622 21:30:42.625963 140597045393152 postprocess_variants.py:1149] Merging and writing variants to VCF and gVCF. I0622 21:30:42.627370 140597045393152 genomics_writer.py:176] Writing output/HG003.output.vcf.gz with NativeVcfWriter. I0622 21:30:42.628537 140597045393152 genomics_writer.py:176] Writing output/HG003.output.g.vcf.gz with NativeVcfWriter. I0622 21:31:27.396288 140597045393152 postprocess_variants.py:1169] Finished writing VCF and gVCF in 0.7461712757746378 minutes. I0622 21:31:27.415654 140597045393152 genomics_reader.py:223] Reading output/HG003.output.vcf.gz with NativeVcfReader. real 0m53.037s. user 0m47.191s. sys 0m6.700s. ```. @Asppagh Can you check whether this is inline with what you're seeing? it's expected for make_examples step to run for about 30+min on a 8core instance. If anything above differs significantly from what you're seeing. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:262,availability,cluster,cluster,262,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:262,deployability,cluster,cluster,262,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:335,energy efficiency,core,cores,335,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:382,energy efficiency,core,core,382,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:538,energy efficiency,cpu,cpu,538,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:217,performance,time,times,217,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:411,performance,time,time,411,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:538,performance,cpu,cpu,538,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:748,performance,time,time,748,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:387,reliability,doe,does,387,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:416,security,sign,significantly,416,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:46,usability,user,user,46,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:110,usability,user,user,110,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:172,usability,user,user,172,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:804,usability,help,help,804,"Hi @pichuan,. Make example . real	44m32.310s. user	292m40.886s. sys	3m37.117s. Call variant. real	15m54.223s. user	63m3.052s. sys	0m45.200s. Post process. real	18m19.080s. user	0m45.307s. sys	0m5.393s. I ran multiple times with different machines that is in our cluster,. This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. . Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:173,performance,time,time,173,"Hi @Asppagh ,. from your numbers, it seems like your make_examples isn't too different from my run. I'm most surprised by your postprocess_variants. It seems like your user time of postprocess_variants is similar to mine (less than a minute), however the real wall time is 18min? Given that I don't know enough about your system setting, any guess why this would be the case? On a high level I wonder if most of the time are somehow blocked in file I/O, but it still seems extreme.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:265,performance,time,time,265,"Hi @Asppagh ,. from your numbers, it seems like your make_examples isn't too different from my run. I'm most surprised by your postprocess_variants. It seems like your user time of postprocess_variants is similar to mine (less than a minute), however the real wall time is 18min? Given that I don't know enough about your system setting, any guess why this would be the case? On a high level I wonder if most of the time are somehow blocked in file I/O, but it still seems extreme.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:416,performance,time,time,416,"Hi @Asppagh ,. from your numbers, it seems like your make_examples isn't too different from my run. I'm most surprised by your postprocess_variants. It seems like your user time of postprocess_variants is similar to mine (less than a minute), however the real wall time is 18min? Given that I don't know enough about your system setting, any guess why this would be the case? On a high level I wonder if most of the time are somehow blocked in file I/O, but it still seems extreme.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:449,performance,I/O,I/O,449,"Hi @Asppagh ,. from your numbers, it seems like your make_examples isn't too different from my run. I'm most surprised by your postprocess_variants. It seems like your user time of postprocess_variants is similar to mine (less than a minute), however the real wall time is 18min? Given that I don't know enough about your system setting, any guess why this would be the case? On a high level I wonder if most of the time are somehow blocked in file I/O, but it still seems extreme.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:168,usability,user,user,168,"Hi @Asppagh ,. from your numbers, it seems like your make_examples isn't too different from my run. I'm most surprised by your postprocess_variants. It seems like your user time of postprocess_variants is similar to mine (less than a minute), however the real wall time is 18min? Given that I don't know enough about your system setting, any guess why this would be the case? On a high level I wonder if most of the time are somehow blocked in file I/O, but it still seems extreme.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:119,usability,help,help,119,"Hi @Asppagh I'm closing this issue given that I haven't had more information from you. If you have more information to help us reproduce this issue, please feel free to reopen!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:301,availability,cluster,cluster,301,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:301,deployability,cluster,cluster,301,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:373,energy efficiency,core,cores,373,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:420,energy efficiency,core,core,420,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:574,energy efficiency,cpu,cpu,574,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:256,performance,time,times,256,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:449,performance,time,time,449,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:574,performance,cpu,cpu,574,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:784,performance,time,time,784,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:425,reliability,doe,does,425,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:454,security,sign,significantly,454,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:74,usability,user,user,74,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:141,usability,user,user,141,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:206,usability,user,user,206,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/463:840,usability,help,help,840,"> Hi [@pichuan](https://github.com/pichuan), Make example real 44m32.310s user 292m40.886s sys 3m37.117s. > . > Call variant real 15m54.223s user 63m3.052s sys 0m45.200s. > . > Post process real 18m19.080s user 0m45.307s sys 0m5.393s. > . > I ran multiple times with different machines that is in our cluster, This is the best runtime that I get is here, but this is on 32 cores machine, however I believe number of the core does not change the run time significantly, plus ram usage also is very low, less than 8 GB. Could you please clarify a bit for me in each step(ram/ cpu) what can expedite the process? in this case, results comes in less than 2 hours, but in our real dataset it takes 1-2 day to get the result for each sample. Do you have any idea how I can get a reasonable time less than 12 hours for example? I. appreciate your help. I am kinda facing the same problem was wondering if you found a solution for this. best!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/464:372,deployability,depend,depend,372,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:606,deployability,depend,depends,606,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:372,integrability,depend,depend,372,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:586,integrability,topic,topic,586,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:606,integrability,depend,depends,606,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:467,interoperability,distribut,distributions,467,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:154,modifiability,Pac,PacBio,154,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:372,modifiability,depend,depend,372,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:606,modifiability,depend,depends,606,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:372,safety,depend,depend,372,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:606,safety,depend,depends,606,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:372,testability,depend,depend,372,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:606,testability,depend,depends,606,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:643,testability,simpl,simple,643,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/464:643,usability,simpl,simple,643,"Hi @maryawood ,. The default values in make_examples.py are our recommendations. We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:. https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238. ```. special_args['vsc_min_fraction_indels'] = 0.12. ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464
https://github.com/google/deepvariant/issues/465:23,availability,error,error,23,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1065,availability,error,error,1065,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:82,deployability,fail,failed,82,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:286,deployability,releas,release,286,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:330,deployability,fail,failed,330,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:374,deployability,log,logs,374,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:834,deployability,log,logs,834,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1071,integrability,messag,messages,1071,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1071,interoperability,messag,messages,1071,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:23,performance,error,error,23,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1028,performance,parallel,parallel,1028,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1065,performance,error,error,1065,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:82,reliability,fail,failed,82,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:330,reliability,fail,failed,330,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:23,safety,error,error,23,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:374,safety,log,logs,374,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:702,safety,input,input,702,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:834,safety,log,logs,834,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1065,safety,error,error,1065,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:374,security,log,logs,374,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:834,security,log,logs,834,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:374,testability,log,logs,374,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:834,testability,log,logs,834,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:23,usability,error,error,23,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:144,usability,stop,stopped,144,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:403,usability,clear,clear,403,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:702,usability,input,input,702,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1065,usability,error,error,1065,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1093,usability,clear,clear,1093,"Hi @Asppagh . From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs? This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:178,availability,error,errors,178,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:77,deployability,log,log,77,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:157,modifiability,paramet,parameter,157,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:178,performance,error,errors,178,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:77,safety,log,log,77,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:178,safety,error,errors,178,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:77,security,log,log,77,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:77,testability,log,log,77,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:30,usability,command,command,30,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:178,usability,error,errors,178,"Hi @pichuan . I just used the command you suggested but I am not getting any log. could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors? Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:43,availability,error,error,43,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:128,availability,error,errors,128,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:81,deployability,stack,stack,81,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:43,performance,error,error,43,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:128,performance,error,errors,128,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:43,safety,error,error,43,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:128,safety,error,errors,128,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:87,testability,trace,trace,87,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:43,usability,error,error,43,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:128,usability,error,errors,128,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before). In this case it seems like we're trying to find out why it didn't. . I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:192,availability,error,error,192,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:267,availability,error,error,267,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4658,availability,operat,operation,4658,"probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). Fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4680,availability,error,error,4680,"[W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6715,availability,error,error,6715,"pp.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6890,availability,error,error,6890,"e_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in che",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11342,availability,operat,operation,11342,"am_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). Fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11364,availability,error,error,11364,"ker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13399,availability,error,error,13399,"pp.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13574,availability,error,error,13574,"e_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13696,availability,failur,failure,13696,""", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17916,availability,operat,operation,17916," truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). Fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17938,availability,error,error,17938,"idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19973,availability,error,error,19973,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:20148,availability,error,error,20148,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:114,deployability,fail,fail,114,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:124,deployability,observ,observe,124,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:136,deployability,log,logs,136,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1494,deployability,log,log,1494,"blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1514,deployability,stack,stack,1514,"riant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4668,deployability,fail,failed,4668," truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5283,deployability,Fail,Failed,5283,"reparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5529,deployability,modul,module,5529,"3:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6487,deployability,Fail,Failed,6487,"iant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6515,deployability,Fail,Failed,6515," 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6992,deployability,fail,failed,6992,"iant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7472,deployability,modul,module,7472," Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11352,deployability,fail,failed,11352,"ead] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11967,deployability,Fail,Failed,11967,"reparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:12213,deployability,modul,module,12213,"3:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13171,deployability,Fail,Failed,13171,"iant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed fr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13199,deployability,Fail,Failed,13199," 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warnin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13696,deployability,fail,failure,13696,""", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13746,deployability,stack,stackt,13746,", examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13997,deployability,Log,Logging,13997,"s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14031,deployability,log,logged,14031,"t/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14099,deployability,log,logging,14099,"ssage + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quicks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14355,deployability,log,logging,14355,"bedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14402,deployability,log,logs,14402," could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14607,deployability,instal,install,14607,"https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.trun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15217,deployability,log,log,15217,"fter flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17926,deployability,fail,failed,17926,"ed. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18541,deployability,Fail,Failed,18541,"reparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18787,deployability,modul,module,18787,"3:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19745,deployability,Fail,Failed,19745,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19773,deployability,Fail,Failed,19773,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:329,energy efficiency,current,currently,329,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:198,integrability,messag,messages,198,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1839,integrability,buffer,buffer,1839,".10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7768,integrability,sub,subprocess,7768,"ef file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7861,integrability,sub,subprocess,7861,"out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7942,integrability,sub,subprocess,7942,"oogle/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The ind",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8027,integrability,buffer,buffer,8027,"mples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14022,integrability,Messag,Messages,14022,"epvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quicks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:198,interoperability,messag,messages,198,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:381,interoperability,share,share,381,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14022,interoperability,Messag,Messages,14022,"epvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quicks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1639,modifiability,interm,intermediate,1639,"a/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1687,modifiability,Interm,Intermediate,1687,"ichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3110,modifiability,deco,decode,3110,"). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 1396544313443",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4876,modifiability,exten,extend,4876,"2878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5529,modifiability,modul,module,5529,"3:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7472,modifiability,modul,module,7472," Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7532,modifiability,pac,packages,7532,"RAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7632,modifiability,pac,packages,7632,"lse (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9842,modifiability,deco,decode,9842,"`. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11560,modifiability,exten,extend,11560,"54144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:12213,modifiability,modul,module,12213,"3:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16334,modifiability,deco,decode,16334," is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18134,modifiability,exten,extend,18134,"start-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18787,modifiability,modul,module,18787,"3:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:192,performance,error,error,192,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:267,performance,error,error,267,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1796,performance,time,time,1796,"3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1811,performance,parallel,parallel,1811,"t-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4272,performance,Overhead,Overhead,4272,"728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4680,performance,error,error,4680,"[W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6715,performance,error,error,6715,"pp.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6890,performance,error,error,6890,"e_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in che",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6973,performance,parallel,parallel,6973,"eepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Comm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7984,performance,time,time,7984,"s job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7999,performance,parallel,parallel,7999,"pt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10956,performance,Overhead,Overhead,10956,"der.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11364,performance,error,error,11364,"ker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13399,performance,error,error,13399,"pp.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13574,performance,error,error,13574,"e_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13696,performance,failur,failure,13696,""", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15270,performance,cach,cached,15270,"```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is def",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17530,performance,Overhead,Overhead,17530,"is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17938,performance,error,error,17938,"idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19973,performance,error,error,19973,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:20148,performance,error,error,20148,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:114,reliability,fail,fail,114,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4668,reliability,fail,failed,4668," truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5283,reliability,Fail,Failed,5283,"reparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6487,reliability,Fail,Failed,6487,"iant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6515,reliability,Fail,Failed,6515," 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6992,reliability,fail,failed,6992,"iant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11352,reliability,fail,failed,11352,"ead] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11967,reliability,Fail,Failed,11967,"reparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13171,reliability,Fail,Failed,13171,"iant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed fr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13199,reliability,Fail,Failed,13199," 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warnin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13696,reliability,fail,failure,13696,""", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17926,reliability,fail,failed,17926,"ed. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18541,reliability,Fail,Failed,18541,"reparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19745,reliability,Fail,Failed,19745,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19773,reliability,Fail,Failed,19773,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:136,safety,log,logs,136,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:157,safety,test,test,157,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:192,safety,error,error,192,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:267,safety,error,error,267,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:390,safety,test,test,390,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:638,safety,test,testdata,638,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:739,safety,test,testdata,739,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:817,safety,test,testdata,817,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:870,safety,test,testdata,870,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:935,safety,test,testdata,935,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:990,safety,test,testdata,990,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1102,safety,input,input,1102," step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1250,safety,input,input,1250," reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1299,safety,input,input,1299,"lly helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1465,safety,compl,completeness,1465,"b.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 1396",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1494,safety,log,log,1494,"blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1904,safety,input,input,1904,"runcated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1952,safety,input,input,1952,"chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2160,safety,input,input,2160,"""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2253,safety,input,input,2253,"ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2376,safety,input,input,2376,"f.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2515,safety,input,inputs,2515,"k trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2567,safety,input,input,2567,"0 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2660,safety,input,input,2660,"/tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2783,safety,input,input,2783,":*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.trunca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3085,safety,input,input,3085,"tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3325,safety,input,input,3325,"39654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3418,safety,input,input,3418,"m with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I062",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3541,safety,input,input,3541," marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.7403",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3654,safety,input,input,3654,": /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3747,safety,input,input,3747,"genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3870,safety,input,input,3870," 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4295,safety,input,inputs,4295," EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4680,safety,error,error,4680,"[W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5340,safety,except,exception,5340,"344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5359,safety,except,exception,5359,"s.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5529,safety,modul,module,5529,"3:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6715,safety,error,error,6715,"pp.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6890,safety,error,error,6890,"e_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in che",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7058,safety,input,input,7058,"ates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7104,safety,input,input,7104,"ssor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7472,safety,modul,module,7472," Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8092,safety,input,input,8092,"--reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8140,safety,input,input,8140,"ted.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8359,safety,input,input,8359,":416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8456,safety,input,input,8456,"ine 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8601,safety,input,input,8601,"sr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.43786",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8652,safety,input,input,8652,"line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common cont",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8809,safety,compl,completeness,8809,"utable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8892,safety,input,input,8892,"_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8985,safety,input,input,8985," seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9108,safety,input,input,9108,"20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9247,safety,input,inputs,9247,"cf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9299,safety,input,input,9299,"' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9392,safety,input,input,9392,"_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9515,safety,input,input,9515,"${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.trunca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9817,safety,input,input,9817,"s, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10057,safety,input,input,10057,"40118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10150,safety,input,input,10150,"m with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10273,safety,input,input,10273," marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 can",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10386,safety,input,input,10386,": /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 byte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10479,safety,input,input,10479,"genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10602,safety,input,input,10602," 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10979,safety,input,inputs,10979,"S_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11364,safety,error,error,11364,"ker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:12024,safety,except,exception,12024,"654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:12043,safety,except,exception,12043,"s.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:12213,safety,modul,module,12213,"3:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13399,safety,error,error,13399,"pp.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13574,safety,error,error,13574,"e_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13997,safety,Log,Logging,13997,"s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14031,safety,log,logged,14031,"t/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14099,safety,log,logging,14099,"ssage + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quicks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14355,safety,log,logging,14355,"bedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14402,safety,log,logs,14402," could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14967,safety,test,testdata,14967,"ag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15031,safety,test,testdata,15031,"ed at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15217,safety,log,log,15217,"fter flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15332,safety,input,input,15332," already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15435,safety,test,testdata,15435,"ity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15571,safety,test,testdata,15571,"s/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15713,safety,input,inputs,15713,"ty pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15765,safety,input,input,15765,". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15868,safety,test,testdata,15868,":""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16004,safety,test,testdata,16004,"\. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16309,safety,input,input,16309,"rker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16549,safety,input,input,16549,"Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16652,safety,test,testdata,16652,"1.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16788,safety,test,testdata,16788,". [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16904,safety,input,input,16904,"ncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17007,safety,test,testdata,17007,"a/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17143,safety,test,testdata,17143,"igs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17553,safety,input,inputs,17553,"s probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17938,safety,error,error,17938,"idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18598,safety,except,exception,18598,"570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18617,safety,except,exception,18617,"s.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18787,safety,modul,module,18787,"3:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19973,safety,error,error,19973,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:20148,safety,error,error,20148,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:136,security,log,logs,136,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1465,security,compl,completeness,1465,"b.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 1396",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1494,security,log,log,1494,"blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5277,security,loss,loss,5277,"d for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(reg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6481,security,loss,loss,6481,"deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. ap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8809,security,compl,completeness,8809,"utable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11961,security,loss,loss,11961,"d for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(reg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13165,security,loss,loss,13165,"deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be cha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13997,security,Log,Logging,13997,"s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14031,security,log,logged,14031,"t/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14099,security,log,logging,14099,"ssage + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quicks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14355,security,log,logging,14355,"bedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14402,security,log,logs,14402," could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15217,security,log,log,15217,"fter flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18535,security,loss,loss,18535,"d for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(reg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19739,security,loss,loss,19739,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:124,testability,observ,observe,124,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:136,testability,log,logs,136,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:157,testability,test,test,157,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:390,testability,test,test,390,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:638,testability,test,testdata,638,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:739,testability,test,testdata,739,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:817,testability,test,testdata,817,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:870,testability,test,testdata,870,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:935,testability,test,testdata,935,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:990,testability,test,testdata,990,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1272,testability,unit,unittest,1272,"e like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1494,testability,log,log,1494,"blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1520,testability,trace,trace,1520,"quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1926,testability,unit,unittest,1926,"tart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4708,testability,Trace,Traceback,4708,"ile is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:5380,testability,Trace,Traceback,5380,"s (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7080,testability,unit,unittest,7080,"runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7374,testability,Trace,Traceback,7374,"y"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried runni",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8114,testability,unit,unittest,8114,"S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8623,testability,unit,unittest,8623,"dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11392,testability,Trace,Traceback,11392,"obably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:12064,testability,Trace,Traceback,12064,"s (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13997,testability,Log,Logging,13997,"s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14031,testability,log,logged,14031,"t/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14099,testability,log,logging,14099,"ssage + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quicks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14355,testability,log,logging,14355,"bedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14402,testability,log,logs,14402," could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14967,testability,test,testdata,14967,"ag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:14992,testability,unit,unittest,14992," Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading qui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15031,testability,test,testdata,15031,"ed at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15217,testability,log,log,15217,"fter flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15435,testability,test,testdata,15435,"ity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15571,testability,test,testdata,15571,"s/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15868,testability,test,testdata,15868,":""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16004,testability,test,testdata,16004,"\. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16652,testability,test,testdata,16652,"1.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16788,testability,test,testdata,16788,". [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17007,testability,test,testdata,17007,"a/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17143,testability,test,testdata,17143,"igs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17966,testability,Trace,Traceback,17966,"lder than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:18638,testability,Trace,Traceback,18638,"s (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:192,usability,error,error,192,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:267,usability,error,error,267,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:306,usability,help,helpful,306,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:355,usability,help,help,355,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1102,usability,input,input,1102," step to fail, and observe the logs. However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1250,usability,input,input,1250," reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1299,usability,input,input,1299,"lly helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`. ```. ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1779,usability,command,command,1779,". ```. head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1904,usability,input,input,1904,"runcated.bam. cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1952,usability,input,input,1952,"chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. ```. I ran:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2160,usability,input,input,2160,"""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2253,usability,input,input,2253,"ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2376,usability,input,input,2376,"f.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1. ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2515,usability,input,inputs,2515,"k trace in make_examples:. ```. I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2567,usability,input,input,2567,"0 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2660,usability,input,input,2660,"/tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2783,usability,input,input,2783,":*****. ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.trunca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3085,usability,input,input,3085,"tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3325,usability,input,input,3325,"39654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3418,usability,input,input,3418,"m with NativeSamReader. I0629 23:08:49.663154 139654431344384 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I062",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3541,usability,input,input,3541," marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.7403",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3654,usability,input,input,3654,": /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.663682 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3747,usability,input,input,3747,"genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:49.664449 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:3870,usability,input,input,3870," 139654431344384 make_examples.py:648] Common contigs are ['chr20']. I0629 23:08:49.855972 139654431344384 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:08:49.856404: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4295,usability,input,inputs,4295," EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.858718 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:4680,usability,error,error,4680,"[W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz. I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz. I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]. I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6715,usability,error,error,6715,"pp.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:6890,usability,error,error,6890,"e_examples_runner(options). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in che",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7058,usability,input,input,7058,"ates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7104,usability,input,input,7104,"ssor.process(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7276,usability,user,user,7276,"gion). File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7790,usability,command,command,7790,"/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:7973,usability,Command,Command,7973,"llel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8092,usability,input,input,8092,"--reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8140,usability,input,input,8140,"ted.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s. user 1m28.020s. sys 0m5.611s. I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8327,usability,statu,status,8327,"9667868600064 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8359,usability,input,input,8359,":416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8456,usability,input,input,8456,"ine 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8601,usability,input,input,8601,"sr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.43786",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8652,usability,input,input,8652,"line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common cont",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8892,usability,input,input,8892,"_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:8985,usability,input,input,8985," seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9108,usability,input,input,9108,"20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9247,usability,input,inputs,9247,"cf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} )' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9299,usability,input,input,9299,"' returned non-zero exit status 1. ```. ## With the same input, I tried running just the make_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9392,usability,input,input,9392,"_examples step:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9515,usability,input,input,9515,"${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""/output/make_examples.tfrecord.gz"" \. --gvcf ""/output/gvcf.tfrecord.gz"". ```. For the sake of completeness, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.trunca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:9817,usability,input,input,9817,"s, here is my output:. ```. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.429270 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10057,usability,input,input,10057,"40118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10150,usability,input,input,10150,"m with NativeSamReader. I0629 23:22:14.436468 140118199654144 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10273,usability,input,input,10273," marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 can",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10386,usability,input,input,10386,": /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.437053 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 byte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10479,usability,input,input,10479,"genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.437869 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10602,usability,input,input,10602," 140118199654144 make_examples.py:648] Common contigs are ['chr20']. I0629 23:22:14.600047 140118199654144 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:22:14.600408: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:10979,usability,input,inputs,10979,"S_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.602724 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:11364,usability,error,error,11364,"ker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz. I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz. I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]. I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13399,usability,error,error,13399,"pp.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13574,usability,error,error,13574,"e_examples_runner(options). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13770,usability,clear,clear,13770,"mes = region_processor.process(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:13943,usability,help,helpfull,13943,"region_reads(region). File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear. But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```. -v,--verbosity: Logging verbosity level. Messages logged at this level or. lower will be included. Set to 1 for debug logging. If the flag was not set. or supplied, the value will be changed from the default of -1 (warning) to 0. (info) after flags are parsed. (default: '-1'). (an integer). ```. You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15332,usability,input,input,15332," already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity? I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```. # Pull the image. BIN_VERSION=1.1.0. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15713,usability,input,inputs,15713,"ty pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:15765,usability,input,input,15765,". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \. --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \. --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz"". ```. Here is the log I got from my Singularity run:. ```. INFO: Using cached SIF image. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16309,usability,input,input,16309,"rker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16549,usability,input,input,16549,"Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:16904,usability,input,input,16904,"ncated.bam.bai. I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']. I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17553,usability,input,inputs,17553,"s probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. [W::bam_hdr_read] EOF marker is absent. The input is probably truncated. [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:17938,usability,error,error,17938,"idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai. I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader. I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz. I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz. I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds. I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]. I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]. I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]. [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads. reads.extend(sam_reader.query(region)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:19973,usability,error,error,19973,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:20148,usability,error,error,20148,"/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner. candidates, examples, gvcfs, runtimes = region_processor.process(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process. reads = self.region_reads(region). File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads. error_message + '\nFailed to parse BAM/CRAM file. '. ValueError: Data loss: Failed to parse SAM record. Failed to parse BAM/CRAM file. This is often caused by:. (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file. (2) Your BAM/CRAM file could be corrupted. Please check its md5. If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:123,usability,help,help,123,"Hi @Asppagh , . I'm closing this issue given that I haven't had more information from you. If you have more information to help us reproduce this issue, please feel free to reopen!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/467:135,deployability,releas,release,135,"@kokyriakidis currently we do not have a way of doing this, but we are working on an option to output candidate alleles in an upcoming release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/467
https://github.com/google/deepvariant/issues/467:14,energy efficiency,current,currently,14,"@kokyriakidis currently we do not have a way of doing this, but we are working on an option to output candidate alleles in an upcoming release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/467
https://github.com/google/deepvariant/issues/467:579,deployability,releas,release,579,"If you want to directly take from the output of make_examples and create vcf from there, you can see this implementation here:. https://github.com/google/deepvariant/blob/r1.1/deepvariant/labeler/labeled_examples_to_vcf.py. Note that this currently requires the examples to have labels (i.e., generated with `training` mode). But you can relax the constraint here:. https://github.com/google/deepvariant/blob/r1.1/deepvariant/labeler/labeled_examples_to_vcf.py#L123-L127. But, also note that I'm not sure if this code properly deals with multi-allelics. So, waiting for the next release to use the new option that @danielecook mentioned might be a better solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/467
https://github.com/google/deepvariant/issues/467:239,energy efficiency,current,currently,239,"If you want to directly take from the output of make_examples and create vcf from there, you can see this implementation here:. https://github.com/google/deepvariant/blob/r1.1/deepvariant/labeler/labeled_examples_to_vcf.py. Note that this currently requires the examples to have labels (i.e., generated with `training` mode). But you can relax the constraint here:. https://github.com/google/deepvariant/blob/r1.1/deepvariant/labeler/labeled_examples_to_vcf.py#L123-L127. But, also note that I'm not sure if this code properly deals with multi-allelics. So, waiting for the next release to use the new option that @danielecook mentioned might be a better solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/467
https://github.com/google/deepvariant/issues/467:63,deployability,releas,release,63,"Thanks a lot, both of you, for your reply. I will for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/467
https://github.com/google/deepvariant/issues/469:77,deployability,releas,release,77,Thanks @mattwood-codifiedgenomics . I'll take a look at this before the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6547,availability,echo,echo,6547,"ataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1578,deployability,log,logs,1578,"I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2603,deployability,updat,update,2603,"{HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2623,deployability,instal,install,2623,"ols"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2653,deployability,instal,install,2653,"19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3244,deployability,log,log,3244,"_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3885,deployability,log,log,3885,"ON}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4270,deployability,updat,update,4270,"${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4290,deployability,instal,install,4290,"g_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_lis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4328,deployability,instal,install,4328,"??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.wi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4349,deployability,upgrad,upgrade,4349,"KET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-???",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4377,deployability,version,version,4377," $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4464,deployability,instal,install,4464,"USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4503,deployability,instal,install,4503,"nt:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_confi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5854,deployability,version,version,5854,"utput_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6080,deployability,instal,install,6080,"temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6256,deployability,instal,install,6256,"R_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6281,deployability,instal,install,6281,"ttern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7074,deployability,log,log,7074,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7258,deployability,continu,continue,7258,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7576,deployability,depend,dependencies,7576,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7589,deployability,instal,installed,7589,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:213,energy efficiency,GPU,GPU,213,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:237,energy efficiency,GPU,GPU,237,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:588,energy efficiency,CPU,CPU,588,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:648,energy efficiency,cpu,cpu,648,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:690,energy efficiency,cloud,cloud-platform,690,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:772,energy efficiency,cloud,cloud,772,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:874,energy efficiency,cpu,cpu-platform,874,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1089,energy efficiency,model,models,1089,"/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1225,energy efficiency,model,model,1225," because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6088,energy efficiency,cloud,cloud-tpu-client,6088,"n=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6289,energy efficiency,cloud,cloud-tpu-client,6289,"${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2805,integrability,buffer,buffer,2805,"IR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3428,integrability,buffer,buffer,3428,"BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4377,integrability,version,version,4377," $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5854,integrability,version,version,5854,"utput_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7576,integrability,depend,dependencies,7576,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:696,interoperability,platform,platform,696,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:878,interoperability,platform,platform,878,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6413,interoperability,distribut,distribute,6413,"ation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:913,modifiability,variab,variables,913,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4349,modifiability,upgrad,upgrade,4349,"KET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-???",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4377,modifiability,version,version,4377," $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5854,modifiability,version,version,5854,"utput_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7576,modifiability,depend,dependencies,7576,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:213,performance,GPU,GPU,213,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:237,performance,GPU,GPU,237,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:588,performance,CPU,CPU,588,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:648,performance,cpu,cpu,648,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:826,performance,disk,disk-size,826,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:874,performance,cpu,cpu-platform,874,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2631,performance,parallel,parallel,2631,"F=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2748,performance,time,time,2748,"GISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2780,performance,parallel,parallel,2780,"am"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3371,performance,time,time,3371,"ET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3403,performance,parallel,parallel,3403,"gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4587,performance,time,time,4587,"ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5180,performance,time,time,5180,"cords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5779,performance,time,time,5779,"put_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5833,performance,network,network,5833,"abel.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_confi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6600,performance,time,time,6600,"\. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1460,safety,input,input,1460,"ng-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highco",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1578,safety,log,logs,1578,"I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2603,safety,updat,update,2603,"{HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3244,safety,log,log,3244,"_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3885,safety,log,log,3885,"ON}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4270,safety,updat,update,4270,"${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6180,safety,test,test,6180,"me python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using $",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7074,safety,log,log,7074,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7576,safety,depend,dependencies,7576,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7633,safety,test,tests,7633,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1089,security,model,models,1089,"/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1225,security,model,model,1225," because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1578,security,log,logs,1578,"I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2596,security,apt,apt,2596,"T_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${RE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2603,security,updat,update,2603,"{HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2616,security,apt,apt,2616,"ariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2646,security,apt,apt,2646,"}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3244,security,log,log,3244,"_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3885,security,log,log,3885,"ON}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4263,security,apt,apt,4263,"l -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PRO",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4270,security,updat,update,4270,"${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4283,security,apt,apt,4283,"}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5833,security,network,network,5833,"abel.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_confi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7074,security,log,log,7074,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:167,testability,Context,Context,167,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1578,testability,log,logs,1578,"I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3244,testability,log,log,3244,"_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3885,testability,log,log,3885,"ON}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[gcp]==2.26.0. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6180,testability,test,test,6180,"me python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using $",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7074,testability,log,log,7074,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7576,testability,depend,dependencies,7576,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7628,testability,unit,unit,7628,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:7633,testability,test,tests,7633,"}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:642,usability,USER,USER,642,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:798,usability,custom,custom-,798,"First thing I'm trying to do is to see if I can follow similar steps in:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1460,usability,input,input,1460,"ng-case-study.md. and. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:. ```. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highco",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1625,usability,tool,tools,1625,"ces create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1804-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west2-b"" \. --min-cpu-platform ""Intel Skylake"". ```. Set variables. ```. YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT. OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant"". BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64. ```. ```. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3468,usability,USER,USER,3468,"llFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 instal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3482,usability,USER,USER,3482,"-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz"". ```. ```. sudo apt -y update. sudo apt -y install parallel. sudo apt -y install docker.io. ```. ```. sudo docker pull google/deepvariant:""${BIN_VERSION}"". ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. ```. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \. ${OUTPUT_BUCKET}. ```. ```. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```. sudo apt -y update. sudo apt -y install python3-dev python3-pip. pip3 install setuptools --upgrade. # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163. pip3 install apache_beam[",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5813,usability,USER,USER,5813,"T}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/trai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6341,usability,USER,USER,6341,"record-?????-of-00064.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually delete",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6635,usability,USER,USER,6635,"ging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:6649,usability,USER,USER,6649,"""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.3 \. --zone=us-central1-c. ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:. ```. pip3 install cloud-tpu-client. ``` . And then:. ```. TPU_NAME=""${USER}-demo-tpu"". TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""). ```. Check the IP:. ```. $ echo ${TPU_IP}. grpc://10.33.164.2:8470. ```. ```. ( time sudo docker run \. -v /home/${USER}:/home/${USER} \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/model_train \. --use_tpu \. --master=""${TPU_IP}"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""" \. ) 2>&1 | tee ""${LOG_DIR}/train.log"". ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:. ```. gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c. ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:144,deployability,depend,dependency,144,"Hi @mattwood-codifiedgenomics , I have an internal tracker to track this, so I'll close this for now. If I encounter any issues when adding the dependency, I'll let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:144,integrability,depend,dependency,144,"Hi @mattwood-codifiedgenomics , I have an internal tracker to track this, so I'll close this for now. If I encounter any issues when adding the dependency, I'll let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:144,modifiability,depend,dependency,144,"Hi @mattwood-codifiedgenomics , I have an internal tracker to track this, so I'll close this for now. If I encounter any issues when adding the dependency, I'll let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:144,safety,depend,dependency,144,"Hi @mattwood-codifiedgenomics , I have an internal tracker to track this, so I'll close this for now. If I encounter any issues when adding the dependency, I'll let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:144,testability,depend,dependency,144,"Hi @mattwood-codifiedgenomics , I have an internal tracker to track this, so I'll close this for now. If I encounter any issues when adding the dependency, I'll let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:82,usability,close,close,82,"Hi @mattwood-codifiedgenomics , I have an internal tracker to track this, so I'll close this for now. If I encounter any issues when adding the dependency, I'll let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:42,deployability,updat,update,42,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:171,deployability,releas,release,171,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:267,deployability,version,version,267,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:308,deployability,version,version,308,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:431,deployability,version,version,431,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1135,deployability,releas,release,1135,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:267,integrability,version,version,267,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:308,integrability,version,version,308,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:431,integrability,version,version,431,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1213,interoperability,Specif,Specifically,1213,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:1242,interoperability,specif,specify,1242,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:267,modifiability,version,version,267,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:308,modifiability,version,version,308,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:431,modifiability,version,version,431,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:356,performance,time,time,356,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:410,performance,network,network,410,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:653,performance,time,time,653,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:42,safety,updat,update,42,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:57,safety,test,tested,57,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:227,safety,test,test,227,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:42,security,updat,update,42,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:410,security,network,network,410,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:57,testability,test,tested,57,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:227,testability,test,test,227,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:390,usability,USER,USER,390,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:688,usability,USER,USER,688,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:702,usability,USER,USER,702,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```. time gcloud compute tpus create ${USER}-demo-tpu \. --network=default \. --version=2.5.0 \. --zone=us-central1-c. ```. And then with that TPU, this worked:. ```. OUTPUT_GCS_BUCKET=<OURBUCKET>. OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \. -v /home/${USER}:/home/${USER} \. deepvariant:latest \. /opt/deepvariant/bin/model_train \. --use_tpu \. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=512 \. --learning_rate=0.008 \. --start_from_checkpoint="""". ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:. ```. --tpu_name=""pichuan-demo-tpu"" \. --tpu_zone=""us-central1-c"" \. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/470:39,interoperability,Platform,Platform,39,Some information on the tool chain:. - Platform: NovaSeq 6000. - Trimming: fastP (auto-mode). - Alignment (Bwa 0.7.17). - Fixmate (samtools). - Merging across lanes (samtools). - Duplicate marking (samtools). - Variant calling with DeepVariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:24,usability,tool,tool,24,Some information on the tool chain:. - Platform: NovaSeq 6000. - Trimming: fastP (auto-mode). - Alignment (Bwa 0.7.17). - Fixmate (samtools). - Merging across lanes (samtools). - Duplicate marking (samtools). - Variant calling with DeepVariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:40,deployability,pipelin,pipeline,40,"I should also note that we ran the same pipeline on data we produced in-house, and that worked perfectly fine (including the locus I picked out here). So I am wondering if there is something about the reads our collaborator provided that somehow conflicts with assumptions made by DeepVariant (?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:40,integrability,pipelin,pipeline,40,"I should also note that we ran the same pipeline on data we produced in-house, and that worked perfectly fine (including the locus I picked out here). So I am wondering if there is something about the reads our collaborator provided that somehow conflicts with assumptions made by DeepVariant (?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:246,interoperability,conflict,conflicts,246,"I should also note that we ran the same pipeline on data we produced in-house, and that worked perfectly fine (including the locus I picked out here). So I am wondering if there is something about the reads our collaborator provided that somehow conflicts with assumptions made by DeepVariant (?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:826,deployability,releas,releases,826,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:578,usability,visual,visualize,578,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:691,usability,support,support,691,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:750,usability,support,support,750,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1261,usability,visual,visualization,1261,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1297,usability,support,supports,1297,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1342,usability,support,support,1342,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1381,usability,support,support,1381,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1450,usability,user,user-images,1450,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1716,usability,user,user-images,1716,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment. Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. . It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate. This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:. ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:. ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:97,deployability,releas,releases,97,"Thanks for digging into this - yea I guess that makes sense. Will be waiting for a fix in future releases then , thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:9,deployability,updat,update,9,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56. Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:31,performance,time,time,31,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56. Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:9,safety,updat,update,9,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56. Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:162,safety,compl,completed,162,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56. Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:275,safety,input,input,275,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56. Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:9,security,updat,update,9,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56. Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:162,security,compl,completed,162,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56. Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:275,usability,input,input,275,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56. Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:139,deployability,releas,release,139,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:233,deployability,updat,update,233,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:307,deployability,version,version,307,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:349,deployability,releas,release,349,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:22,energy efficiency,model,model,22,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:307,integrability,version,version,307,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:307,modifiability,version,version,307,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:233,safety,updat,update,233,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:22,security,model,model,22,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:233,security,updat,update,233,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:120,usability,help,helps,120,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:357,usability,help,helped,357,"Hi, is this using WGS model? You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps? In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:55,deployability,upgrad,upgrading,55,"This was exome data, so WES. . And interestingly, yes, upgrading from 1.1 to 1.2 did solve this issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:55,modifiability,upgrad,upgrading,55,"This was exome data, so WES. . And interestingly, yes, upgrading from 1.1 to 1.2 did solve this issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:435,deployability,releas,release,435,"Thanks @marchoeppner . Good to hear that 1.2 worked. I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:488,deployability,releas,releases,488,"Thanks @marchoeppner . Good to hear that 1.2 worked. I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:542,energy efficiency,model,model,542,"Thanks @marchoeppner . Good to hear that 1.2 worked. I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:593,energy efficiency,model,model,593,"Thanks @marchoeppner . Good to hear that 1.2 worked. I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:542,security,model,model,542,"Thanks @marchoeppner . Good to hear that 1.2 worked. I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:593,security,model,model,593,"Thanks @marchoeppner . Good to hear that 1.2 worked. I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:942,usability,close,close,942,"Thanks @marchoeppner . Good to hear that 1.2 worked. I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/471:100,availability,sli,slightly,100,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:683,energy efficiency,core,cores,683,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:747,energy efficiency,CPU,CPU,747,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:751,energy efficiency,core,cores,751,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:426,modifiability,PAC,PACBIO,426,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:747,performance,CPU,CPU,747,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:11,reliability,doe,does,11,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:100,reliability,sli,slightly,100,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:223,safety,input,input,223,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:444,safety,input,input,444,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:493,safety,input,input,493,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:466,testability,unit,unittest,466,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:20,usability,command,command,20,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:51,usability,document,document,51,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:89,usability,command,command,89,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:223,usability,input,input,223,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:444,usability,input,input,444,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:493,usability,input,input,493,"@canna1998 does the command used in our quickstart document for you? I noticed that your command is slightly different and is not mounting the necessary directories using `-v`. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/pull/472:346,energy efficiency,reduc,reduce,346,"Hi @dkurt ,. I've tested your change, and compare to without using OpenVINO (we internally are now using `intel-tensorflow==2.5.0`). Here are the call_variants runtime comparison on our latest code:. * WGS:. * Without OpenVINO: ~166m (this was an average from 3 runs). * With OpenVINO: 151m26.692s (this was from one run. I can do 2 more runs to reduce the variance for comparison). My PacBio and Hybrid runs are still going. I can report them when they're done. I want to check whether this is roughly inline with your expectation, or if there are things I need to tweak further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:386,modifiability,Pac,PacBio,386,"Hi @dkurt ,. I've tested your change, and compare to without using OpenVINO (we internally are now using `intel-tensorflow==2.5.0`). Here are the call_variants runtime comparison on our latest code:. * WGS:. * Without OpenVINO: ~166m (this was an average from 3 runs). * With OpenVINO: 151m26.692s (this was from one run. I can do 2 more runs to reduce the variance for comparison). My PacBio and Hybrid runs are still going. I can report them when they're done. I want to check whether this is roughly inline with your expectation, or if there are things I need to tweak further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:18,safety,test,tested,18,"Hi @dkurt ,. I've tested your change, and compare to without using OpenVINO (we internally are now using `intel-tensorflow==2.5.0`). Here are the call_variants runtime comparison on our latest code:. * WGS:. * Without OpenVINO: ~166m (this was an average from 3 runs). * With OpenVINO: 151m26.692s (this was from one run. I can do 2 more runs to reduce the variance for comparison). My PacBio and Hybrid runs are still going. I can report them when they're done. I want to check whether this is roughly inline with your expectation, or if there are things I need to tweak further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:18,testability,test,tested,18,"Hi @dkurt ,. I've tested your change, and compare to without using OpenVINO (we internally are now using `intel-tensorflow==2.5.0`). Here are the call_variants runtime comparison on our latest code:. * WGS:. * Without OpenVINO: ~166m (this was an average from 3 runs). * With OpenVINO: 151m26.692s (this was from one run. I can do 2 more runs to reduce the variance for comparison). My PacBio and Hybrid runs are still going. I can report them when they're done. I want to check whether this is roughly inline with your expectation, or if there are things I need to tweak further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:34,deployability,updat,updated,34,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:103,deployability,log,log,103,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:34,safety,updat,updated,34,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:103,safety,log,log,103,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:34,security,updat,updated,34,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:103,security,log,log,103,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/472:103,testability,log,log,103,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472
https://github.com/google/deepvariant/pull/473:50,usability,confirm,confirm,50,"@MariaNattestad I'm waiting for @williamrowell to confirm that this looks good. Once he confirms, I'll ask you to take a look and merge. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/473
https://github.com/google/deepvariant/pull/473:88,usability,confirm,confirms,88,"@MariaNattestad I'm waiting for @williamrowell to confirm that this looks good. Once he confirms, I'll ask you to take a look and merge. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/473
https://github.com/google/deepvariant/pull/473:64,deployability,releas,release,64,Looks good to me. Will the flags will stay the same in the next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/473
https://github.com/google/deepvariant/pull/473:61,deployability,releas,release,61,"@williamrowell yes, the flags will stay the same in the next release for DeepVariant, and we're adding this to DeepTrio too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/473
https://github.com/google/deepvariant/issues/474:135,availability,cluster,cluster,135,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:481,availability,avail,available,481,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:506,availability,cluster,cluster,506,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:631,availability,cluster,cluster,631,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:135,deployability,cluster,cluster,135,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:506,deployability,cluster,cluster,506,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:524,deployability,contain,container,524,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:631,deployability,cluster,cluster,631,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:742,deployability,stage,stages,742,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:786,deployability,resourc,resources,786,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:332,energy efficiency,CPU,CPU,332,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:336,energy efficiency,core,cores,336,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:349,energy efficiency,GPU,GPU,349,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:662,energy efficiency,optim,optimize,662,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:786,energy efficiency,resourc,resources,786,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:829,energy efficiency,CPU,CPUs,829,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:864,energy efficiency,GPU,GPUs,864,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:915,energy efficiency,CPU,CPU,915,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:332,performance,CPU,CPU,332,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:349,performance,GPU,GPU,349,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:662,performance,optimiz,optimize,662,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:705,performance,time,times,705,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:786,performance,resourc,resources,786,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:829,performance,CPU,CPUs,829,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:864,performance,GPU,GPUs,864,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:915,performance,CPU,CPU,915,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:481,reliability,availab,available,481,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:481,safety,avail,available,481,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:786,safety,resourc,resources,786,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:481,security,availab,available,481,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:436,testability,simpl,simple,436,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:786,testability,resourc,resources,786,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:200,usability,help,help,200,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:436,usability,simpl,simple,436,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:538,usability,command,commands,538,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:1067,usability,help,helps,1067,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster. Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:259,availability,cluster,cluster,259,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:259,deployability,cluster,cluster,259,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:718,deployability,modul,module,718,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:192,energy efficiency,cloud,cloud,192,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:725,energy efficiency,load,load,725,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:298,integrability,sub,submission,298,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:400,modifiability,pac,pacbio,400,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:536,modifiability,pac,pacbio,536,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:718,modifiability,modul,module,718,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:936,modifiability,PAC,PACBIO,936,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:725,performance,load,load,725,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:718,safety,modul,module,718,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:56,usability,close,closer,56,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:594,usability,command,command,594,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:1135,usability,user,users,1135,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:1213,usability,user,users,1213,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :. - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add. - Our cluster runs pbs so I need to create a submission script for this. - We also have singularity. - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a. my reference sequence in path_b. my pacbio bam in path_c. Could you advise on the singularity command to execute this job? Below is what i've tried but I must be missing and misunderstanding a few key elements. =====. module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO. --ref=""References/Panu3.0_X_Y_Mito.fa"" \. --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \. --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \. --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \. ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:346,availability,error,error,346,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:352,integrability,messag,messages,352,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:352,interoperability,messag,messages,352,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:346,performance,error,error,346,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:312,reliability,doe,doesn,312,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:203,safety,input,input-files-eg-could-not-open,203,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:266,safety,input,input,266,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:346,safety,error,error,346,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:70,usability,command,command,70,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:203,usability,input,input-files-eg-could-not-open,203,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:266,usability,input,input,266,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:346,usability,error,error,346,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:. https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:28,deployability,continu,continuing,28,I'll close this since we're continuing the conversation over email.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:43,interoperability,convers,conversation,43,I'll close this since we're continuing the conversation over email.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:5,usability,close,close,5,I'll close this since we're continuing the conversation over email.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/475:759,availability,avail,available,759,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:118,deployability,updat,update,118,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:182,deployability,releas,release,182,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:195,deployability,fail,failing,195,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:649,energy efficiency,model,model,649,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:470,performance,content,content,470,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:195,reliability,fail,failing,195,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:269,reliability,doe,does,269,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:684,reliability,doe,does,684,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:759,reliability,availab,available,759,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:118,safety,updat,update,118,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:759,safety,avail,available,759,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:118,security,updat,update,118,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:649,security,model,model,649,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:679,security,NIST,NIST,679,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:751,security,NIST,NIST,751,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:759,security,availab,available,759,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:664,testability,simpl,simply,664,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:62,usability,prefer,preferable,62,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:148,usability,document,documentation,148,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:308,usability,prefer,preferable,308,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:664,usability,simpl,simply,664,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:16,deployability,updat,update,16,"Hi @maca8e . To update this issue, we have now made the change to DeepVariant_unfiltered GLnexus preset in the DeepTrio documentation. Thank you again for pointing this out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:16,safety,updat,update,16,"Hi @maca8e . To update this issue, we have now made the change to DeepVariant_unfiltered GLnexus preset in the DeepTrio documentation. Thank you again for pointing this out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:16,security,updat,update,16,"Hi @maca8e . To update this issue, we have now made the change to DeepVariant_unfiltered GLnexus preset in the DeepTrio documentation. Thank you again for pointing this out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:120,usability,document,documentation,120,"Hi @maca8e . To update this issue, we have now made the change to DeepVariant_unfiltered GLnexus preset in the DeepTrio documentation. Thank you again for pointing this out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:169,energy efficiency,model,models,169,"Hi @AndrewCarroll . Thanks for pointing out the preprint, I hadn't come across that yet so it helped a great deal understanding the differences between the parent-child models and the training. Glad to hear that the `DeepVariant_unfiltered` config is now the recommended preset. Many thanks,. Macabe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:169,security,model,models,169,"Hi @AndrewCarroll . Thanks for pointing out the preprint, I hadn't come across that yet so it helped a great deal understanding the differences between the parent-child models and the training. Glad to hear that the `DeepVariant_unfiltered` config is now the recommended preset. Many thanks,. Macabe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:114,testability,understand,understanding,114,"Hi @AndrewCarroll . Thanks for pointing out the preprint, I hadn't come across that yet so it helped a great deal understanding the differences between the parent-child models and the training. Glad to hear that the `DeepVariant_unfiltered` config is now the recommended preset. Many thanks,. Macabe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:94,usability,help,helped,94,"Hi @AndrewCarroll . Thanks for pointing out the preprint, I hadn't come across that yet so it helped a great deal understanding the differences between the parent-child models and the training. Glad to hear that the `DeepVariant_unfiltered` config is now the recommended preset. Many thanks,. Macabe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/476:189,availability,error,error,189,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:253,availability,avail,available,253,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:238,deployability,manag,manager,238,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:238,energy efficiency,manag,manager,238,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:230,modifiability,pac,package,230,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:189,performance,error,error,189,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:253,reliability,availab,available,253,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:189,safety,error,error,189,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:238,safety,manag,manager,238,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:253,safety,avail,available,253,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:91,security,modif,modified,91,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:225,security,apt,apt,225,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:253,security,availab,available,253,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:189,usability,error,error,189,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:206,usability,indicat,indicate,206,@williambrandler can you provide more detail regarding what you are trying to do? Have you modified the existing dockerfile to replace the base image with the Databricks runtime image? The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:17,deployability,build,building,17,"hey yes, this is building off the databricks runtime, Ubuntu 18.04. So replacing,. ARG FROM_IMAGE=ubuntu:20.04. with,. ARG FROM_IMAGE=databricksruntime/minimal:9.x. I also added, . `sudo -H apt-get -qq -y install python3-apt python3-distutils`. what do you recommend doing to troubleshoot?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:205,deployability,instal,install,205,"hey yes, this is building off the databricks runtime, Ubuntu 18.04. So replacing,. ARG FROM_IMAGE=ubuntu:20.04. with,. ARG FROM_IMAGE=databricksruntime/minimal:9.x. I also added, . `sudo -H apt-get -qq -y install python3-apt python3-distutils`. what do you recommend doing to troubleshoot?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:190,security,apt,apt-get,190,"hey yes, this is building off the databricks runtime, Ubuntu 18.04. So replacing,. ARG FROM_IMAGE=ubuntu:20.04. with,. ARG FROM_IMAGE=databricksruntime/minimal:9.x. I also added, . `sudo -H apt-get -qq -y install python3-apt python3-distutils`. what do you recommend doing to troubleshoot?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:221,security,apt,apt,221,"hey yes, this is building off the databricks runtime, Ubuntu 18.04. So replacing,. ARG FROM_IMAGE=ubuntu:20.04. with,. ARG FROM_IMAGE=databricksruntime/minimal:9.x. I also added, . `sudo -H apt-get -qq -y install python3-apt python3-distutils`. what do you recommend doing to troubleshoot?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:152,usability,minim,minimal,152,"hey yes, this is building off the databricks runtime, Ubuntu 18.04. So replacing,. ARG FROM_IMAGE=ubuntu:20.04. with,. ARG FROM_IMAGE=databricksruntime/minimal:9.x. I also added, . `sudo -H apt-get -qq -y install python3-apt python3-distutils`. what do you recommend doing to troubleshoot?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:28,deployability,version,version,28,@williambrandler the latest version of DV requires Python 3.8 and Ubuntu 20.04. Are you able to upgrade Ubuntu on the databricks image?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:96,deployability,upgrad,upgrade,96,@williambrandler the latest version of DV requires Python 3.8 and Ubuntu 20.04. Are you able to upgrade Ubuntu on the databricks image?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:28,integrability,version,version,28,@williambrandler the latest version of DV requires Python 3.8 and Ubuntu 20.04. Are you able to upgrade Ubuntu on the databricks image?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:28,modifiability,version,version,28,@williambrandler the latest version of DV requires Python 3.8 and Ubuntu 20.04. Are you able to upgrade Ubuntu on the databricks image?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:96,modifiability,upgrad,upgrade,96,@williambrandler the latest version of DV requires Python 3.8 and Ubuntu 20.04. Are you able to upgrade Ubuntu on the databricks image?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:185,availability,state,statement,185,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:37,deployability,depend,dependencies,37,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:97,deployability,version,version,97,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:238,deployability,Build,Build,238,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:300,deployability,instal,installs,300,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:320,deployability,depend,dependencies,320,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:408,deployability,build,build-prereq,408,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:37,integrability,depend,dependencies,37,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:97,integrability,version,version,97,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:185,integrability,state,statement,185,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:320,integrability,depend,dependencies,320,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:28,interoperability,specif,specific,28,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:37,modifiability,depend,dependencies,37,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:97,modifiability,version,version,97,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:320,modifiability,depend,dependencies,320,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:12,performance,time,time,12,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:37,safety,depend,dependencies,37,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:125,safety,test,tested,125,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:320,safety,depend,dependencies,320,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:37,testability,depend,dependencies,37,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:125,testability,test,tested,125,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:320,testability,depend,dependencies,320,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:198,usability,help,help,198,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:612,availability,state,statement,612,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:902,availability,state,statement,902,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:39,deployability,depend,dependencies,39,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:99,deployability,version,version,99,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:187,deployability,build,builds,187,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:267,deployability,updat,updated,267,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:455,deployability,updat,update,455,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:670,deployability,Build,Build,670,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:732,deployability,instal,installs,732,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:752,deployability,depend,dependencies,752,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:840,deployability,build,build-prereq,840,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:886,deployability,build,build-prereq,886,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1039,deployability,build,builds,1039,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1125,deployability,build,build,1125,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1174,deployability,instal,installs,1174,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1239,deployability,build,build-prereq,1239,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1320,deployability,instal,installed,1320,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1401,deployability,build,build,1401,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1458,deployability,stage,stage,1458,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1485,deployability,instal,installed,1485,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:39,integrability,depend,dependencies,39,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:99,integrability,version,version,99,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:612,integrability,state,statement,612,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:752,integrability,depend,dependencies,752,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:902,integrability,state,statement,902,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:30,interoperability,specif,specific,30,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:39,modifiability,depend,dependencies,39,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:99,modifiability,version,version,99,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:752,modifiability,depend,dependencies,752,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:14,performance,time,time,14,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:39,safety,depend,dependencies,39,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:127,safety,test,tested,127,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:267,safety,updat,updated,267,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:296,safety,test,tested,296,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:455,safety,updat,update,455,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:752,safety,depend,dependencies,752,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:267,security,updat,updated,267,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:455,security,updat,update,455,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:39,testability,depend,dependencies,39,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:127,testability,test,tested,127,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:296,testability,test,tested,296,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:752,testability,depend,dependencies,752,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:625,usability,help,help,625,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1013,usability,tool,tools,1013,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1546,usability,clear,clear,1546,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04? @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > . > Would also like some clarification on this statement to help me figure out what is going on,. > . > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:49,deployability,version,version,49,"thanks! . Ah after some digging I see the newest version of the Databricks Runtime is supposed to be using Ubuntu 20.04, but the existing docker containers have not been upgraded to reflect that. . Let me see if I can get this resolved...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:145,deployability,contain,containers,145,"thanks! . Ah after some digging I see the newest version of the Databricks Runtime is supposed to be using Ubuntu 20.04, but the existing docker containers have not been upgraded to reflect that. . Let me see if I can get this resolved...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:170,deployability,upgrad,upgraded,170,"thanks! . Ah after some digging I see the newest version of the Databricks Runtime is supposed to be using Ubuntu 20.04, but the existing docker containers have not been upgraded to reflect that. . Let me see if I can get this resolved...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:49,integrability,version,version,49,"thanks! . Ah after some digging I see the newest version of the Databricks Runtime is supposed to be using Ubuntu 20.04, but the existing docker containers have not been upgraded to reflect that. . Let me see if I can get this resolved...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:49,modifiability,version,version,49,"thanks! . Ah after some digging I see the newest version of the Databricks Runtime is supposed to be using Ubuntu 20.04, but the existing docker containers have not been upgraded to reflect that. . Let me see if I can get this resolved...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:170,modifiability,upgrad,upgraded,170,"thanks! . Ah after some digging I see the newest version of the Databricks Runtime is supposed to be using Ubuntu 20.04, but the existing docker containers have not been upgraded to reflect that. . Let me see if I can get this resolved...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:54,deployability,instal,installation,54,Hello! I also need to run DV on Ubuntu 18.04. But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:120,deployability,instal,installed,120,Hello! I also need to run DV on Ubuntu 18.04. But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:207,deployability,instal,installation,207,Hello! I also need to run DV on Ubuntu 18.04. But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:152,performance,time,times,152,Hello! I also need to run DV on Ubuntu 18.04. But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:82,safety,compl,complex,82,Hello! I also need to run DV on Ubuntu 18.04. But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:82,security,compl,complex,82,Hello! I also need to run DV on Ubuntu 18.04. But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:101,deployability,build,build,101,"Hi @serge2016 , . to help us understand your use case a bit more, can you tell us why you decided to build from scratch rather than using Docker or Singularity? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:29,testability,understand,understand,29,"Hi @serge2016 , . to help us understand your use case a bit more, can you tell us why you decided to build from scratch rather than using Docker or Singularity? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:21,usability,help,help,21,"Hi @serge2016 , . to help us understand your use case a bit more, can you tell us why you decided to build from scratch rather than using Docker or Singularity? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:138,deployability,instal,installed,138,"Sorry for the delay. I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:176,deployability,instal,installation,176,"Sorry for the delay. I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:238,deployability,build,build,238,"Sorry for the delay. I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:168,energy efficiency,current,current,168,"Sorry for the delay. I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:279,interoperability,conflict,conflicting,279,"Sorry for the delay. I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:309,modifiability,pac,packages,309,"Sorry for the delay. I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:132,usability,tool,tools,132,"Sorry for the delay. I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:23,deployability,instal,installation,23,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:104,deployability,instal,installation,104,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:175,deployability,build,building,175,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:269,deployability,instal,installation,269,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:232,safety,compl,complicated,232,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:232,security,compl,complicated,232,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:359,deployability,log,log,359,"Hi @serge2016 ,. If you have a setup that works for Ubuntu20.04 as well as Ubuntu18.04, I'd be happy to use it as our default. You can send a pull request with your changes that works for both Ubuntu20.04 and Ubuntu18.04. Even though we can't directly merge pull requests, I can make corresponding internal changes and mention your contribution in the commit log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:359,safety,log,log,359,"Hi @serge2016 ,. If you have a setup that works for Ubuntu20.04 as well as Ubuntu18.04, I'd be happy to use it as our default. You can send a pull request with your changes that works for both Ubuntu20.04 and Ubuntu18.04. Even though we can't directly merge pull requests, I can make corresponding internal changes and mention your contribution in the commit log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:359,security,log,log,359,"Hi @serge2016 ,. If you have a setup that works for Ubuntu20.04 as well as Ubuntu18.04, I'd be happy to use it as our default. You can send a pull request with your changes that works for both Ubuntu20.04 and Ubuntu18.04. Even though we can't directly merge pull requests, I can make corresponding internal changes and mention your contribution in the commit log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:359,testability,log,log,359,"Hi @serge2016 ,. If you have a setup that works for Ubuntu20.04 as well as Ubuntu18.04, I'd be happy to use it as our default. You can send a pull request with your changes that works for both Ubuntu20.04 and Ubuntu18.04. Even though we can't directly merge pull requests, I can make corresponding internal changes and mention your contribution in the commit log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:60,deployability,build,build,60,"And , @williambrandler , let us know whether you're able to build it with Ubuntu20.04. If you have further questions, feel free to reopen this issue, or open a new one.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:34,deployability,build,building,34,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:216,deployability,Stage,Stage,216,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:223,deployability,Instal,Install,223,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:259,deployability,build,build-prereq,259,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:357,deployability,build,build-prereq,357,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:425,deployability,fail,failed,425,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:454,deployability,build,build-prereq,454,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:560,performance,time,time,560,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:425,reliability,fail,failed,425,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:568,safety,test,test,568,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:505,testability,simpl,simple,505,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:568,testability,test,test,568,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:593,testability,plan,plan,593,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:292,usability,command,command,292,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:389,usability,command,command,389,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:505,usability,simpl,simple,505,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting. 18 0.297 ./build-prereq.sh: line 50: bazel: command not found. 18 0.298 ~/bazel /opt/deepvariant. 18 0.298 ./build-prereq.sh: line 56: curl: command not found. ------. executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:76,deployability,build,build-prereq,76,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:105,deployability,instal,install,105,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:244,deployability,build,build-prereq,244,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:333,deployability,instal,installed,333,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:398,deployability,build,build-prereq,398,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:486,deployability,instal,installed,486,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:496,energy efficiency,current,currently,496,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:97,testability,plan,plan,97,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:31,usability,command,command,31,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:143,usability,command,command,143,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:64,deployability,build,build,64,"@pichuan I finally got back to this. I was able to successfully build the container for the ultima genomics fork of deep variant to run on databricks (for CPUs, not tried GPUs yet). . It did require a few small tweaks to get it working. And I would be happy to run any folks through it if they want to run deep variant on databricks in future...be it on GCP, Azure or AWS. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:74,deployability,contain,container,74,"@pichuan I finally got back to this. I was able to successfully build the container for the ultima genomics fork of deep variant to run on databricks (for CPUs, not tried GPUs yet). . It did require a few small tweaks to get it working. And I would be happy to run any folks through it if they want to run deep variant on databricks in future...be it on GCP, Azure or AWS. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:155,energy efficiency,CPU,CPUs,155,"@pichuan I finally got back to this. I was able to successfully build the container for the ultima genomics fork of deep variant to run on databricks (for CPUs, not tried GPUs yet). . It did require a few small tweaks to get it working. And I would be happy to run any folks through it if they want to run deep variant on databricks in future...be it on GCP, Azure or AWS. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:171,energy efficiency,GPU,GPUs,171,"@pichuan I finally got back to this. I was able to successfully build the container for the ultima genomics fork of deep variant to run on databricks (for CPUs, not tried GPUs yet). . It did require a few small tweaks to get it working. And I would be happy to run any folks through it if they want to run deep variant on databricks in future...be it on GCP, Azure or AWS. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:155,performance,CPU,CPUs,155,"@pichuan I finally got back to this. I was able to successfully build the container for the ultima genomics fork of deep variant to run on databricks (for CPUs, not tried GPUs yet). . It did require a few small tweaks to get it working. And I would be happy to run any folks through it if they want to run deep variant on databricks in future...be it on GCP, Azure or AWS. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:171,performance,GPU,GPUs,171,"@pichuan I finally got back to this. I was able to successfully build the container for the ultima genomics fork of deep variant to run on databricks (for CPUs, not tried GPUs yet). . It did require a few small tweaks to get it working. And I would be happy to run any folks through it if they want to run deep variant on databricks in future...be it on GCP, Azure or AWS. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:210,interoperability,share,share,210,"Thank you @williambrandler ! Glad to know it works for you. If we (or any other users) need to run DeepVariant on Databricks and encounter any issues, we'll definitely reach out. If you have some quick tips to share, please feel free to post them here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:80,usability,user,users,80,"Thank you @williambrandler ! Glad to know it works for you. If we (or any other users) need to run DeepVariant on Databricks and encounter any issues, we'll definitely reach out. If you have some quick tips to share, please feel free to post them here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:202,usability,tip,tips,202,"Thank you @williambrandler ! Glad to know it works for you. If we (or any other users) need to run DeepVariant on Databricks and encounter any issues, we'll definitely reach out. If you have some quick tips to share, please feel free to post them here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/477:105,energy efficiency,model,modelckpt-to-modelpb-savedmodel-format,105,I found this helpful https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/477
https://github.com/google/deepvariant/issues/477:137,interoperability,format,format,137,I found this helpful https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/477
https://github.com/google/deepvariant/issues/477:105,security,model,modelckpt-to-modelpb-savedmodel-format,105,I found this helpful https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/477
https://github.com/google/deepvariant/issues/477:13,usability,help,helpful,13,I found this helpful https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/477
https://github.com/google/deepvariant/issues/478:116,interoperability,specif,specifically,116,"Hi @leaxael , sorry somehow we missed this earlier. I think I roughly understand your question. Given that we don't specifically do anything about the file permissions, I think this is related to docker and sudo. Is there a particular suggestion that you'll like to make here? One alternative for users who don't want to use Docker is to consider Singularity as an alternative. Would that be better for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:156,safety,permiss,permissions,156,"Hi @leaxael , sorry somehow we missed this earlier. I think I roughly understand your question. Given that we don't specifically do anything about the file permissions, I think this is related to docker and sudo. Is there a particular suggestion that you'll like to make here? One alternative for users who don't want to use Docker is to consider Singularity as an alternative. Would that be better for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:70,testability,understand,understand,70,"Hi @leaxael , sorry somehow we missed this earlier. I think I roughly understand your question. Given that we don't specifically do anything about the file permissions, I think this is related to docker and sudo. Is there a particular suggestion that you'll like to make here? One alternative for users who don't want to use Docker is to consider Singularity as an alternative. Would that be better for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:297,usability,user,users,297,"Hi @leaxael , sorry somehow we missed this earlier. I think I roughly understand your question. Given that we don't specifically do anything about the file permissions, I think this is related to docker and sudo. Is there a particular suggestion that you'll like to make here? One alternative for users who don't want to use Docker is to consider Singularity as an alternative. Would that be better for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/479:83,safety,test,test,83,"Hi @DLPerf , thanks for looking into this. I can look into your suggestions above, test out the changes, and report back. If you want to just create a PR with your fixes suggested above, that will certainly make my job easier and avoid miscommunication. Note that our repo isn't quite setup to directly take external pull requests, but if you create one, I can create a corresponding internal commit and point to your PR when we commit it. Let me know if you want to create a PR, or just for me to try to follow your suggestion above. Either way works. If you'll create a PR, I'll wait for that first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:230,safety,avoid,avoid,230,"Hi @DLPerf , thanks for looking into this. I can look into your suggestions above, test out the changes, and report back. If you want to just create a PR with your fixes suggested above, that will certainly make my job easier and avoid miscommunication. Note that our repo isn't quite setup to directly take external pull requests, but if you create one, I can create a corresponding internal commit and point to your PR when we commit it. Let me know if you want to create a PR, or just for me to try to follow your suggestion above. Either way works. If you'll create a PR, I'll wait for that first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:83,testability,test,test,83,"Hi @DLPerf , thanks for looking into this. I can look into your suggestions above, test out the changes, and report back. If you want to just create a PR with your fixes suggested above, that will certainly make my job easier and avoid miscommunication. Note that our repo isn't quite setup to directly take external pull requests, but if you create one, I can create a corresponding internal commit and point to your PR when we commit it. Let me know if you want to create a PR, or just for me to try to follow your suggestion above. Either way works. If you'll create a PR, I'll wait for that first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:150,deployability,updat,update,150,"Thanks @DLPerf . Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:184,performance,perform,performance,184,"Thanks @DLPerf . Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:150,safety,updat,update,150,"Thanks @DLPerf . Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:150,security,updat,update,150,"Thanks @DLPerf . Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:184,usability,perform,performance,184,"Thanks @DLPerf . Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:410,deployability,version,version,410,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:549,deployability,Updat,Update,549,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:116,integrability,batch,batch,116,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:410,integrability,version,version,410,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:410,modifiability,version,version,410,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:116,performance,batch,batch,116,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:534,performance,time,time,534,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:549,safety,Updat,Update,549,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:549,security,Updat,Update,549,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:586,usability,progress,progress,586,"Hi @DLPerf ,. I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. . (Update: I might be making a bit more progress here.). But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:36,deployability,updat,update,36,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:879,deployability,observ,observed,879,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:2115,deployability,updat,update,2115,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:594,energy efficiency,measur,measure,594,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:679,energy efficiency,CPU,CPU,679,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:715,energy efficiency,CPU,CPU,715,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:926,energy efficiency,model,models,926,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:99,integrability,batch,batch,99,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:1978,integrability,batch,batch,1978,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:1226,modifiability,Pac,PacBio,1226,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:1326,modifiability,Pac,PacBio,1326,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:99,performance,batch,batch,99,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:679,performance,CPU,CPU,679,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:715,performance,CPU,CPU,715,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:1720,performance,time,times,1720,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:1978,performance,batch,batch,1978,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:528,reliability,doe,doesn,528,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:1522,reliability,doe,does,1522,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:36,safety,updat,update,36,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:707,safety,test,test,707,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:785,safety,test,test,785,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:2115,safety,updat,update,2115,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:36,security,updat,update,36,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:926,security,model,models,926,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:1576,security,sign,significant,1576,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:2115,security,updat,update,2115,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:707,testability,test,test,707,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:785,testability,test,test,785,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:879,testability,observ,observed,879,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:2107,testability,plan,plan,2107,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:1933,usability,efficien,efficient,1933,"en, I changed these lines:. https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194. to:. ```. parsed = tf.io.parse_example(serialized=tf_example,. features=self.feature_extraction_spec). image = parsed['image/encoded']. if self.tensor_shape:. image = tf.io.decode_raw(image, tf.uint8). image = tf.reshape(image, [-1]+self.tensor_shape). ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:. (1) Accuracy is the same for 4 models - this is expected. (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina). ```. real 114m10.723s. real 193m19.324s. real 74m39.826s. ```. ## WES (Illumina). ```. real 7m26.571s. real 1m24.083s. real 1m3.679s. ```. ## PacBio (HiFi). ```. real 126m28.198s. real 175m40.960s. real 67m49.753s. ```. ## Hybrid (Illumina + PacBio HiFi). ```. real 161m32.681s. real 200m26.225s. real 63m20.731s. ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:. (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers. (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:18,usability,close,close,18,"Hi @DLPerf , I'll close this for now because I've done one round of investigation. If you have more thoughts on this, please feel free to comment again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/480:450,security,sign,signal,450,"If you're curious about how the underlying multiallelic variants are represented, you can see the ""Multiallelic variants"" section in https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ . Another good blog post is https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. But to answer your question - Yes, DeepVariant is designed to be able to make calls like the one that you described, assuming there's signal form the BAM file to support it. Closing this now. Feel free to reopen if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/480
https://github.com/google/deepvariant/issues/480:478,usability,support,support,478,"If you're curious about how the underlying multiallelic variants are represented, you can see the ""Multiallelic variants"" section in https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ . Another good blog post is https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. But to answer your question - Yes, DeepVariant is designed to be able to make calls like the one that you described, assuming there's signal form the BAM file to support it. Closing this now. Feel free to reopen if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/480
https://github.com/google/deepvariant/issues/481:37,availability,error,error,37,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:43,integrability,messag,message,43,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:43,interoperability,messag,message,43,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:37,performance,error,error,37,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:37,safety,error,error,37,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:124,safety,test,tested,124,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:331,safety,test,testdata,331,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:407,safety,test,testdata,407,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:464,safety,test,testdata,464,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:517,safety,test,testdata,517,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:569,safety,test,testdata,569,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:124,testability,test,tested,124,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:331,testability,test,testdata,331,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:407,testability,test,testdata,407,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:464,testability,test,testdata,464,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:517,testability,test,testdata,517,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:569,testability,test,testdata,569,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:37,usability,error,error,37,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:801,usability,support,supported,801,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices? I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```. $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai. $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi. ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:37,safety,test,tested,37,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:82,safety,test,tested,82,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:230,safety,prevent,prevent,230,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:230,security,preven,prevent,230,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:37,testability,test,tested,37,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:82,testability,test,tested,82,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:213,usability,document,documentation,213,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/481:251,usability,user,users,251,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481
https://github.com/google/deepvariant/issues/482:26,deployability,stack,stack,26,Can you share more of the stack trace?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:8,interoperability,share,share,8,Can you share more of the stack trace?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:32,testability,trace,trace,32,Can you share more of the stack trace?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:20,usability,user,user-images,20,![dv_error](https://user-images.githubusercontent.com/56719787/133974583-c77a3de2-6bb2-4b7f-b6dc-dbc1540c75bb.png).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:19,deployability,log,log,19,"@kirti141 from the log, I agree that it isn't quite clear. . Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:106,energy efficiency,CPU,CPU,106,"@kirti141 from the log, I agree that it isn't quite clear. . Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:110,energy efficiency,core,cores,110,"@kirti141 from the log, I agree that it isn't quite clear. . Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:106,performance,CPU,CPU,106,"@kirti141 from the log, I agree that it isn't quite clear. . Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:19,safety,log,log,19,"@kirti141 from the log, I agree that it isn't quite clear. . Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:19,security,log,log,19,"@kirti141 from the log, I agree that it isn't quite clear. . Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:19,testability,log,log,19,"@kirti141 from the log, I agree that it isn't quite clear. . Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:52,usability,clear,clear,52,"@kirti141 from the log, I agree that it isn't quite clear. . Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:194,availability,error,error,194,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:321,deployability,log,log,321,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:15,energy efficiency,core,cores,15,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:411,energy efficiency,CPU,CPU,411,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:415,energy efficiency,core,cores,415,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:194,performance,error,error,194,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:411,performance,CPU,CPU,411,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:86,safety,input,input,86,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:194,safety,error,error,194,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:321,safety,log,log,321,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:321,security,log,log,321,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:697,security,auth,auth,697,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:755,security,Triag,Triage,755,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:173,testability,coverag,coverage,173,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:321,testability,log,log,321,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:86,usability,input,input,86,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:194,usability,error,error,194,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:357,usability,clear,clear,357,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than. 20 Gb file size, but when I increase the file size / coverage, I get the. error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it. > isn't quite clear. > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:146,deployability,releas,release,146,"Hmm, empirically we've been able to handle larger input BAM than 20GB with less RAM you had. . Another known issue (which we will fix in the next release) is that if auxiliary fields are being read in, and if the BAM has many of them, it could unnecessarily increase the RAM usage. However, in this case given that you're using WGS model (which by default isn't parsing aux fields), that shouldn't be an issue... @kirti141 Another question for you - is there a BAM file that you can make public (with no sensitive information, of course) that we can attempt to reproduce this issue? Thanks again for reporting this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:332,energy efficiency,model,model,332,"Hmm, empirically we've been able to handle larger input BAM than 20GB with less RAM you had. . Another known issue (which we will fix in the next release) is that if auxiliary fields are being read in, and if the BAM has many of them, it could unnecessarily increase the RAM usage. However, in this case given that you're using WGS model (which by default isn't parsing aux fields), that shouldn't be an issue... @kirti141 Another question for you - is there a BAM file that you can make public (with no sensitive information, of course) that we can attempt to reproduce this issue? Thanks again for reporting this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:488,integrability,pub,public,488,"Hmm, empirically we've been able to handle larger input BAM than 20GB with less RAM you had. . Another known issue (which we will fix in the next release) is that if auxiliary fields are being read in, and if the BAM has many of them, it could unnecessarily increase the RAM usage. However, in this case given that you're using WGS model (which by default isn't parsing aux fields), that shouldn't be an issue... @kirti141 Another question for you - is there a BAM file that you can make public (with no sensitive information, of course) that we can attempt to reproduce this issue? Thanks again for reporting this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:50,safety,input,input,50,"Hmm, empirically we've been able to handle larger input BAM than 20GB with less RAM you had. . Another known issue (which we will fix in the next release) is that if auxiliary fields are being read in, and if the BAM has many of them, it could unnecessarily increase the RAM usage. However, in this case given that you're using WGS model (which by default isn't parsing aux fields), that shouldn't be an issue... @kirti141 Another question for you - is there a BAM file that you can make public (with no sensitive information, of course) that we can attempt to reproduce this issue? Thanks again for reporting this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:332,security,model,model,332,"Hmm, empirically we've been able to handle larger input BAM than 20GB with less RAM you had. . Another known issue (which we will fix in the next release) is that if auxiliary fields are being read in, and if the BAM has many of them, it could unnecessarily increase the RAM usage. However, in this case given that you're using WGS model (which by default isn't parsing aux fields), that shouldn't be an issue... @kirti141 Another question for you - is there a BAM file that you can make public (with no sensitive information, of course) that we can attempt to reproduce this issue? Thanks again for reporting this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:50,usability,input,input,50,"Hmm, empirically we've been able to handle larger input BAM than 20GB with less RAM you had. . Another known issue (which we will fix in the next release) is that if auxiliary fields are being read in, and if the BAM has many of them, it could unnecessarily increase the RAM usage. However, in this case given that you're using WGS model (which by default isn't parsing aux fields), that shouldn't be an issue... @kirti141 Another question for you - is there a BAM file that you can make public (with no sensitive information, of course) that we can attempt to reproduce this issue? Thanks again for reporting this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:37,testability,coverag,coverage,37,One quick question: How much is your coverage?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:293,deployability,releas,release,293,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:488,energy efficiency,model,model,488,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:685,integrability,pub,public,685,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:191,safety,input,input,191,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:488,security,model,model,488,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:1075,security,auth,auth,1075,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:1133,security,Triag,Triage,1133,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:53,testability,Coverag,Coverage,53,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:191,usability,input,input,191,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with. > less RAM you had. >. > Another known issue (which we will fix in the next release) is that if. > auxiliary fields are being read in, and if the BAM has many of them, it. > could unnecessarily increase the RAM usage. However, in this case given. > that you're using WGS model (which by default isn't parsing aux fields),. > that shouldn't be an issue... >. > @kirti141 <https://github.com/kirti141> Another question for you - is. > there a BAM file that you can make public (with no sensitive information,. > of course) that we can attempt to reproduce this issue? Thanks again for. > reporting this. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:510,deployability,releas,release,510,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:708,energy efficiency,model,model,708,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:41,integrability,pub,public,41,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:107,integrability,pub,public,107,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:909,integrability,pub,public,909,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:126,safety,test,test,126,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:405,safety,input,input,405,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:708,security,model,model,708,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:1308,security,auth,auth,1308,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:1368,security,Triag,Triage,1368,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:126,testability,test,test,126,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:256,testability,Coverag,Coverage,256,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:60,usability,help,help,60,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:405,usability,input,input,405,"I have no problem in making my .bam file public, but please help me where. to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM. >. > Coverage is 46x. >. > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>. > wrote:. >. >> Hmm, empirically we've been able to handle larger input BAM than 20GB. >> with less RAM you had. >>. >> Another known issue (which we will fix in the next release) is that if. >> auxiliary fields are being read in, and if the BAM has many of them, it. >> could unnecessarily increase the RAM usage. However, in this case given. >> that you're using WGS model (which by default isn't parsing aux fields),. >> that shouldn't be an issue... >>. >> @kirti141 <https://github.com/kirti141> Another question for you - is. >> there a BAM file that you can make public (with no sensitive information,. >> of course) that we can attempt to reproduce this issue? Thanks again for. >> reporting this. >>. >> . >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,. >> or unsubscribe. >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>. >> . >> Triage notifications on the go with GitHub Mobile for iOS. >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. >> or Android. >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >>. >>. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:394,energy efficiency,core,cores,394,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. ```. on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:64,safety,compl,complicated,64,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. ```. on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:252,safety,test,testdata,252,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. ```. on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:346,safety,test,testdata,346,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. ```. on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:64,security,compl,complicated,64,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. ```. on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:252,testability,test,testdata,252,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. ```. on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:346,testability,test,testdata,346,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:. ```. $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. ```. on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:656,energy efficiency,core,cores,656,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:22,safety,test,test,22,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:316,safety,compl,complicated,316,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:512,safety,test,testdata,512,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:608,safety,test,testdata,608,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:316,security,compl,complicated,316,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:1026,security,auth,auth,1026,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:1084,security,Triag,Triage,1084,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:22,testability,test,test,22,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:512,testability,test,testdata,512,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:608,testability,test,testdata,608,"It might work on your test data, but not working on all my samples...I. tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>. wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing). > turns out to be more complicated than I thought. I'll have to think about. > what's a best way for this. >. > For now, I can try to run DV 1.2 on this large BAM file:. >. > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam. >. > on a 64 cores CentOS machine, to see if I can reproduce the issue. >. > I'll report back after I have a chance to run it. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>. > . > Triage notifications on the go with GitHub Mobile for iOS. > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>. > or Android. > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. >. >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:115,usability,close,close,115,"Hi @kirti141 , if you have any follow ups on suggestions to reproduce this issue, please let us know. For now I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/483:316,integrability,protocol,protocol,316,"It might have something to do with the `--regions`. Can you try just running on just one small region, something like '--regions=""chr20:10000000-10010000""`? What was the bedtools command you used for making that bed file? The bed file to use for `--regions` in a WES run should be the exome regions from the capture protocol.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:316,interoperability,protocol,protocol,316,"It might have something to do with the `--regions`. Can you try just running on just one small region, something like '--regions=""chr20:10000000-10010000""`? What was the bedtools command you used for making that bed file? The bed file to use for `--regions` in a WES run should be the exome regions from the capture protocol.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:179,usability,command,command,179,"It might have something to do with the `--regions`. Can you try just running on just one small region, something like '--regions=""chr20:10000000-10010000""`? What was the bedtools command you used for making that bed file? The bed file to use for `--regions` in a WES run should be the exome regions from the capture protocol.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:721,availability,error,error,721,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:748,deployability,fail,failed,748,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:631,integrability,protocol,protocol,631,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:243,interoperability,specif,specific,243,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:631,interoperability,protocol,protocol,631,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:721,performance,error,error,721,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:729,performance,parallel,parallel,729,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:748,reliability,fail,failed,748,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:721,safety,error,error,721,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:866,safety,input,input,866,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1014,safety,input,input,1014,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:64,usability,person,personally,64,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:116,usability,help,help,116,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:371,usability,command,command,371,"Hi Maria,. I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :). Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:. bedtools bamtobed [OPTIONS] -i <BAM>. It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me. (""idt_capture_novogene.grch38.bed""). Again I get the error:. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
