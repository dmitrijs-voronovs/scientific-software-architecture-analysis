id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/root-project/root/pull/2154:537,performance,error,error,537,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:802,performance,failur,failures,802,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:935,performance,error,error,935,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:29,reliability,fail,failures,29,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:295,reliability,Fail,Failures,295,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:381,reliability,doe,doesn,381,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:802,reliability,fail,failures,802,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:912,reliability,fail,fails,912,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:0,safety,Test,Testing,0,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:24,safety,test,test,24,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:199,safety,test,testReport,199,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:483,safety,depend,dependency,483,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:537,safety,error,error,537,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:701,safety,depend,depending,701,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:935,safety,error,error,935,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:0,testability,Test,Testing,0,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:24,testability,test,test,24,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:199,testability,test,testReport,199,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:483,testability,depend,dependency,483,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:701,testability,depend,depending,701,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:537,usability,error,error,537,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:731,usability,stop,stopped,731,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2154:935,usability,error,error,935,"Testing jenkins; We had test failures in runtime nightlies such as this one:. https://epsft-jenkins.cern.ch/view/ROOT/job/root-nightly-runtime-cxxmodules/95/BUILDTYPE=Debug,COMPILER=gcc62,LABEL=slc6/testReport/junit/projectroot.roottest.root.math/smatrix/roottest_root_math_smatrix_testKalman/. Failures were due to what @pcanal commented in #2135, that some so files in. roottest doesn't have external linkage. (It means that if you call. dlopen(libfoo.so), linux kernel can't find dependency libraries and it. emits ""undefined symbol"" error when they try to initialize global. variables in libfoo.so but couldn't find symbol definition). With pch, rootmap files were providing information about the depending library. However we stopped generating rootmap files in #2127 and that's why we. got these failures. To fix this issue, I implemented a callback to. TCling which gets called when DynamicLibraryManager fails with. ""undefined error"". I'm open to suggestion especially in DynamicLibraryManager.cpp.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2154
https://github.com/root-project/root/pull/2155:20,deployability,build,build,20,"Respect 'genvector' build option; We have the option since a long time, but it was totally ignored until now.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2155
https://github.com/root-project/root/pull/2155:66,performance,time,time,66,"Respect 'genvector' build option; We have the option since a long time, but it was totally ignored until now.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2155
https://github.com/root-project/root/pull/2157:435,availability,down,down,435,"[TREEPROC] Refactoring: move dataset info to TTreeProcessorMT ; Previously all the dataset information was stored in the thread-local. TTreeViews, causing data duplication between threads and requiring. that TTreeProcessorMT queries it when it needs part of it. Now TTreeProcessorMT stores all the dataset information (tree name,. filenames, friend names, friend file names, entry list) as constant. thread-global data which is passed down to TTreeView::GetTreeReader.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2157
https://github.com/root-project/root/pull/2157:11,modifiability,Refact,Refactoring,11,"[TREEPROC] Refactoring: move dataset info to TTreeProcessorMT ; Previously all the dataset information was stored in the thread-local. TTreeViews, causing data duplication between threads and requiring. that TTreeProcessorMT queries it when it needs part of it. Now TTreeProcessorMT stores all the dataset information (tree name,. filenames, friend names, friend file names, entry list) as constant. thread-global data which is passed down to TTreeView::GetTreeReader.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2157
https://github.com/root-project/root/pull/2157:11,performance,Refactor,Refactoring,11,"[TREEPROC] Refactoring: move dataset info to TTreeProcessorMT ; Previously all the dataset information was stored in the thread-local. TTreeViews, causing data duplication between threads and requiring. that TTreeProcessorMT queries it when it needs part of it. Now TTreeProcessorMT stores all the dataset information (tree name,. filenames, friend names, friend file names, entry list) as constant. thread-global data which is passed down to TTreeView::GetTreeReader.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2157
https://github.com/root-project/root/pull/2158:53,availability,failur,failure,53,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:989,availability,failur,failure,989,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1621,availability,down,down,1621,"ed_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. I.e. the major difference is that we have the shared_ptr constructor (wh",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:3014,availability,state,statement,3014,"wn are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. I.e. the major difference is that we have the shared_ptr constructor (which contains std::forward) being interpreted (rather than compiled) and calling a compiled function (the base class constructor) that takes (is given) an std::forward. It seems that the parameter and then mangled/placed-wrong in that case. If I recall correctly this is a know ABI incompatibility between clang and gcc; especially (only?) if it involves temporary. Splitting the line in two statement, works around the problem:. {code:c++}. auto rlm_ptr = std::make_shared<RLoopManager>(nullptr, validCols);. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(rlm_ptr);. {code}. In my case, it solves the problem because more of the '2nd' make_shared called are inlined by the compiler (when generating code for the 1st step), including the __shared_ptr constructor ... forcing the interpreter to generate the code and hence side-stepping the issue. [And if the std::forward issue is limited to temporary it would side-step the issue even if the __shared_ptr constructor was still outline.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:53,deployability,fail,failure,53,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:164,deployability,fail,failing,164,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:810,deployability,contain,contains,810,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:989,deployability,fail,failure,989,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1082,deployability,version,version,1082,"ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl call",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1124,deployability,version,version,1124,"we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::R",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1481,deployability,stack,stack,1481,"hared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:2627,deployability,contain,contains,2627,"wn are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. I.e. the major difference is that we have the shared_ptr constructor (which contains std::forward) being interpreted (rather than compiled) and calling a compiled function (the base class constructor) that takes (is given) an std::forward. It seems that the parameter and then mangled/placed-wrong in that case. If I recall correctly this is a know ABI incompatibility between clang and gcc; especially (only?) if it involves temporary. Splitting the line in two statement, works around the problem:. {code:c++}. auto rlm_ptr = std::make_shared<RLoopManager>(nullptr, validCols);. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(rlm_ptr);. {code}. In my case, it solves the problem because more of the '2nd' make_shared called are inlined by the compiler (when generating code for the 1st step), including the __shared_ptr constructor ... forcing the interpreter to generate the code and hence side-stepping the issue. [And if the std::forward issue is limited to temporary it would side-step the issue even if the __shared_ptr constructor was still outline.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:45,energy efficiency,current,current,45,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1047,energy efficiency,optim,optimization,1047,"failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1284,energy efficiency,optim,optimization,1284,"lptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopM",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:333,integrability,compon,component,333,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1082,integrability,version,version,1082,"ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl call",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1124,integrability,version,version,1124,"we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::R",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1609,integrability,rout,routine,1609,". : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. I.e. the major difference is that we have the shared_ptr const",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:3014,integrability,state,statement,3014,"wn are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. I.e. the major difference is that we have the shared_ptr constructor (which contains std::forward) being interpreted (rather than compiled) and calling a compiled function (the base class constructor) that takes (is given) an std::forward. It seems that the parameter and then mangled/placed-wrong in that case. If I recall correctly this is a know ABI incompatibility between clang and gcc; especially (only?) if it involves temporary. Splitting the line in two statement, works around the problem:. {code:c++}. auto rlm_ptr = std::make_shared<RLoopManager>(nullptr, validCols);. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(rlm_ptr);. {code}. In my case, it solves the problem because more of the '2nd' make_shared called are inlined by the compiler (when generating code for the 1st step), including the __shared_ptr constructor ... forcing the interpreter to generate the code and hence side-stepping the issue. [And if the std::forward issue is limited to temporary it would side-step the issue even if the __shared_ptr constructor was still outline.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:90,interoperability,incompatib,incompatibility,90,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:333,interoperability,compon,component,333,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:450,interoperability,standard,standard,450,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:2904,interoperability,incompatib,incompatibility,2904,"wn are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. I.e. the major difference is that we have the shared_ptr constructor (which contains std::forward) being interpreted (rather than compiled) and calling a compiled function (the base class constructor) that takes (is given) an std::forward. It seems that the parameter and then mangled/placed-wrong in that case. If I recall correctly this is a know ABI incompatibility between clang and gcc; especially (only?) if it involves temporary. Splitting the line in two statement, works around the problem:. {code:c++}. auto rlm_ptr = std::make_shared<RLoopManager>(nullptr, validCols);. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(rlm_ptr);. {code}. In my case, it solves the problem because more of the '2nd' make_shared called are inlined by the compiler (when generating code for the 1st step), including the __shared_ptr constructor ... forcing the interpreter to generate the code and hence side-stepping the issue. [And if the std::forward issue is limited to temporary it would side-step the issue even if the __shared_ptr constructor was still outline.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:333,modifiability,compon,component,333,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1082,modifiability,version,version,1082,"ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl call",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1124,modifiability,version,version,1124,"we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::R",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:2809,modifiability,paramet,parameter,2809,"wn are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. I.e. the major difference is that we have the shared_ptr constructor (which contains std::forward) being interpreted (rather than compiled) and calling a compiled function (the base class constructor) that takes (is given) an std::forward. It seems that the parameter and then mangled/placed-wrong in that case. If I recall correctly this is a know ABI incompatibility between clang and gcc; especially (only?) if it involves temporary. Splitting the line in two statement, works around the problem:. {code:c++}. auto rlm_ptr = std::make_shared<RLoopManager>(nullptr, validCols);. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(rlm_ptr);. {code}. In my case, it solves the problem because more of the '2nd' make_shared called are inlined by the compiler (when generating code for the 1st step), including the __shared_ptr constructor ... forcing the interpreter to generate the code and hence side-stepping the issue. [And if the std::forward issue is limited to temporary it would side-step the issue even if the __shared_ptr constructor was still outline.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:53,performance,failur,failure,53,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:989,performance,failur,failure,989,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1047,performance,optimiz,optimization,1047,"failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1284,performance,optimiz,optimization,1284,"lptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopM",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:53,reliability,fail,failure,53,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:164,reliability,fail,failing,164,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:989,reliability,fail,failure,989,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:296,safety,valid,validCols,296,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:736,safety,test,test,736,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1025,safety,test,test,1025,"t, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1259,safety,test,test,1259,"make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compi",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:3119,safety,valid,validCols,3119,"wn are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. interp std::make_shared<ROOT::RDF::RInterface …. interp std::allocate_shared<ROOT::RDF::RInterface …. interp std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. I.e. the major difference is that we have the shared_ptr constructor (which contains std::forward) being interpreted (rather than compiled) and calling a compiled function (the base class constructor) that takes (is given) an std::forward. It seems that the parameter and then mangled/placed-wrong in that case. If I recall correctly this is a know ABI incompatibility between clang and gcc; especially (only?) if it involves temporary. Splitting the line in two statement, works around the problem:. {code:c++}. auto rlm_ptr = std::make_shared<RLoopManager>(nullptr, validCols);. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(rlm_ptr);. {code}. In my case, it solves the problem because more of the '2nd' make_shared called are inlined by the compiler (when generating code for the 1st step), including the __shared_ptr constructor ... forcing the interpreter to generate the code and hence side-stepping the issue. [And if the std::forward issue is limited to temporary it would side-step the issue even if the __shared_ptr constructor was still outline.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:736,testability,test,test,736,"Fix RDF interpreted snapshot, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1025,testability,test,test,1025,"t, ROOT-9236; The current failure in snapshot is due to an ABI incompatibility (or so it seems) that we may have encountered before. The failing line is:. {code:c++}. auto snapshotRDF = std::make_shared<RInterface<RLoopManager>>(std::make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2158:1259,testability,test,test,1259,"make_shared<RLoopManager>(nullptr, validCols));. {code}. and one of the component of make_shared is the call to the following shared_ptr constructor. {code:c++}. // This constructor is non-standard, it is used by allocate_shared. template<typename _Alloc, typename... _Args>. shared_ptr(_Sp_make_shared_tag __tag, const _Alloc& __a,. _Args&&... __args). : __shared_ptr<_Tp>(__tag, __a, std::forward<_Args>(__args)...). { }. {code}. Note the use of std::forward. The snapshot test has 2 steps:. 1. Do the work, including a call to SnapshotImpl which contains the problem line, with only compiled code. 2. Do the same work relying on interpreted code, in which case SnapshotImpl is called via the interpreter. The symptoms of the failure is that **if** the snapshot test is compiled with optimization then. a. The compiled version works fine. b. In the interpreted version the constructor of the RInterface is wrong because the shared_ptr its constructor sees is **not** initialized. If the snapshot test is compiled without optimization then both steps succeeds. The main difference between the two is the amount of fully realized (i.e. non-inlined) functions emitted by the compiler. In the success fully case we have a stack like. {code}. interp SnapshotImpl calls. compiled std::make_shared<ROOT::Detail::RDF::RLoopManager … [in debug mode. this routine and down are used compiled]. compiled std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all compiled. compiled ROOT::Detail::RDF::RLoopManager::RLoopManager. compiled std::make_shared<ROOT::RDF::RInterface …. compiled std::allocate_shared<ROOT::RDF::RInterface …. compiled std::shared_ptr< ROOT::RDF::RInterface. compiled std::__shared_ptr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager. {code}. {code}. interp SnapshotImpl calls. interp std::make_shared<ROOT::Detail::RDF::RLoopManager …. interp std::allocate_shared<ROOT::Detail::RDF::RLoopManager. ... implementation details .. all interpreted. compi",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2158
https://github.com/root-project/root/pull/2159:20,deployability,build,build,20,[TREEPROCMT] Do not build test if not an imt build; Sorry!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2159
https://github.com/root-project/root/pull/2159:45,deployability,build,build,45,[TREEPROCMT] Do not build test if not an imt build; Sorry!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2159
https://github.com/root-project/root/pull/2159:26,safety,test,test,26,[TREEPROCMT] Do not build test if not an imt build; Sorry!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2159
https://github.com/root-project/root/pull/2159:26,testability,test,test,26,[TREEPROCMT] Do not build test if not an imt build; Sorry!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2159
https://github.com/root-project/root/pull/2160:304,availability,failur,failures,304,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:36,deployability,modul,module,36,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:304,deployability,fail,failures,304,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:403,deployability,build,build,403,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:848,energy efficiency,load,loaded,848,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:896,energy efficiency,load,load,896,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:36,modifiability,modul,module,36,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:43,performance,cach,cache,43,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:304,performance,failur,failures,304,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:848,performance,load,loaded,848,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:896,performance,load,load,896,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:304,reliability,fail,failures,304,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:36,safety,modul,module,36,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:415,safety,test,testReport,415,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:415,testability,test,testReport,415,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:193,usability,close,closed,193,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2160:795,usability,behavi,behavior,795,"Revert ""Add the cwg to the prebuilt module cache path.""; This reverts commit 5298b418eec4129351888f41cb7c3bfc90161e22. This commit was mistakenly committed. PR was opened in #1730, but it was. closed and moved to #1761. I didn't notice this and created another PR. in #1980. This change was causing 100+ failures in runtime cxxmodules nightlies. (Eg. https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29183/testReport/junit/projectroot/runtutorials/tutorial_fit_FittingDemo/). We want to have **proper** PrebuildModulesPaths which information were. extracted from LD_LIBRARY_PATH and DYLD_LIBRARY_PATH, not a random ""."". Because of this commit, we were trying to autoload libraries generated. by roottest on-demand (for example ""./h1analysisTreeReader_C.so"") This. is not an intentional behavior, these autogenerated libraries are. already loaded by roottest and what we want to do is to load **proper**. libraries like libHist.so instead.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2160
https://github.com/root-project/root/pull/2161:239,deployability,build,build,239,"[WIP] Experimental PyROOT; This branch creates an experimental PyROOT directory in ROOT. * Reuses modern Cppyy code *with no changes*. * Enables to progressively add ROOT pythonizations on top. **Note: cppyy requires C++14, so it will not build with GCC 4.9**",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2161
https://github.com/root-project/root/pull/2161:91,modifiability,Reu,Reuses,91,"[WIP] Experimental PyROOT; This branch creates an experimental PyROOT directory in ROOT. * Reuses modern Cppyy code *with no changes*. * Enables to progressively add ROOT pythonizations on top. **Note: cppyy requires C++14, so it will not build with GCC 4.9**",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2161
https://github.com/root-project/root/pull/2161:148,usability,progress,progressively,148,"[WIP] Experimental PyROOT; This branch creates an experimental PyROOT directory in ROOT. * Reuses modern Cppyy code *with no changes*. * Enables to progressively add ROOT pythonizations on top. **Note: cppyy requires C++14, so it will not build with GCC 4.9**",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2161
https://github.com/root-project/root/pull/2162:39,deployability,fail,fails,39,Fix execUnloading test which sometimes fails with centos7/gcc7; https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29347/testReport/projectroot.roottest.root/meta/roottest_root_meta_execUnloading_auto/. TCling::GetDataMember is triggering deserialization. I think. we had RAII here in the past but maybe it was accidentaly deleted?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2162
https://github.com/root-project/root/pull/2162:116,deployability,build,build,116,Fix execUnloading test which sometimes fails with centos7/gcc7; https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29347/testReport/projectroot.roottest.root/meta/roottest_root_meta_execUnloading_auto/. TCling::GetDataMember is triggering deserialization. I think. we had RAII here in the past but maybe it was accidentaly deleted?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2162
https://github.com/root-project/root/pull/2162:39,reliability,fail,fails,39,Fix execUnloading test which sometimes fails with centos7/gcc7; https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29347/testReport/projectroot.roottest.root/meta/roottest_root_meta_execUnloading_auto/. TCling::GetDataMember is triggering deserialization. I think. we had RAII here in the past but maybe it was accidentaly deleted?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2162
https://github.com/root-project/root/pull/2162:18,safety,test,test,18,Fix execUnloading test which sometimes fails with centos7/gcc7; https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29347/testReport/projectroot.roottest.root/meta/roottest_root_meta_execUnloading_auto/. TCling::GetDataMember is triggering deserialization. I think. we had RAII here in the past but maybe it was accidentaly deleted?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2162
https://github.com/root-project/root/pull/2162:128,safety,test,testReport,128,Fix execUnloading test which sometimes fails with centos7/gcc7; https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29347/testReport/projectroot.roottest.root/meta/roottest_root_meta_execUnloading_auto/. TCling::GetDataMember is triggering deserialization. I think. we had RAII here in the past but maybe it was accidentaly deleted?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2162
https://github.com/root-project/root/pull/2162:318,safety,accid,accidentaly,318,Fix execUnloading test which sometimes fails with centos7/gcc7; https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29347/testReport/projectroot.roottest.root/meta/roottest_root_meta_execUnloading_auto/. TCling::GetDataMember is triggering deserialization. I think. we had RAII here in the past but maybe it was accidentaly deleted?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2162
https://github.com/root-project/root/pull/2162:18,testability,test,test,18,Fix execUnloading test which sometimes fails with centos7/gcc7; https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29347/testReport/projectroot.roottest.root/meta/roottest_root_meta_execUnloading_auto/. TCling::GetDataMember is triggering deserialization. I think. we had RAII here in the past but maybe it was accidentaly deleted?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2162
https://github.com/root-project/root/pull/2162:128,testability,test,testReport,128,Fix execUnloading test which sometimes fails with centos7/gcc7; https://epsft-jenkins.cern.ch/job/root-pullrequests-build/29347/testReport/projectroot.roottest.root/meta/roottest_root_meta_execUnloading_auto/. TCling::GetDataMember is triggering deserialization. I think. we had RAII here in the past but maybe it was accidentaly deleted?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2162
https://github.com/root-project/root/pull/2164:20,performance,parallel,parallel,20,[TREE] Only trigger parallel GetEntry if number of branches > 1;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2164
https://github.com/root-project/root/pull/2165:29,usability,custom,custom,29,[DF] Fix Snapshot of aliased custom columns; Fixes ROOT-9452,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2165
https://github.com/root-project/root/pull/2166:29,usability,custom,custom,29,[DF] Fix Snapshot of aliased custom columns; Fixes ROOT-9452,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2166
https://github.com/root-project/root/pull/2167:36,deployability,modul,modules,36,"Fix TROOT::LoadClass for C++runtime modules; Before, it was loading the library which doesn't exists(by checking gSystem->DynamicPathName). It made no sence. This also fix our runtime cxxmodules.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2167
https://github.com/root-project/root/pull/2167:11,energy efficiency,Load,LoadClass,11,"Fix TROOT::LoadClass for C++runtime modules; Before, it was loading the library which doesn't exists(by checking gSystem->DynamicPathName). It made no sence. This also fix our runtime cxxmodules.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2167
https://github.com/root-project/root/pull/2167:60,energy efficiency,load,loading,60,"Fix TROOT::LoadClass for C++runtime modules; Before, it was loading the library which doesn't exists(by checking gSystem->DynamicPathName). It made no sence. This also fix our runtime cxxmodules.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2167
https://github.com/root-project/root/pull/2167:36,modifiability,modul,modules,36,"Fix TROOT::LoadClass for C++runtime modules; Before, it was loading the library which doesn't exists(by checking gSystem->DynamicPathName). It made no sence. This also fix our runtime cxxmodules.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2167
https://github.com/root-project/root/pull/2167:11,performance,Load,LoadClass,11,"Fix TROOT::LoadClass for C++runtime modules; Before, it was loading the library which doesn't exists(by checking gSystem->DynamicPathName). It made no sence. This also fix our runtime cxxmodules.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2167
https://github.com/root-project/root/pull/2167:60,performance,load,loading,60,"Fix TROOT::LoadClass for C++runtime modules; Before, it was loading the library which doesn't exists(by checking gSystem->DynamicPathName). It made no sence. This also fix our runtime cxxmodules.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2167
https://github.com/root-project/root/pull/2167:86,reliability,doe,doesn,86,"Fix TROOT::LoadClass for C++runtime modules; Before, it was loading the library which doesn't exists(by checking gSystem->DynamicPathName). It made no sence. This also fix our runtime cxxmodules.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2167
https://github.com/root-project/root/pull/2167:36,safety,modul,modules,36,"Fix TROOT::LoadClass for C++runtime modules; Before, it was loading the library which doesn't exists(by checking gSystem->DynamicPathName). It made no sence. This also fix our runtime cxxmodules.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2167
https://github.com/root-project/root/pull/2169:13,safety,Avoid,Avoid,13,[TREEPROCMT] Avoid useless construction of thread-local chains;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2169
https://github.com/root-project/root/pull/2171:0,deployability,Releas,Release,0,Release notes.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2171
https://github.com/root-project/root/pull/2172:0,deployability,Releas,Release,0,Release notes.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2172
https://github.com/root-project/root/pull/2173:17,availability,state,state,17,Testing jenkings state.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2173
https://github.com/root-project/root/pull/2173:17,integrability,state,state,17,Testing jenkings state.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2173
https://github.com/root-project/root/pull/2173:0,safety,Test,Testing,0,Testing jenkings state.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2173
https://github.com/root-project/root/pull/2173:0,testability,Test,Testing,0,Testing jenkings state.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2173
https://github.com/root-project/root/pull/2174:93,availability,error,error,93,"[TREEPROCMT] Explicitly initialize fFriendInfo in ctors; This should fix clang's compilation error:. ""constructor for 'ROOT::TTreeProcessorMT' must explicitly initialize the const member 'fFriendInfo'""",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2174
https://github.com/root-project/root/pull/2174:93,performance,error,error,93,"[TREEPROCMT] Explicitly initialize fFriendInfo in ctors; This should fix clang's compilation error:. ""constructor for 'ROOT::TTreeProcessorMT' must explicitly initialize the const member 'fFriendInfo'""",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2174
https://github.com/root-project/root/pull/2174:93,safety,error,error,93,"[TREEPROCMT] Explicitly initialize fFriendInfo in ctors; This should fix clang's compilation error:. ""constructor for 'ROOT::TTreeProcessorMT' must explicitly initialize the const member 'fFriendInfo'""",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2174
https://github.com/root-project/root/pull/2174:93,usability,error,error,93,"[TREEPROCMT] Explicitly initialize fFriendInfo in ctors; This should fix clang's compilation error:. ""constructor for 'ROOT::TTreeProcessorMT' must explicitly initialize the const member 'fFriendInfo'""",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2174
https://github.com/root-project/root/pull/2175:536,deployability,version,version,536,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:532,energy efficiency,GPU,GPU,532,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:536,integrability,version,version,536,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:8,modifiability,layer,layer,8,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:48,modifiability,Layer,Layer,48,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:269,modifiability,refact,refactored,269,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:536,modifiability,version,version,536,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:269,performance,refactor,refactored,269,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:532,performance,GPU,GPU,532,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:93,safety,test,test,93,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:110,safety,Test,Tests,110,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:246,safety,test,testing,246,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:359,safety,test,tests,359,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:365,security,expos,exposed,365,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:93,testability,test,test,93,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:98,testability,coverag,coverage,98,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:110,testability,Test,Tests,110,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:246,testability,test,testing,246,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2175:359,testability,test,tests,359,"Pooling layer; This PR enhances the Max Pooling Layer in the following ways:. 1. **Increased test coverage.** Tests have been added for the backwards propagation, covering the cases where `depth > 1`, as well as overlapping receptive fields. The testing suite was also refactored using templates to eliminate code duplication. 2. **Bug Fix**. The newly added tests exposed a bug in the `Reference` implementation of back-propagation, in the case of overlapping receptive fields. The bug is resolved. 3. **CUDA implementation.** The GPU version is now fully functional in both forward and backward propagation.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2175
https://github.com/root-project/root/pull/2176:220,deployability,automat,automatically,220,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2176:79,energy efficiency,model,model,79,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2176:482,integrability,filter,filters,482,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2176:522,integrability,event,event,522,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2176:121,interoperability,share,share,121,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2176:79,security,model,model,79,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2176:220,testability,automat,automatically,220,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2176:39,usability,progress,progress,39,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2176:282,usability,user,users,282,"[WIP][DF] Better df ownership; Work in progress on changing internal ownership model for RDataFrame: children nodes will share ownership of their parent node in the computation graph. Advantages:. * ""dead"" branches will automatically delete themselves from the computation graph. * users don't have to take care of keeping the head node in scope anymore. Disadvantages:. * nodes' dtors will have to deregister them from the RLoopManager, which will still have to keep lists of e.g. filters and actions in order to run the event loop correctly",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2176
https://github.com/root-project/root/pull/2179:46,integrability,inject,injecting,46,[IO] Backport hashing of StreamerInfos before injecting them in the type system; Skip the processing of streamer infos if those have been read before. This has been working for a week in master.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2179
https://github.com/root-project/root/pull/2179:14,security,hash,hashing,14,[IO] Backport hashing of StreamerInfos before injecting them in the type system; Skip the processing of streamer infos if those have been read before. This has been working for a week in master.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2179
https://github.com/root-project/root/pull/2179:46,security,inject,injecting,46,[IO] Backport hashing of StreamerInfos before injecting them in the type system; Skip the processing of streamer infos if those have been read before. This has been working for a week in master.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2179
https://github.com/root-project/root/pull/2180:0,deployability,Updat,Update,0,Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2180
https://github.com/root-project/root/pull/2180:7,deployability,releas,release,7,Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2180
https://github.com/root-project/root/pull/2180:0,safety,Updat,Update,0,Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2180
https://github.com/root-project/root/pull/2180:0,security,Updat,Update,0,Update release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2180
https://github.com/root-project/root/pull/2182:86,availability,operat,operations,86,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:394,availability,down,down,394,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1516,availability,down,downside,1516,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1781,availability,slo,slow,1781,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:267,deployability,depend,depending,267,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:259,integrability,Filter,Filter,259,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:267,integrability,depend,depending,267,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:539,integrability,Filter,Filter,539,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:765,integrability,interfac,interface,765,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1305,integrability,Filter,Filter,1305,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1321,integrability,filter,filters,1321,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1642,integrability,event,event,1642,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:765,interoperability,interfac,interface,765,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:267,modifiability,depend,depending,267,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:765,modifiability,interfac,interface,765,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:122,performance,perform,perform,122,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1781,reliability,slo,slow,1781,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:267,safety,depend,depending,267,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:702,safety,except,except,702,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:744,safety,compl,completely,744,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1745,safety,compl,complex,1745,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:744,security,compl,completely,744,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1745,security,compl,complex,1745,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:267,testability,depend,depending,267,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1449,testability,simpl,simplified,1449,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:102,usability,user,users,102,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:122,usability,perform,perform,122,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:786,usability,user,users,786,"[DF] ROOT-9491 Add common base class to all node types; There are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1449,usability,simpl,simplified,1449,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2182:1576,usability,user,users,1576,"ere are a number of trivial operations that users often want to perform on dataframes that are surprisingly hard to get right, for example adding several `Define`s in a loop or conditionally adding a `Filter` depending on a runtime boolean (both use-cases are challenging in C++, trivial in python). The way I see it, difficulties boil down to the fact that different dataframe nodes have different types (because their types incorporate e.g. the type of the callable passed to a `Filter` and the type of their parent node in the computation graph). In this PR I propose to add a common base class `ROOT::RDF::RNode` to all nodes of the graph (except leaves a.k.a results, which have a completely different interface),. so that users can, for example:. * take any dataframe node by reference in non-template functions as `RNode&`. * `emplace_back` dataframe nodes in ~`std::vector<RNode>`~ `vector<RInterface<RNode>>`. * have non-const pointers to dataframe nodes. and so on. For example, conditionally adding a `Range` do a dataframe now looks like this:. ```c++. auto maybe_ranged = [&df, mustAddRange] { . return mustAddRange ? ROOT::RDF::RNode(d.Range(1)). : ROOT::RDF::RNode(d); . }(); . ```. while before this PR one would have to add fake `Filter(""true"")` filters to normalize the return type of the lambda, involving the interpreter for no reason. Internal `RDataFrame` code is also simplified by the introduction of this common base class. The only downside I can think of is that if this mechanism is abused users might end up with extra, unnecessary virtual calls in their event loop -- on the other hand, this mechanism should only be used in situations that required either complex template magic or dirty and slow tricks before. Questions:. * can we come up with a better name than `ROOT::RDF::ToCommonNodeType` for the function that upcasts any dataframe object to the same type? * should this cast only be explicit through an upcasting function call or should we allow implicit casts?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2182
https://github.com/root-project/root/pull/2183:5,availability,state,state,5,Test state of v614 branch; Because:. * 07a0da40df - (11 days ago) Add missing cling transaction — Philippe Canal (origin/v6-14-00-patches). Was inadvertently added directly.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2183
https://github.com/root-project/root/pull/2183:130,deployability,patch,patches,130,Test state of v614 branch; Because:. * 07a0da40df - (11 days ago) Add missing cling transaction — Philippe Canal (origin/v6-14-00-patches). Was inadvertently added directly.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2183
https://github.com/root-project/root/pull/2183:5,integrability,state,state,5,Test state of v614 branch; Because:. * 07a0da40df - (11 days ago) Add missing cling transaction — Philippe Canal (origin/v6-14-00-patches). Was inadvertently added directly.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2183
https://github.com/root-project/root/pull/2183:0,safety,Test,Test,0,Test state of v614 branch; Because:. * 07a0da40df - (11 days ago) Add missing cling transaction — Philippe Canal (origin/v6-14-00-patches). Was inadvertently added directly.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2183
https://github.com/root-project/root/pull/2183:130,safety,patch,patches,130,Test state of v614 branch; Because:. * 07a0da40df - (11 days ago) Add missing cling transaction — Philippe Canal (origin/v6-14-00-patches). Was inadvertently added directly.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2183
https://github.com/root-project/root/pull/2183:130,security,patch,patches,130,Test state of v614 branch; Because:. * 07a0da40df - (11 days ago) Add missing cling transaction — Philippe Canal (origin/v6-14-00-patches). Was inadvertently added directly.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2183
https://github.com/root-project/root/pull/2183:0,testability,Test,Test,0,Test state of v614 branch; Because:. * 07a0da40df - (11 days ago) Add missing cling transaction — Philippe Canal (origin/v6-14-00-patches). Was inadvertently added directly.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2183
https://github.com/root-project/root/pull/2184:0,safety,Avoid,Avoid,0,Avoid getting confused by out-of-range request to TTreeCache;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2184
https://github.com/root-project/root/pull/2186:207,integrability,transform,transformations,207,[VecOps][DF] Minimal vector<bool> support ; This PR introduces:. - A mechanism to forbid istantiation of RVec<bool>. - A mechanism to treat vector<bool> as such and not as RVec<bool> with jitted actions and transformations. - Tests,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2186
https://github.com/root-project/root/pull/2186:207,interoperability,transform,transformations,207,[VecOps][DF] Minimal vector<bool> support ; This PR introduces:. - A mechanism to forbid istantiation of RVec<bool>. - A mechanism to treat vector<bool> as such and not as RVec<bool> with jitted actions and transformations. - Tests,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2186
https://github.com/root-project/root/pull/2186:226,safety,Test,Tests,226,[VecOps][DF] Minimal vector<bool> support ; This PR introduces:. - A mechanism to forbid istantiation of RVec<bool>. - A mechanism to treat vector<bool> as such and not as RVec<bool> with jitted actions and transformations. - Tests,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2186
https://github.com/root-project/root/pull/2186:226,testability,Test,Tests,226,[VecOps][DF] Minimal vector<bool> support ; This PR introduces:. - A mechanism to forbid istantiation of RVec<bool>. - A mechanism to treat vector<bool> as such and not as RVec<bool> with jitted actions and transformations. - Tests,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2186
https://github.com/root-project/root/pull/2186:13,usability,Minim,Minimal,13,[VecOps][DF] Minimal vector<bool> support ; This PR introduces:. - A mechanism to forbid istantiation of RVec<bool>. - A mechanism to treat vector<bool> as such and not as RVec<bool> with jitted actions and transformations. - Tests,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2186
https://github.com/root-project/root/pull/2186:34,usability,support,support,34,[VecOps][DF] Minimal vector<bool> support ; This PR introduces:. - A mechanism to forbid istantiation of RVec<bool>. - A mechanism to treat vector<bool> as such and not as RVec<bool> with jitted actions and transformations. - Tests,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2186
https://github.com/root-project/root/pull/2187:638,availability,slo,slower,638,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:328,deployability,modul,modules,328,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:337,deployability,infrastructur,infrastructure,337,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:554,deployability,depend,dependency,554,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:715,deployability,patch,patch,715,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:554,integrability,depend,dependency,554,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:328,modifiability,modul,modules,328,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:554,modifiability,depend,dependency,554,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:638,reliability,slo,slower,638,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:328,safety,modul,modules,328,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:554,safety,depend,dependency,554,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:715,safety,patch,patch,715,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:715,security,patch,patch,715,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2187:554,testability,depend,dependency,554,"Revert ""[cxxmodules] Don't generate rootmap files with cxxmodules""; This reverts commit 8bb0a978a34e8f026a98642afe118e15d2356b6c. With ACLiC, which means if you do ""root.exe hsimple.C+"", ROOT generates. library for hsimple.C and execute this library instead of interpreting it at. runtime. This didn't work with our ""preloading modules"" infrastructure,. as it's not even interpreting. We can fix this by. 1. Adding NEEDED section when generating so files. This is like a ""static linker"" solution, which means we'll change. rootcling_impl to properly add dependency libraries. 2. Try to get callback from library. I think this makes ACLiC slower, so I like the 1st solution. However, for now, let's just revert this patch. edit:. https://gist.github.com/yamaguchi1024/644b7ee431fce460fb27c1402e92c903. https://gist.github.com/yamaguchi1024/d5a69666d1e10f0b2cfc176a07792420",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2187
https://github.com/root-project/root/pull/2188:9,safety,test,test,9,[IO] Add test for hashing of streamer info record + misc fixes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2188
https://github.com/root-project/root/pull/2188:18,security,hash,hashing,18,[IO] Add test for hashing of streamer info record + misc fixes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2188
https://github.com/root-project/root/pull/2188:9,testability,test,test,9,[IO] Add test for hashing of streamer info record + misc fixes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2188
https://github.com/root-project/root/pull/2189:84,availability,slo,slot,84,[RDF] Remove unused column in dataframe tutorial; It does not fail because `fHistos[slot]->Fill(valuesArr.data());` is valid with number of columns larger than dimension of `THN`.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2189
https://github.com/root-project/root/pull/2189:62,deployability,fail,fail,62,[RDF] Remove unused column in dataframe tutorial; It does not fail because `fHistos[slot]->Fill(valuesArr.data());` is valid with number of columns larger than dimension of `THN`.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2189
https://github.com/root-project/root/pull/2189:53,reliability,doe,does,53,[RDF] Remove unused column in dataframe tutorial; It does not fail because `fHistos[slot]->Fill(valuesArr.data());` is valid with number of columns larger than dimension of `THN`.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2189
https://github.com/root-project/root/pull/2189:62,reliability,fail,fail,62,[RDF] Remove unused column in dataframe tutorial; It does not fail because `fHistos[slot]->Fill(valuesArr.data());` is valid with number of columns larger than dimension of `THN`.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2189
https://github.com/root-project/root/pull/2189:84,reliability,slo,slot,84,[RDF] Remove unused column in dataframe tutorial; It does not fail because `fHistos[slot]->Fill(valuesArr.data());` is valid with number of columns larger than dimension of `THN`.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2189
https://github.com/root-project/root/pull/2189:119,safety,valid,valid,119,[RDF] Remove unused column in dataframe tutorial; It does not fail because `fHistos[slot]->Fill(valuesArr.data());` is valid with number of columns larger than dimension of `THN`.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2189
https://github.com/root-project/root/pull/2190:60,usability,custom,custom,60,[DF] Check number of template pars and columns when booking custom actions;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2190
https://github.com/root-project/root/pull/2193:197,usability,help,helpful,197,"[DF] ROOT-9468 - Add TColumnValue extern templates for common types; this mitigates ROOT-9468 speeding up considerably the instantiation of large templates, such as Snapshot of many columns. It is helpful in general too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2193
https://github.com/root-project/root/pull/2194:111,availability,failur,failures,111,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:9,deployability,patch,patch,9,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:111,deployability,fail,failures,111,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:131,deployability,modul,modules,131,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:131,modifiability,modul,modules,131,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:111,performance,failur,failures,111,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:216,performance,time,time,216,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:111,reliability,fail,failures,111,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:9,safety,patch,patch,9,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:131,safety,modul,modules,131,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2194:9,security,patch,patch,9,"Followup patch for #2097; Explicitly include cling/Interpreter/Runtim…; …ePrintValue.h. This fixes prettyprint failures in runtime modules. We need to include cling/Interpreter/RuntimePrintValue.h at. initialization time, as ClingPrintValue is calling cling::printValue. with various arguments which were overloaded in RuntimePrintValue.h.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2194
https://github.com/root-project/root/pull/2195:17,energy efficiency,Optim,Optimise,17,[DF] ROOT-9470 - Optimise GetBranchNames saving them in the RInterface;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2195
https://github.com/root-project/root/pull/2196:17,safety,test,test,17,[DF] Clean after test case;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2196
https://github.com/root-project/root/pull/2196:17,testability,test,test,17,[DF] Clean after test case;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2196
https://github.com/root-project/root/pull/2197:428,deployability,contain,contained,428,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:495,deployability,contain,contains,495,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:419,integrability,buffer,buffers,419,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:648,integrability,buffer,buffer,648,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:1008,integrability,messag,message,1008,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:1008,interoperability,messag,message,1008,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:157,modifiability,scenario,scenario,157,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:376,performance,content,content,376,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:863,reliability,doe,does,863,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:521,safety,avoid,avoid,521,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2197:316,usability,effectiv,effectively,316,"[DF] Fix ROOT-9471; With very few entries passing a given RDataFrame cutflow, it can happen. that a TBB task ends up processing zero ""good"" entries. In that scenario, it could happen that the first TTree header flushed to. the output file from a Snapshot action was for a tree with no entries and. no branches. This effectively set the number of branches to. zero for all the content written to file, even if following buffers. contained well-formed TTrees. With this commit, if the output tree contains zero entries, we avoid. flushing it to file. And since the next task will remove this TTree from. the output directory, we can be sure that the buffer with the malformed. TTree will never be written out. This fixes ROOT-9471. <hr>. The fix requires that output trees are deregistered from output directories at the end of a task, so that the output directory does not ever try to flush them. This is done by giving `SnapshotHelperMT` ownership of its output trees (see explanation in the relevant commit message).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2197
https://github.com/root-project/root/pull/2201:175,deployability,contain,containing,175,[cxxmodules] Evaluate code with interpreter in pretty print; This is an alternative implementation of #2194. Changed Pythonize to. call Evaluate instead of doing Calc on code containing. cling::printValue.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2201
https://github.com/root-project/root/pull/2202:115,deployability,build,build,115,"Parallelizing the TKDTree for better performance (with TThreadExecutor).; Parallel the TKDTree with TThread. . Now build TKDTree can be with parameter n=1,2,4... stands for number of threading (default 1)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2202
https://github.com/root-project/root/pull/2202:141,modifiability,paramet,parameter,141,"Parallelizing the TKDTree for better performance (with TThreadExecutor).; Parallel the TKDTree with TThread. . Now build TKDTree can be with parameter n=1,2,4... stands for number of threading (default 1)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2202
https://github.com/root-project/root/pull/2202:0,performance,Parallel,Parallelizing,0,"Parallelizing the TKDTree for better performance (with TThreadExecutor).; Parallel the TKDTree with TThread. . Now build TKDTree can be with parameter n=1,2,4... stands for number of threading (default 1)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2202
https://github.com/root-project/root/pull/2202:37,performance,perform,performance,37,"Parallelizing the TKDTree for better performance (with TThreadExecutor).; Parallel the TKDTree with TThread. . Now build TKDTree can be with parameter n=1,2,4... stands for number of threading (default 1)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2202
https://github.com/root-project/root/pull/2202:74,performance,Parallel,Parallel,74,"Parallelizing the TKDTree for better performance (with TThreadExecutor).; Parallel the TKDTree with TThread. . Now build TKDTree can be with parameter n=1,2,4... stands for number of threading (default 1)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2202
https://github.com/root-project/root/pull/2202:37,usability,perform,performance,37,"Parallelizing the TKDTree for better performance (with TThreadExecutor).; Parallel the TKDTree with TThread. . Now build TKDTree can be with parameter n=1,2,4... stands for number of threading (default 1)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2202
https://github.com/root-project/root/pull/2203:23,integrability,filter,filter,23,"[cxxmodules] Fix bloom filter without .gnu.hash; In some node where ROOT was using lagacy compiler, they didn't compile. shared library with a linker flag --hash-style=gnu. So libraries didn't. have .gnu.hash section. We need to search symbols for these libraries,. too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2203
https://github.com/root-project/root/pull/2203:121,interoperability,share,shared,121,"[cxxmodules] Fix bloom filter without .gnu.hash; In some node where ROOT was using lagacy compiler, they didn't compile. shared library with a linker flag --hash-style=gnu. So libraries didn't. have .gnu.hash section. We need to search symbols for these libraries,. too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2203
https://github.com/root-project/root/pull/2203:43,security,hash,hash,43,"[cxxmodules] Fix bloom filter without .gnu.hash; In some node where ROOT was using lagacy compiler, they didn't compile. shared library with a linker flag --hash-style=gnu. So libraries didn't. have .gnu.hash section. We need to search symbols for these libraries,. too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2203
https://github.com/root-project/root/pull/2203:157,security,hash,hash-style,157,"[cxxmodules] Fix bloom filter without .gnu.hash; In some node where ROOT was using lagacy compiler, they didn't compile. shared library with a linker flag --hash-style=gnu. So libraries didn't. have .gnu.hash section. We need to search symbols for these libraries,. too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2203
https://github.com/root-project/root/pull/2203:204,security,hash,hash,204,"[cxxmodules] Fix bloom filter without .gnu.hash; In some node where ROOT was using lagacy compiler, they didn't compile. shared library with a linker flag --hash-style=gnu. So libraries didn't. have .gnu.hash section. We need to search symbols for these libraries,. too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2203
https://github.com/root-project/root/pull/2204:54,integrability,filter,filter,54,"[cxxmodules] Support both sysv and gnu hash for bloom filter; By default, libraries didn't have .gnu.hash section when compiled by old. compilers like gcc6. We need .gnu.hash section for bloomfilter to. quickly check symbols in the given library.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2204
https://github.com/root-project/root/pull/2204:39,security,hash,hash,39,"[cxxmodules] Support both sysv and gnu hash for bloom filter; By default, libraries didn't have .gnu.hash section when compiled by old. compilers like gcc6. We need .gnu.hash section for bloomfilter to. quickly check symbols in the given library.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2204
https://github.com/root-project/root/pull/2204:101,security,hash,hash,101,"[cxxmodules] Support both sysv and gnu hash for bloom filter; By default, libraries didn't have .gnu.hash section when compiled by old. compilers like gcc6. We need .gnu.hash section for bloomfilter to. quickly check symbols in the given library.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2204
https://github.com/root-project/root/pull/2204:170,security,hash,hash,170,"[cxxmodules] Support both sysv and gnu hash for bloom filter; By default, libraries didn't have .gnu.hash section when compiled by old. compilers like gcc6. We need .gnu.hash section for bloomfilter to. quickly check symbols in the given library.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2204
https://github.com/root-project/root/pull/2204:13,usability,Support,Support,13,"[cxxmodules] Support both sysv and gnu hash for bloom filter; By default, libraries didn't have .gnu.hash section when compiled by old. compilers like gcc6. We need .gnu.hash section for bloomfilter to. quickly check symbols in the given library.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2204
https://github.com/root-project/root/pull/2205:41,deployability,build,builds,41,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:65,deployability,depend,dependency,65,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:123,deployability,automat,automatically,123,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:253,deployability,version,versions,253,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:299,deployability,build,build,299,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:381,deployability,depend,dependency,381,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:65,integrability,depend,dependency,65,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:253,integrability,version,versions,253,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:381,integrability,depend,dependency,381,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:65,modifiability,depend,dependency,65,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:253,modifiability,version,versions,253,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:381,modifiability,depend,dependency,381,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:65,safety,depend,dependency,65,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:381,safety,depend,dependency,381,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:65,testability,depend,dependency,65,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:123,testability,automat,automatically,123,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:381,testability,depend,dependency,381,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2205:232,usability,command,command,232,"Add VDT to list of builtin targets so it builds before ROOT; The dependency of `Vdt::Vdt` imported target should have been automatically added by CMake due to it being listed in the `BUILD_BYPRODUCTS` of the `ExternalProject_Add()` command, but not all versions of CMake work, so it is necessary to build it early by force. Targets listed in `ROOT_BUILTIN_TARGETS`. get added as a dependency of the move_headers target, which is reasonable since they often provide headers without which ROOT cannot be built in any case (e.g. `vdt/vdtMath.h`).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2205
https://github.com/root-project/root/pull/2206:51,usability,Document,Document,51,[PyROOT] Add tutorial for pretty printing feature; Document the feature in a tutorial and point out the difference between `__str__` and `__repr__` in Python (and what `print` calls).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2206
https://github.com/root-project/root/pull/2207:167,availability,cluster,cluster,167,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2207:114,deployability,build,build,114,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2207:167,deployability,cluster,cluster,167,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2207:41,safety,input,input,41,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2207:156,safety,input,input,156,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2207:247,safety,test,test,247,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2207:247,testability,test,test,247,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2207:41,usability,input,input,41,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2207:156,usability,input,input,156,"[DF] Do not add output chain as clone of input chain in MT Snapshot; There's no need for it, as in the MT case we build one output tree. per task (i.e. per input tree cluster), so there will be no need. of resetting output branch addresses. TODO: test that this is still true when friends are present (I don't think it is?)",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2207
https://github.com/root-project/root/pull/2208:75,availability,error,error,75,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2208:75,performance,error,error,75,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2208:222,performance,memor,memory,222,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2208:321,performance,memor,memory,321,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2208:75,safety,error,error,75,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2208:75,usability,error,error,75,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2208:222,usability,memor,memory,222,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2208:249,usability,behavi,behavior,249,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2208:321,usability,memor,memory,321,"Fix zero initialization in `TCudaMatrix`; Fixes a previously fatal runtime error when trying to initialize a `cudaMatrix` with zeroes. The previous implementation was trying to assign directly into a raw pointer to device memory, which is undefined behavior. The fixed implementations creates the array of zeroes in host memory and then copies to device.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2208
https://github.com/root-project/root/pull/2209:18,safety,detect,detect,18,"Added function to detect c++ attributes at function definition.; Now, it is possible to define functions with c++ attributes without the .rawInput mode. For example functions like `[[ noreturn ]] foo() { ... }` or `[[deprecated]] [[nodiscard]] int bar(){ … }`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2209
https://github.com/root-project/root/pull/2209:18,security,detect,detect,18,"Added function to detect c++ attributes at function definition.; Now, it is possible to define functions with c++ attributes without the .rawInput mode. For example functions like `[[ noreturn ]] foo() { ... }` or `[[deprecated]] [[nodiscard]] int bar(){ … }`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2209
https://github.com/root-project/root/pull/2211:35,deployability,build,building,35,"Rip out ruby, unmaintained and not building since years (ROOT-9193).;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2211
https://github.com/root-project/root/pull/2213:129,deployability,Modul,ModuleName,129,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:186,deployability,Modul,ModuleName,186,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:200,deployability,modul,modulemap,200,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:281,deployability,build,build,281,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:319,deployability,Modul,ModuleName,319,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:333,deployability,modul,modulemap,333,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:411,deployability,modul,module,411,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:439,deployability,Modul,ModuleName,439,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:511,deployability,modul,module,511,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:816,deployability,instal,installed,816,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:831,deployability,modul,modulemap,831,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:862,deployability,modul,module,862,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:869,deployability,modul,modulemap,869,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:502,energy efficiency,Load,Load,502,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:603,energy efficiency,load,loadSubdirectoryModuleMaps,603,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:799,energy efficiency,load,load,799,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:42,integrability,sub,subdirectories,42,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:348,integrability,sub,subdirectory,348,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:540,integrability,sub,subdirectories,540,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:129,modifiability,Modul,ModuleName,129,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:186,modifiability,Modul,ModuleName,186,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:200,modifiability,modul,modulemap,200,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:319,modifiability,Modul,ModuleName,319,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:333,modifiability,modul,modulemap,333,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:411,modifiability,modul,module,411,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:439,modifiability,Modul,ModuleName,439,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:511,modifiability,modul,module,511,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:831,modifiability,modul,modulemap,831,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:862,modifiability,modul,module,862,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:869,modifiability,modul,modulemap,869,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:502,performance,Load,Load,502,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:603,performance,load,loadSubdirectoryModuleMaps,603,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:799,performance,load,load,799,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:13,safety,Test,Testing,13,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:129,safety,Modul,ModuleName,129,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:186,safety,Modul,ModuleName,186,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:200,safety,modul,modulemap,200,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:319,safety,Modul,ModuleName,319,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:333,safety,modul,modulemap,333,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:411,safety,modul,module,411,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:439,safety,Modul,ModuleName,439,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:511,safety,modul,module,511,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:831,safety,modul,modulemap,831,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:862,safety,modul,module,862,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:869,safety,modul,modulemap,869,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2213:13,testability,Test,Testing,13,"[cxxmodules] Testing! Do not iterate over subdirectories; In clang/lib/Lex/HeaderSearch.cpp HeaderSearch::lookupModule(StringRef ModuleName, StringRef SearchName),.   1. It searches for ModuleName in modulemap directly under the search directory (E.g ""/usr/include"" or ""/home/yuka/build/include"").   2. It searches for ModuleName in modulemap in a subdirectory of the search directory with the same name as the module. If it couldn't find ModuleName in both 1 and 2, it comes to this line.  283     // Load all module maps in the immediate subdirectories of this search.  284     // directory.  285     loadSubdirectoryModuleMaps(SearchDirs[Idx]);. Which results in iterating over all files in the search path for example ""/usr/include"". For us, we had a problem with CMSSW because Clang started to load our default installed root modulemap in ""usr/include/root/module.modulemap"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2213
https://github.com/root-project/root/pull/2214:150,integrability,sub,subsystem,150,ROOT-9483 -- Prevent nullptr deref. in compiled macros; When compiling TMVA GUI methods stand alone (e.g. clang++ not through `root -l`) the graphics subsystem of ROOT is not initialised leading to a null pointer dereference. See [ROOT-9483](https://sft.its.cern.ch/jira/browse/ROOT-9483?filter=-2),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2214
https://github.com/root-project/root/pull/2214:288,integrability,filter,filter,288,ROOT-9483 -- Prevent nullptr deref. in compiled macros; When compiling TMVA GUI methods stand alone (e.g. clang++ not through `root -l`) the graphics subsystem of ROOT is not initialised leading to a null pointer dereference. See [ROOT-9483](https://sft.its.cern.ch/jira/browse/ROOT-9483?filter=-2),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2214
https://github.com/root-project/root/pull/2214:13,safety,Prevent,Prevent,13,ROOT-9483 -- Prevent nullptr deref. in compiled macros; When compiling TMVA GUI methods stand alone (e.g. clang++ not through `root -l`) the graphics subsystem of ROOT is not initialised leading to a null pointer dereference. See [ROOT-9483](https://sft.its.cern.ch/jira/browse/ROOT-9483?filter=-2),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2214
https://github.com/root-project/root/pull/2214:13,security,Preven,Prevent,13,ROOT-9483 -- Prevent nullptr deref. in compiled macros; When compiling TMVA GUI methods stand alone (e.g. clang++ not through `root -l`) the graphics subsystem of ROOT is not initialised leading to a null pointer dereference. See [ROOT-9483](https://sft.its.cern.ch/jira/browse/ROOT-9483?filter=-2),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2214
https://github.com/root-project/root/pull/2215:317,deployability,patch,patch,317,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:438,deployability,patch,patch,438,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:300,energy efficiency,CPU,CPU,300,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:0,performance,Perform,Performance,0,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:129,performance,time,times,129,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:162,performance,time,time,162,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:275,performance,time,time,275,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:300,performance,CPU,CPU,300,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:351,performance,cach,caches,351,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:317,safety,patch,patch,317,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:438,safety,patch,patch,438,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:317,security,patch,patch,317,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:438,security,patch,patch,438,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:0,usability,Perform,Performance,0,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2215:142,usability,minim,minimization,142,"Performance fix for MnMachinePrecision; I noticed while debugging Minuit, that `MnMachinePrecision`'s constructor is called many times during minimization. Every time it is created, it goes through a loop to computes the machine precision. Since the number is the same every time, this is a waste of CPU cycles. This patch runs the loop only once and caches the result. New instances of MnMachinePrecision use the pre-computed value. The patch has no side effects.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2215
https://github.com/root-project/root/pull/2216:104,deployability,log,logically,104,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:325,energy efficiency,reduc,reduction,325,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:612,energy efficiency,CPU,CPU,612,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:374,interoperability,architectur,architecture,374,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:14,modifiability,Layer,Layer,14,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:87,modifiability,layer,layer,87,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:612,performance,CPU,CPU,612,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:104,safety,log,logically,104,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:191,safety,compl,complex,191,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:422,safety,test,test,422,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:104,security,log,logically,104,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:191,security,compl,complex,191,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:104,testability,log,logically,104,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:258,testability,simpl,simpler,258,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:422,testability,test,test,422,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:649,testability,assert,asserted,649,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:258,usability,simpl,simpler,258,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2216:276,usability,Help,Helper,276,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2216
https://github.com/root-project/root/pull/2217:19,integrability,sub,sub-object,19,Running rules when sub-object is split;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2217
https://github.com/root-project/root/pull/2219:19,energy efficiency,optim,optimisations,19,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:301,energy efficiency,Optim,Optimise,301,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:339,energy efficiency,CPU,CPU,339,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:407,energy efficiency,Optim,Optimise,407,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:491,energy efficiency,reduc,reduces,491,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:145,performance,network,network,145,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:155,performance,time,times,155,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:184,performance,perform,performances,184,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:339,performance,CPU,CPU,339,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:434,performance,network,networks,434,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:517,performance,time,time,517,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:532,performance,network,network,532,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:540,safety,test,testing,540,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:145,security,network,network,145,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:265,security,loss,loss,265,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:434,security,network,networks,434,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:499,security,sign,significantly,499,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:532,security,network,network,532,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:573,security,session,session,573,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:540,testability,test,testing,540,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:184,usability,perform,performances,184,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2219:356,usability,minim,minimum,356,"MethodDL fixes and optimisations; Apply several improvements and fixes in MethodDL , mainly: . - Use now Float instead of Double. This makes the network 2 times faster without loosing performances in quality of results. - Compute now the regularisation term in the loss once/epoch when evaluating . - Optimise the MultiThread execution on CPU by setting a minimum number of elements to execute per task . - Optimise evaluation of the networks, by implementing GetMvaValues in MethodDL. This reduces significantly the time spent for network testing and evaluating in a TMVA session",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2219
https://github.com/root-project/root/pull/2220:285,availability,error,error,285,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:374,availability,operat,operator,374,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:98,deployability,build,build,98,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:148,deployability,BUILD,BUILDTYPE,148,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:158,deployability,Releas,Release,158,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:206,deployability,build,build,206,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:245,energy efficiency,core,core,245,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:302,interoperability,specif,specification,302,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:285,performance,error,error,285,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:331,reliability,doe,does,331,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:285,safety,error,error,285,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:292,safety,except,exception,292,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2220:285,usability,error,error,285,"Revert ""Fix GCC8 new warnings (related to ROOT-9448)""; Reverts root-project/root#2130:. ```. /mnt/build/jenkins/workspace/lcg_personal_experimental/BUILDTYPE/Release/COMPILER/clang600binutils/LABEL/centos7/build/projects/ROOT-HEAD/src/ROOT/HEAD/core/newdelete/src/NewDelete.cxx:214:7: error: exception specification in declaration does not match previous declaration. void *operator new(size_t /*size*/, std::align_val_t /*al*/) throw(). ^. ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2220
https://github.com/root-project/root/pull/2221:41,usability,help,helpers,41,"[DF] Add `FinalizeTask` method to action helpers, call it at the end of each task;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2221
https://github.com/root-project/root/pull/2223:59,energy efficiency,green,green,59,[DF] Fix ROOT-9471 in v6.14; To be merged if nightlies are green tonight :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2223
https://github.com/root-project/root/pull/2224:30,integrability,Buffer,Buffer,30,Add possibility to extend XML Buffer size for XMNL I/O in TMVA;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2224
https://github.com/root-project/root/pull/2224:26,interoperability,XML,XML,26,Add possibility to extend XML Buffer size for XMNL I/O in TMVA;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2224
https://github.com/root-project/root/pull/2224:19,modifiability,exten,extend,19,Add possibility to extend XML Buffer size for XMNL I/O in TMVA;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2224
https://github.com/root-project/root/pull/2224:51,performance,I/O,I/O,51,Add possibility to extend XML Buffer size for XMNL I/O in TMVA;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2224
https://github.com/root-project/root/pull/2226:14,deployability,stack,stack,14,"[DF] Use std::stack instead of std::vector, other small fixes;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2226
https://github.com/root-project/root/pull/2227:44,availability,failur,failures,44,"[DF] Fix ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles""; To be merged after #2226 (the first commits belong to #2226).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2227
https://github.com/root-project/root/pull/2227:44,deployability,fail,failures,44,"[DF] Fix ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles""; To be merged after #2226 (the first commits belong to #2226).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2227
https://github.com/root-project/root/pull/2227:44,performance,failur,failures,44,"[DF] Fix ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles""; To be merged after #2226 (the first commits belong to #2226).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2227
https://github.com/root-project/root/pull/2227:44,reliability,fail,failures,44,"[DF] Fix ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles""; To be merged after #2226 (the first commits belong to #2226).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2227
https://github.com/root-project/root/pull/2228:163,availability,failur,failures,163,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:9,deployability,stack,stacks,9,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:163,deployability,fail,failures,163,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:369,deployability,contain,contains,369,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:1158,deployability,stack,stacks,1158,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:96,energy efficiency,green,green,96,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:163,performance,failur,failures,163,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:726,performance,multi-thread,multi-thread,726,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:799,performance,multi-thread,multi-thread,799,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:1075,performance,content,contents,1075,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:163,reliability,fail,failures,163,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:634,security,stolen,stolen,634,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2228:1020,usability,resum,resumed,1020,"[DF] Use stacks rather than single output files in Snapshot MT; EDIT: to merge if nightlies are green for master. This resolves ROOT-9456, ""[Jenkins][DF] Sporadic failures in test_snapshotNFiles"". The issue is due to interleaved TBB task execution in the same thread. When nested task spawning is present (i.e. when TBB tasks spawn other. TBB tasks and the thread pool contains at least three threads), the TBB. task stealing mechanism makes it possible that a thread starts executing. a new instance of the ""parent"" tasks before it finishes the execution of. the previous parent task (this happens when the thread is waiting for a. ""stolen"" child task to finish, so it steals another threads' parent. task). In the case of a multi-thread snapshot, one needs to write at least two. branches for the multi-thread branch writing to kick in and have nested. task spawning. The crash was caused by a new parent task overwriting the output TTree. of the previous parent task in a given thread. When the previous parent. task resumed execution, it ended up trying to flush out the contents of. the deleted output TTree. The solution adopted is to use thread-local stacks of output TTrees (as. opposed to single output TTrees), that tasks push and pop when they. start and finish respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2228
https://github.com/root-project/root/pull/2229:147,availability,redund,redundant,147,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:194,availability,redund,redundant,194,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:755,availability,sli,sliding,755,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:806,availability,operat,operation,806,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:933,availability,operat,operation,933,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1028,availability,operat,operation,1028,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1187,availability,operat,operation,1187,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:0,deployability,API,API,0,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:16,deployability,API,API,16,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:77,deployability,API,API,77,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:147,deployability,redundan,redundant,147,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:194,deployability,redundan,redundant,194,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:586,deployability,modul,module,586,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:707,deployability,log,logic,707,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1236,deployability,log,logic,1236,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1330,deployability,modul,module,1330,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1486,deployability,API,API,1486,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:120,energy efficiency,current,currently,120,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1526,energy efficiency,reduc,reduced,1526,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:0,integrability,API,API,0,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:16,integrability,API,API,16,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:77,integrability,API,API,77,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:625,integrability,sub,sub-class,625,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:745,integrability,filter,filter,745,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1391,integrability,filter,filter,1391,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1486,integrability,API,API,1486,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:0,interoperability,API,API,0,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:16,interoperability,API,API,16,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:77,interoperability,API,API,77,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1221,interoperability,share,share,1221,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1486,interoperability,API,API,1486,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:92,modifiability,layer,layers,92,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:389,modifiability,design decis,design decisions,389,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:586,modifiability,modul,module,586,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:657,modifiability,layer,layer,657,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:922,modifiability,layer,layer,922,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:982,modifiability,paramet,parameters,982,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1018,modifiability,layer,layer,1018,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1076,modifiability,paramet,parameters,1076,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1164,modifiability,layer,layer,1164,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1307,modifiability,layer,layer,1307,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1330,modifiability,modul,module,1330,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:687,performance,network,network,687,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:147,reliability,redundan,redundant,147,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:194,reliability,redundan,redundant,194,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:755,reliability,sli,sliding,755,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:147,safety,redund,redundant,147,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:194,safety,redund,redundant,194,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:586,safety,modul,module,586,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:707,safety,log,logic,707,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:772,safety,input,input,772,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:823,safety,input,input,823,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1236,safety,log,logic,1236,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1330,safety,modul,module,1330,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:571,security,auth,authors,571,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:687,security,network,network,687,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:707,security,log,logic,707,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1236,security,log,logic,1236,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1361,security,stride,strides,1361,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:212,testability,context,context,212,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:707,testability,log,logic,707,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:1236,testability,log,logic,1236,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:455,usability,experien,experience,455,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:535,usability,experien,experienced,535,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:772,usability,input,input,772,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:823,usability,input,input,823,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2229:972,usability,learn,learnable,972,"API Redesign; # API Redesign. ## Goal. The goal is this PR is to improve the API of the CNN layers (MaxPooling and Conv currently), by eliminating redundant constructor arguments and fields. By redundant in this context, I refer to arguments that can be directly computed from others, and fields that unnecesseraly exist in multiple classes. ## Key points. Below some discussion points on design decisions I made, but still consider debatable. . Since my experience in production level C++ is very limited I highly value opinions from experienced colleagues and previous authors of the module. ### Making `MaxPoolingLayer` a sub-class of `ConvLayer`. Every layer type in a convolutional network follows the logic existing in our `ConvLayer`:. A filter is sliding over the input and at each step applies an operation to the input elements within its receptive field to produce a single output element. * In a convolutional layer this operation is a linear combination with learnable parameters. * In an average pooling layer the operation is a linear combination with constant parameters (and equal to the inverse of the receptive field's size). * In a max pooling layer its a non linear operation. As we can see they all share the same logic and therefore fields. ## Results. 1. Common fields between the 2 layer types in the CNN module are now not duplicated (strides sizes, padding sizes, filter sizes). The same for the 4 `protected` methods in `ConvLayer`. 2. We now have a cleaner API, as the constructor arguments where reduced from 26 to 16 without any change in the functionality).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2229
https://github.com/root-project/root/pull/2230:99,deployability,fail,fails,99,[DF] Expect test death rather than asserting it; The gtest can safely go on in case this test case fails.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2230
https://github.com/root-project/root/pull/2230:99,reliability,fail,fails,99,[DF] Expect test death rather than asserting it; The gtest can safely go on in case this test case fails.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2230
https://github.com/root-project/root/pull/2230:12,safety,test,test,12,[DF] Expect test death rather than asserting it; The gtest can safely go on in case this test case fails.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2230
https://github.com/root-project/root/pull/2230:63,safety,safe,safely,63,[DF] Expect test death rather than asserting it; The gtest can safely go on in case this test case fails.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2230
https://github.com/root-project/root/pull/2230:89,safety,test,test,89,[DF] Expect test death rather than asserting it; The gtest can safely go on in case this test case fails.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2230
https://github.com/root-project/root/pull/2230:12,testability,test,test,12,[DF] Expect test death rather than asserting it; The gtest can safely go on in case this test case fails.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2230
https://github.com/root-project/root/pull/2230:35,testability,assert,asserting,35,[DF] Expect test death rather than asserting it; The gtest can safely go on in case this test case fails.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2230
https://github.com/root-project/root/pull/2230:89,testability,test,test,89,[DF] Expect test death rather than asserting it; The gtest can safely go on in case this test case fails.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2230
https://github.com/root-project/root/pull/2231:140,availability,failur,failures,140,"Revert ""[DF] Use std::numeric_limits instead of climits""; This reverts commit 87801d05a76cdf10a1cf9d7ee33540d3e0e314bf. This should fix the failures (not understood) in dataframe_nodes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2231
https://github.com/root-project/root/pull/2231:140,deployability,fail,failures,140,"Revert ""[DF] Use std::numeric_limits instead of climits""; This reverts commit 87801d05a76cdf10a1cf9d7ee33540d3e0e314bf. This should fix the failures (not understood) in dataframe_nodes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2231
https://github.com/root-project/root/pull/2231:140,performance,failur,failures,140,"Revert ""[DF] Use std::numeric_limits instead of climits""; This reverts commit 87801d05a76cdf10a1cf9d7ee33540d3e0e314bf. This should fix the failures (not understood) in dataframe_nodes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2231
https://github.com/root-project/root/pull/2231:140,reliability,fail,failures,140,"Revert ""[DF] Use std::numeric_limits instead of climits""; This reverts commit 87801d05a76cdf10a1cf9d7ee33540d3e0e314bf. This should fix the failures (not understood) in dataframe_nodes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2231
https://github.com/root-project/root/pull/2233:37,energy efficiency,GPU,GPU,37,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:41,interoperability,Architectur,Architecture,41,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:360,interoperability,architectur,architectures,360,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:8,modifiability,Layer,Layer,8,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:118,modifiability,Layer,Layer,118,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:245,modifiability,refact,refactored,245,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:37,performance,GPU,GPU,37,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:245,performance,refactor,refactored,245,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:78,safety,test,tests,78,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:271,safety,test,testing,271,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:329,safety,test,tests,329,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:78,testability,test,tests,78,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:271,testability,test,testing,271,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2233:329,testability,test,tests,329,Reshape Layer implementation for the GPU Architecture; This PR implements and tests all the functions of the `Reshape Layer` in CUDA. Those are:. 1. The `Flatten` function. 2. The `Deflatten` function. 3. The ` Reshape` function. I additionally refactored the respective testing suite to remove code duplication between the same tests implemented in different architectures. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2233
https://github.com/root-project/root/pull/2234:145,availability,cluster,cluster,145,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:145,deployability,cluster,cluster,145,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:494,integrability,event,event,494,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:354,reliability,doe,does,354,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:5,safety,Avoid,Avoid,5,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:246,safety,input,input,246,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:270,safety,input,input,270,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:450,safety,safe,safe,450,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:246,usability,input,input,246,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2234:270,usability,input,input,270,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2234
https://github.com/root-project/root/pull/2236:641,energy efficiency,CPU,CPU,641,"Add vectorized implementations of first batch of TMath functions; This PR adds vectorized implementations of the following TMath functions using VecCore backend :. - Log2. - Breit-Wigner. - Gaus. - LaplaceDist. - LaplaceDistI. - Freq. - Bessel I0, I1, J0, J1. The first batch includes functions for which a definite speedup is obtained. Left out are the ones with more conditional branches. Work is ongoing to implement them as well. [Here](https://github.com/root-project/rootbench/pull/79) is the PR for benchmarks. Benchmarks from a trial run :. ```. ----------------------------------------------------------------------. Benchmark Time CPU Iterations. -----------------------------------------------------------------------. BM_TMath_Log2 340895 ns 340801 ns 2042. BM_TMath_BreitWigner 42236 ns 42227 ns 16562. BM_TMath_Gaus 280188 ns 280130 ns 2476. BM_TMath_LaplaceDist 246254 ns 246176 ns 2834. BM_TMath_LaplaceDistI 291277 ns 291221 ns 2405. BM_TMath_Freq 388384 ns 388278 ns 1816. BM_TMath_BesselI0 283500 ns 283445 ns 2466. BM_TMath_BesselI1 327932 ns 327847 ns 2134. BM_TMath_BesselJ0 744044 ns 743897 ns 938. BM_TMath_BesselJ1 735381 ns 735235 ns 937. BM_VectorizedTMath_Log2 97462 ns 97433 ns 7079. BM_VectorizedTMath_BreitWigner 20773 ns 20769 ns 33494. BM_VectorizedTMath_Gaus 127413 ns 127385 ns 5519. BM_VectorizedTMath_LaplaceDist 118903 ns 118870 ns 5845. BM_VectorizedTMath_LaplaceDistI 130724 ns 130693 ns 5367. BM_VectorizedTMath_Freq 267444 ns 267389 ns 2590. BM_VectorizedTMath_BesselI0 177544 ns 177503 ns 3936. BM_VectorizedTMath_BesselI1 206571 ns 206523 ns 3370. BM_VectorizedTMath_BesselJ0 326378 ns 326312 ns 2144. BM_VectorizedTMath_BesselJ1 343600 ns 343531 ns 2014. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2236
https://github.com/root-project/root/pull/2236:40,integrability,batch,batch,40,"Add vectorized implementations of first batch of TMath functions; This PR adds vectorized implementations of the following TMath functions using VecCore backend :. - Log2. - Breit-Wigner. - Gaus. - LaplaceDist. - LaplaceDistI. - Freq. - Bessel I0, I1, J0, J1. The first batch includes functions for which a definite speedup is obtained. Left out are the ones with more conditional branches. Work is ongoing to implement them as well. [Here](https://github.com/root-project/rootbench/pull/79) is the PR for benchmarks. Benchmarks from a trial run :. ```. ----------------------------------------------------------------------. Benchmark Time CPU Iterations. -----------------------------------------------------------------------. BM_TMath_Log2 340895 ns 340801 ns 2042. BM_TMath_BreitWigner 42236 ns 42227 ns 16562. BM_TMath_Gaus 280188 ns 280130 ns 2476. BM_TMath_LaplaceDist 246254 ns 246176 ns 2834. BM_TMath_LaplaceDistI 291277 ns 291221 ns 2405. BM_TMath_Freq 388384 ns 388278 ns 1816. BM_TMath_BesselI0 283500 ns 283445 ns 2466. BM_TMath_BesselI1 327932 ns 327847 ns 2134. BM_TMath_BesselJ0 744044 ns 743897 ns 938. BM_TMath_BesselJ1 735381 ns 735235 ns 937. BM_VectorizedTMath_Log2 97462 ns 97433 ns 7079. BM_VectorizedTMath_BreitWigner 20773 ns 20769 ns 33494. BM_VectorizedTMath_Gaus 127413 ns 127385 ns 5519. BM_VectorizedTMath_LaplaceDist 118903 ns 118870 ns 5845. BM_VectorizedTMath_LaplaceDistI 130724 ns 130693 ns 5367. BM_VectorizedTMath_Freq 267444 ns 267389 ns 2590. BM_VectorizedTMath_BesselI0 177544 ns 177503 ns 3936. BM_VectorizedTMath_BesselI1 206571 ns 206523 ns 3370. BM_VectorizedTMath_BesselJ0 326378 ns 326312 ns 2144. BM_VectorizedTMath_BesselJ1 343600 ns 343531 ns 2014. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2236
https://github.com/root-project/root/pull/2236:270,integrability,batch,batch,270,"Add vectorized implementations of first batch of TMath functions; This PR adds vectorized implementations of the following TMath functions using VecCore backend :. - Log2. - Breit-Wigner. - Gaus. - LaplaceDist. - LaplaceDistI. - Freq. - Bessel I0, I1, J0, J1. The first batch includes functions for which a definite speedup is obtained. Left out are the ones with more conditional branches. Work is ongoing to implement them as well. [Here](https://github.com/root-project/rootbench/pull/79) is the PR for benchmarks. Benchmarks from a trial run :. ```. ----------------------------------------------------------------------. Benchmark Time CPU Iterations. -----------------------------------------------------------------------. BM_TMath_Log2 340895 ns 340801 ns 2042. BM_TMath_BreitWigner 42236 ns 42227 ns 16562. BM_TMath_Gaus 280188 ns 280130 ns 2476. BM_TMath_LaplaceDist 246254 ns 246176 ns 2834. BM_TMath_LaplaceDistI 291277 ns 291221 ns 2405. BM_TMath_Freq 388384 ns 388278 ns 1816. BM_TMath_BesselI0 283500 ns 283445 ns 2466. BM_TMath_BesselI1 327932 ns 327847 ns 2134. BM_TMath_BesselJ0 744044 ns 743897 ns 938. BM_TMath_BesselJ1 735381 ns 735235 ns 937. BM_VectorizedTMath_Log2 97462 ns 97433 ns 7079. BM_VectorizedTMath_BreitWigner 20773 ns 20769 ns 33494. BM_VectorizedTMath_Gaus 127413 ns 127385 ns 5519. BM_VectorizedTMath_LaplaceDist 118903 ns 118870 ns 5845. BM_VectorizedTMath_LaplaceDistI 130724 ns 130693 ns 5367. BM_VectorizedTMath_Freq 267444 ns 267389 ns 2590. BM_VectorizedTMath_BesselI0 177544 ns 177503 ns 3936. BM_VectorizedTMath_BesselI1 206571 ns 206523 ns 3370. BM_VectorizedTMath_BesselJ0 326378 ns 326312 ns 2144. BM_VectorizedTMath_BesselJ1 343600 ns 343531 ns 2014. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2236
https://github.com/root-project/root/pull/2236:40,performance,batch,batch,40,"Add vectorized implementations of first batch of TMath functions; This PR adds vectorized implementations of the following TMath functions using VecCore backend :. - Log2. - Breit-Wigner. - Gaus. - LaplaceDist. - LaplaceDistI. - Freq. - Bessel I0, I1, J0, J1. The first batch includes functions for which a definite speedup is obtained. Left out are the ones with more conditional branches. Work is ongoing to implement them as well. [Here](https://github.com/root-project/rootbench/pull/79) is the PR for benchmarks. Benchmarks from a trial run :. ```. ----------------------------------------------------------------------. Benchmark Time CPU Iterations. -----------------------------------------------------------------------. BM_TMath_Log2 340895 ns 340801 ns 2042. BM_TMath_BreitWigner 42236 ns 42227 ns 16562. BM_TMath_Gaus 280188 ns 280130 ns 2476. BM_TMath_LaplaceDist 246254 ns 246176 ns 2834. BM_TMath_LaplaceDistI 291277 ns 291221 ns 2405. BM_TMath_Freq 388384 ns 388278 ns 1816. BM_TMath_BesselI0 283500 ns 283445 ns 2466. BM_TMath_BesselI1 327932 ns 327847 ns 2134. BM_TMath_BesselJ0 744044 ns 743897 ns 938. BM_TMath_BesselJ1 735381 ns 735235 ns 937. BM_VectorizedTMath_Log2 97462 ns 97433 ns 7079. BM_VectorizedTMath_BreitWigner 20773 ns 20769 ns 33494. BM_VectorizedTMath_Gaus 127413 ns 127385 ns 5519. BM_VectorizedTMath_LaplaceDist 118903 ns 118870 ns 5845. BM_VectorizedTMath_LaplaceDistI 130724 ns 130693 ns 5367. BM_VectorizedTMath_Freq 267444 ns 267389 ns 2590. BM_VectorizedTMath_BesselI0 177544 ns 177503 ns 3936. BM_VectorizedTMath_BesselI1 206571 ns 206523 ns 3370. BM_VectorizedTMath_BesselJ0 326378 ns 326312 ns 2144. BM_VectorizedTMath_BesselJ1 343600 ns 343531 ns 2014. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2236
https://github.com/root-project/root/pull/2236:270,performance,batch,batch,270,"Add vectorized implementations of first batch of TMath functions; This PR adds vectorized implementations of the following TMath functions using VecCore backend :. - Log2. - Breit-Wigner. - Gaus. - LaplaceDist. - LaplaceDistI. - Freq. - Bessel I0, I1, J0, J1. The first batch includes functions for which a definite speedup is obtained. Left out are the ones with more conditional branches. Work is ongoing to implement them as well. [Here](https://github.com/root-project/rootbench/pull/79) is the PR for benchmarks. Benchmarks from a trial run :. ```. ----------------------------------------------------------------------. Benchmark Time CPU Iterations. -----------------------------------------------------------------------. BM_TMath_Log2 340895 ns 340801 ns 2042. BM_TMath_BreitWigner 42236 ns 42227 ns 16562. BM_TMath_Gaus 280188 ns 280130 ns 2476. BM_TMath_LaplaceDist 246254 ns 246176 ns 2834. BM_TMath_LaplaceDistI 291277 ns 291221 ns 2405. BM_TMath_Freq 388384 ns 388278 ns 1816. BM_TMath_BesselI0 283500 ns 283445 ns 2466. BM_TMath_BesselI1 327932 ns 327847 ns 2134. BM_TMath_BesselJ0 744044 ns 743897 ns 938. BM_TMath_BesselJ1 735381 ns 735235 ns 937. BM_VectorizedTMath_Log2 97462 ns 97433 ns 7079. BM_VectorizedTMath_BreitWigner 20773 ns 20769 ns 33494. BM_VectorizedTMath_Gaus 127413 ns 127385 ns 5519. BM_VectorizedTMath_LaplaceDist 118903 ns 118870 ns 5845. BM_VectorizedTMath_LaplaceDistI 130724 ns 130693 ns 5367. BM_VectorizedTMath_Freq 267444 ns 267389 ns 2590. BM_VectorizedTMath_BesselI0 177544 ns 177503 ns 3936. BM_VectorizedTMath_BesselI1 206571 ns 206523 ns 3370. BM_VectorizedTMath_BesselJ0 326378 ns 326312 ns 2144. BM_VectorizedTMath_BesselJ1 343600 ns 343531 ns 2014. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2236
https://github.com/root-project/root/pull/2236:636,performance,Time,Time,636,"Add vectorized implementations of first batch of TMath functions; This PR adds vectorized implementations of the following TMath functions using VecCore backend :. - Log2. - Breit-Wigner. - Gaus. - LaplaceDist. - LaplaceDistI. - Freq. - Bessel I0, I1, J0, J1. The first batch includes functions for which a definite speedup is obtained. Left out are the ones with more conditional branches. Work is ongoing to implement them as well. [Here](https://github.com/root-project/rootbench/pull/79) is the PR for benchmarks. Benchmarks from a trial run :. ```. ----------------------------------------------------------------------. Benchmark Time CPU Iterations. -----------------------------------------------------------------------. BM_TMath_Log2 340895 ns 340801 ns 2042. BM_TMath_BreitWigner 42236 ns 42227 ns 16562. BM_TMath_Gaus 280188 ns 280130 ns 2476. BM_TMath_LaplaceDist 246254 ns 246176 ns 2834. BM_TMath_LaplaceDistI 291277 ns 291221 ns 2405. BM_TMath_Freq 388384 ns 388278 ns 1816. BM_TMath_BesselI0 283500 ns 283445 ns 2466. BM_TMath_BesselI1 327932 ns 327847 ns 2134. BM_TMath_BesselJ0 744044 ns 743897 ns 938. BM_TMath_BesselJ1 735381 ns 735235 ns 937. BM_VectorizedTMath_Log2 97462 ns 97433 ns 7079. BM_VectorizedTMath_BreitWigner 20773 ns 20769 ns 33494. BM_VectorizedTMath_Gaus 127413 ns 127385 ns 5519. BM_VectorizedTMath_LaplaceDist 118903 ns 118870 ns 5845. BM_VectorizedTMath_LaplaceDistI 130724 ns 130693 ns 5367. BM_VectorizedTMath_Freq 267444 ns 267389 ns 2590. BM_VectorizedTMath_BesselI0 177544 ns 177503 ns 3936. BM_VectorizedTMath_BesselI1 206571 ns 206523 ns 3370. BM_VectorizedTMath_BesselJ0 326378 ns 326312 ns 2144. BM_VectorizedTMath_BesselJ1 343600 ns 343531 ns 2014. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2236
https://github.com/root-project/root/pull/2236:641,performance,CPU,CPU,641,"Add vectorized implementations of first batch of TMath functions; This PR adds vectorized implementations of the following TMath functions using VecCore backend :. - Log2. - Breit-Wigner. - Gaus. - LaplaceDist. - LaplaceDistI. - Freq. - Bessel I0, I1, J0, J1. The first batch includes functions for which a definite speedup is obtained. Left out are the ones with more conditional branches. Work is ongoing to implement them as well. [Here](https://github.com/root-project/rootbench/pull/79) is the PR for benchmarks. Benchmarks from a trial run :. ```. ----------------------------------------------------------------------. Benchmark Time CPU Iterations. -----------------------------------------------------------------------. BM_TMath_Log2 340895 ns 340801 ns 2042. BM_TMath_BreitWigner 42236 ns 42227 ns 16562. BM_TMath_Gaus 280188 ns 280130 ns 2476. BM_TMath_LaplaceDist 246254 ns 246176 ns 2834. BM_TMath_LaplaceDistI 291277 ns 291221 ns 2405. BM_TMath_Freq 388384 ns 388278 ns 1816. BM_TMath_BesselI0 283500 ns 283445 ns 2466. BM_TMath_BesselI1 327932 ns 327847 ns 2134. BM_TMath_BesselJ0 744044 ns 743897 ns 938. BM_TMath_BesselJ1 735381 ns 735235 ns 937. BM_VectorizedTMath_Log2 97462 ns 97433 ns 7079. BM_VectorizedTMath_BreitWigner 20773 ns 20769 ns 33494. BM_VectorizedTMath_Gaus 127413 ns 127385 ns 5519. BM_VectorizedTMath_LaplaceDist 118903 ns 118870 ns 5845. BM_VectorizedTMath_LaplaceDistI 130724 ns 130693 ns 5367. BM_VectorizedTMath_Freq 267444 ns 267389 ns 2590. BM_VectorizedTMath_BesselI0 177544 ns 177503 ns 3936. BM_VectorizedTMath_BesselI1 206571 ns 206523 ns 3370. BM_VectorizedTMath_BesselJ0 326378 ns 326312 ns 2144. BM_VectorizedTMath_BesselJ1 343600 ns 343531 ns 2014. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2236
https://github.com/root-project/root/pull/2237:197,availability,state,state,197,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:1,energy efficiency,Core,Core,1,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:197,integrability,state,state,197,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:345,modifiability,scal,scaling,345,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:111,performance,lock,lock,111,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:171,performance,cach,cache,171,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:227,performance,lock,lock,227,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:255,performance,cach,cache,255,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:315,performance,lock,lock,315,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:111,security,lock,lock,111,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:206,security,modif,modified,206,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:227,security,lock,lock,227,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:315,security,lock,lock,315,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:326,usability,help,helps,326,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2237:356,usability,workflow,workflows,356,"[Core] Split critical section in TClingClassInfo::GetBaseOffset; the full method was protected with the global lock. Now we split it in two parts, the first one where the cache is looked at and no state is modified with a read lock. The second, where the cache is filled and the interpreter is invoked with a write lock. This helps considerably scaling MT workflows.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2237
https://github.com/root-project/root/pull/2238:85,deployability,upgrad,upgraded,85,"[DF] Try to fix dataframe_nodes test; This PR should be merged after `TSlotStack` is upgraded to not use static thread-local storage,. and one test case is expected to fail until this happens.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2238
https://github.com/root-project/root/pull/2238:168,deployability,fail,fail,168,"[DF] Try to fix dataframe_nodes test; This PR should be merged after `TSlotStack` is upgraded to not use static thread-local storage,. and one test case is expected to fail until this happens.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2238
https://github.com/root-project/root/pull/2238:85,modifiability,upgrad,upgraded,85,"[DF] Try to fix dataframe_nodes test; This PR should be merged after `TSlotStack` is upgraded to not use static thread-local storage,. and one test case is expected to fail until this happens.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2238
https://github.com/root-project/root/pull/2238:168,reliability,fail,fail,168,"[DF] Try to fix dataframe_nodes test; This PR should be merged after `TSlotStack` is upgraded to not use static thread-local storage,. and one test case is expected to fail until this happens.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2238
https://github.com/root-project/root/pull/2238:32,safety,test,test,32,"[DF] Try to fix dataframe_nodes test; This PR should be merged after `TSlotStack` is upgraded to not use static thread-local storage,. and one test case is expected to fail until this happens.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2238
https://github.com/root-project/root/pull/2238:143,safety,test,test,143,"[DF] Try to fix dataframe_nodes test; This PR should be merged after `TSlotStack` is upgraded to not use static thread-local storage,. and one test case is expected to fail until this happens.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2238
https://github.com/root-project/root/pull/2238:32,testability,test,test,32,"[DF] Try to fix dataframe_nodes test; This PR should be merged after `TSlotStack` is upgraded to not use static thread-local storage,. and one test case is expected to fail until this happens.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2238
https://github.com/root-project/root/pull/2238:143,testability,test,test,143,"[DF] Try to fix dataframe_nodes test; This PR should be merged after `TSlotStack` is upgraded to not use static thread-local storage,. and one test case is expected to fail until this happens.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2238
https://github.com/root-project/root/pull/2239:67,integrability,filter,filters,67,"[DF] Better jitted defines; In line with what it's done for jitted filters, now the jitting. of custom columns creates the RCustomColumn and assigns it as the. fConcreteCustomColumn of the corresponding RJittedCustomColumn that. was previously booked. This avoids having the situation in which a certain custom column. has been ""booked"" but is not yet present in the map of custom columns. It will also help with the ""local custom columns"" that are coming. with ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2239
https://github.com/root-project/root/pull/2239:257,safety,avoid,avoids,257,"[DF] Better jitted defines; In line with what it's done for jitted filters, now the jitting. of custom columns creates the RCustomColumn and assigns it as the. fConcreteCustomColumn of the corresponding RJittedCustomColumn that. was previously booked. This avoids having the situation in which a certain custom column. has been ""booked"" but is not yet present in the map of custom columns. It will also help with the ""local custom columns"" that are coming. with ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2239
https://github.com/root-project/root/pull/2239:96,usability,custom,custom,96,"[DF] Better jitted defines; In line with what it's done for jitted filters, now the jitting. of custom columns creates the RCustomColumn and assigns it as the. fConcreteCustomColumn of the corresponding RJittedCustomColumn that. was previously booked. This avoids having the situation in which a certain custom column. has been ""booked"" but is not yet present in the map of custom columns. It will also help with the ""local custom columns"" that are coming. with ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2239
https://github.com/root-project/root/pull/2239:304,usability,custom,custom,304,"[DF] Better jitted defines; In line with what it's done for jitted filters, now the jitting. of custom columns creates the RCustomColumn and assigns it as the. fConcreteCustomColumn of the corresponding RJittedCustomColumn that. was previously booked. This avoids having the situation in which a certain custom column. has been ""booked"" but is not yet present in the map of custom columns. It will also help with the ""local custom columns"" that are coming. with ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2239
https://github.com/root-project/root/pull/2239:374,usability,custom,custom,374,"[DF] Better jitted defines; In line with what it's done for jitted filters, now the jitting. of custom columns creates the RCustomColumn and assigns it as the. fConcreteCustomColumn of the corresponding RJittedCustomColumn that. was previously booked. This avoids having the situation in which a certain custom column. has been ""booked"" but is not yet present in the map of custom columns. It will also help with the ""local custom columns"" that are coming. with ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2239
https://github.com/root-project/root/pull/2239:403,usability,help,help,403,"[DF] Better jitted defines; In line with what it's done for jitted filters, now the jitting. of custom columns creates the RCustomColumn and assigns it as the. fConcreteCustomColumn of the corresponding RJittedCustomColumn that. was previously booked. This avoids having the situation in which a certain custom column. has been ""booked"" but is not yet present in the map of custom columns. It will also help with the ""local custom columns"" that are coming. with ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2239
https://github.com/root-project/root/pull/2239:424,usability,custom,custom,424,"[DF] Better jitted defines; In line with what it's done for jitted filters, now the jitting. of custom columns creates the RCustomColumn and assigns it as the. fConcreteCustomColumn of the corresponding RJittedCustomColumn that. was previously booked. This avoids having the situation in which a certain custom column. has been ""booked"" but is not yet present in the map of custom columns. It will also help with the ""local custom columns"" that are coming. with ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2239
https://github.com/root-project/root/pull/2240:9,availability,slo,slot,9,[DF] Fix slot stack; This reloads #2238 fixing the TSlotStack.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2240
https://github.com/root-project/root/pull/2240:14,deployability,stack,stack,14,[DF] Fix slot stack; This reloads #2238 fixing the TSlotStack.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2240
https://github.com/root-project/root/pull/2240:9,reliability,slo,slot,9,[DF] Fix slot stack; This reloads #2238 fixing the TSlotStack.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2240
https://github.com/root-project/root/pull/2242:145,availability,cluster,cluster,145,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:145,deployability,cluster,cluster,145,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:494,integrability,event,event,494,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:354,reliability,doe,does,354,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:5,safety,Avoid,Avoid,5,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:246,safety,input,input,246,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:270,safety,input,input,270,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:450,safety,safe,safe,450,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:246,usability,input,input,246,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2242:270,usability,input,input,270,"[DF] Avoid TChain::AddClone in Snapshot when unnecessary; Since output trees are recreated for each task, and each task only. processes one tree cluster (and never crosses file boundaries), we. don't need to add the output trees as clones of the input tress. unless the input trees have friends (which might cross file boundaries). even if the main tree does not. Usage of AddClone here is undesirable in the general case because. it generates many (safe) warnings printed at screen during the event. loop, see ROOT-9487.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2242
https://github.com/root-project/root/pull/2243:35,deployability,log,log,35,"netxng: add possibility to control log verbosity with NetXNG.Debug; Addresses ROOT-9311. The verbosity level can also be changed with the environment variable. XRD_LOGLEVEL. The possible levels are: ""Info"", ""Debug"", ""Dump"" .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2243
https://github.com/root-project/root/pull/2243:150,modifiability,variab,variable,150,"netxng: add possibility to control log verbosity with NetXNG.Debug; Addresses ROOT-9311. The verbosity level can also be changed with the environment variable. XRD_LOGLEVEL. The possible levels are: ""Info"", ""Debug"", ""Dump"" .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2243
https://github.com/root-project/root/pull/2243:35,safety,log,log,35,"netxng: add possibility to control log verbosity with NetXNG.Debug; Addresses ROOT-9311. The verbosity level can also be changed with the environment variable. XRD_LOGLEVEL. The possible levels are: ""Info"", ""Debug"", ""Dump"" .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2243
https://github.com/root-project/root/pull/2243:27,security,control,control,27,"netxng: add possibility to control log verbosity with NetXNG.Debug; Addresses ROOT-9311. The verbosity level can also be changed with the environment variable. XRD_LOGLEVEL. The possible levels are: ""Info"", ""Debug"", ""Dump"" .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2243
https://github.com/root-project/root/pull/2243:35,security,log,log,35,"netxng: add possibility to control log verbosity with NetXNG.Debug; Addresses ROOT-9311. The verbosity level can also be changed with the environment variable. XRD_LOGLEVEL. The possible levels are: ""Info"", ""Debug"", ""Dump"" .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2243
https://github.com/root-project/root/pull/2243:27,testability,control,control,27,"netxng: add possibility to control log verbosity with NetXNG.Debug; Addresses ROOT-9311. The verbosity level can also be changed with the environment variable. XRD_LOGLEVEL. The possible levels are: ""Info"", ""Debug"", ""Dump"" .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2243
https://github.com/root-project/root/pull/2243:35,testability,log,log,35,"netxng: add possibility to control log verbosity with NetXNG.Debug; Addresses ROOT-9311. The verbosity level can also be changed with the environment variable. XRD_LOGLEVEL. The possible levels are: ""Info"", ""Debug"", ""Dump"" .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2243
https://github.com/root-project/root/pull/2244:44,performance,disk,disk,44,"[cling] Check file is C_User before hitting disk. Thanks, Vassil!;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2244
https://github.com/root-project/root/pull/2245:79,deployability,version,version,79,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:41,energy efficiency,model,model,41,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:79,integrability,version,version,79,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:210,integrability,event,event,210,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:79,modifiability,version,version,79,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:51,performance,parallel,parallelism,51,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:182,performance,I/O,I/O,182,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:264,performance,I/O,I/O,264,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:372,safety,test,testing,372,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:430,safety,test,testing,430,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:41,security,model,model,41,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:372,testability,test,testing,372,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:430,testability,test,testing,430,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:34,usability,user,users,34,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2245:346,usability,progress,progress,346,"Make TBufferMerger agnostic about users' model for parallelism (v2); The first version of these changes, in PR #1737, was tried by CMS and seems to be ok for them (see [CMS and ROOT I/O](https://indico.cern.ch/event/715802/contributions/2942558/) talk from latest I/O workshop), so we applied their suggestion for not blocking when a merge is in progress, and after local testing this is ready for merge into master after further testing here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2245
https://github.com/root-project/root/pull/2246:4,performance,cach,cache,4,Set cache file dir when using CACHEREAD option; Must call TFile::SetCacheFileDir in order for CACHEREAD option to work. Makes the test work in offline mode with pre-cached file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2246
https://github.com/root-project/root/pull/2246:30,performance,CACH,CACHEREAD,30,Set cache file dir when using CACHEREAD option; Must call TFile::SetCacheFileDir in order for CACHEREAD option to work. Makes the test work in offline mode with pre-cached file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2246
https://github.com/root-project/root/pull/2246:94,performance,CACH,CACHEREAD,94,Set cache file dir when using CACHEREAD option; Must call TFile::SetCacheFileDir in order for CACHEREAD option to work. Makes the test work in offline mode with pre-cached file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2246
https://github.com/root-project/root/pull/2246:165,performance,cach,cached,165,Set cache file dir when using CACHEREAD option; Must call TFile::SetCacheFileDir in order for CACHEREAD option to work. Makes the test work in offline mode with pre-cached file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2246
https://github.com/root-project/root/pull/2246:130,safety,test,test,130,Set cache file dir when using CACHEREAD option; Must call TFile::SetCacheFileDir in order for CACHEREAD option to work. Makes the test work in offline mode with pre-cached file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2246
https://github.com/root-project/root/pull/2246:130,testability,test,test,130,Set cache file dir when using CACHEREAD option; Must call TFile::SetCacheFileDir in order for CACHEREAD option to work. Makes the test work in offline mode with pre-cached file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2246
https://github.com/root-project/root/pull/2247:222,availability,redund,redundant,222,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:174,deployability,instal,installed,174,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:197,deployability,instal,install,197,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:222,deployability,redundan,redundant,222,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:222,reliability,redundan,redundant,222,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:8,safety,test,test,8,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:57,safety,Test,Test,57,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:156,safety,test,test,156,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:210,safety,test,testing,210,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:222,safety,redund,redundant,222,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:8,testability,test,test,8,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:57,testability,Test,Test,57,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:156,testability,test,test,156,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2247:210,testability,test,testing,210,"Include test directory using ROOT_ADD_TEST_SUBDIRECTORY; Test directories should be added using ROOT_ADD_TEST_SUBDIRECTORY. If add_subdirectory is used the test binaries get installed during ""make install"". if(testing) is redundant - it is already part of the ROOT_ADD_TEST_SUBDIRECTORY macro.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2247
https://github.com/root-project/root/pull/2248:52,energy efficiency,reduc,reduce,52,[IO] Do not register mem files; This change aims to reduce contention when creating TBufferMergerFiles,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2248
https://github.com/root-project/root/pull/2248:59,performance,content,contention,59,[IO] Do not register mem files; This change aims to reduce contention when creating TBufferMergerFiles,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2248
https://github.com/root-project/root/pull/2249:12,testability,trace,traces,12,Remove more traces of ios and chirp.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2249
https://github.com/root-project/root/pull/2250:8,safety,Test,Test,8,[cling] Test printValue on unutterable types.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2250
https://github.com/root-project/root/pull/2250:8,testability,Test,Test,8,[cling] Test printValue on unutterable types.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2250
https://github.com/root-project/root/pull/2251:63,availability,failur,failures,63,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:63,deployability,fail,failures,63,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:175,deployability,patch,patch,175,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:474,energy efficiency,reduc,reduced,474,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:320,integrability,batch,batches,320,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:428,modifiability,variab,variability,428,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:63,performance,failur,failures,63,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:320,performance,batch,batches,320,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:63,reliability,fail,failures,63,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:150,reliability,doe,does,150,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:31,safety,input,input,31,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:58,safety,test,test,58,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:108,safety,test,test,108,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:113,safety,input,input,113,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:130,safety,input,inputs,130,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:175,safety,patch,patch,175,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:231,safety,input,input,231,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:244,safety,test,tests,244,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:443,safety,test,test,443,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:175,security,patch,patch,175,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:58,testability,test,test,58,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:108,testability,test,test,108,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:244,testability,test,tests,244,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:443,testability,test,test,443,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:7,usability,Minim,Minimization-DNN,7,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:31,usability,input,input,31,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:113,usability,input,input,113,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:130,usability,input,inputs,130,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2251:231,usability,input,input,231,"[TMVA] Minimization-DNN remove input randomness; Sporadic test failures are caused by the randomness of the test input. (For some inputs the training does not converge). This patch ""fixes"". this by removing the randomness from the input to the tests. One source of randomness still remains, the dataloader shuffles the. batches internally, relying on a source of randomness that is not. reachable from the outside. However, the variability in test output is _significantly_ reduced.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2251
https://github.com/root-project/root/pull/2252:1,energy efficiency,Core,Core,1,[Core] Replace lockguard with read lockguard in a hotspot;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2252
https://github.com/root-project/root/pull/2252:15,performance,lock,lockguard,15,[Core] Replace lockguard with read lockguard in a hotspot;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2252
https://github.com/root-project/root/pull/2252:35,performance,lock,lockguard,35,[Core] Replace lockguard with read lockguard in a hotspot;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2252
https://github.com/root-project/root/pull/2252:50,safety,hot,hotspot,50,[Core] Replace lockguard with read lockguard in a hotspot;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2252
https://github.com/root-project/root/pull/2252:15,security,lock,lockguard,15,[Core] Replace lockguard with read lockguard in a hotspot;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2252
https://github.com/root-project/root/pull/2252:35,security,lock,lockguard,35,[Core] Replace lockguard with read lockguard in a hotspot;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2252
https://github.com/root-project/root/pull/2253:31,availability,error,error,31,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2253
https://github.com/root-project/root/pull/2253:37,integrability,messag,message,37,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2253
https://github.com/root-project/root/pull/2253:37,interoperability,messag,message,37,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2253
https://github.com/root-project/root/pull/2253:31,performance,error,error,31,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2253
https://github.com/root-project/root/pull/2253:31,safety,error,error,31,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2253
https://github.com/root-project/root/pull/2253:31,usability,error,error,31,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2253
https://github.com/root-project/root/pull/2254:20,safety,Avoid,Avoid,20,ROOT-9435 [TMVA] -- Avoid unsafe reinterpret_cast;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2254
https://github.com/root-project/root/pull/2254:26,safety,unsaf,unsafe,26,ROOT-9435 [TMVA] -- Avoid unsafe reinterpret_cast;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2254
https://github.com/root-project/root/pull/2255:13,safety,Test,Testing,13,[cxxmodules] Testing! Kill preloading allmodules; I'm hoping that this works as we're resolving our undefined symbols by. autoloading libraries (which is stronger than pcm!),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2255
https://github.com/root-project/root/pull/2255:13,testability,Test,Testing,13,[cxxmodules] Testing! Kill preloading allmodules; I'm hoping that this works as we're resolving our undefined symbols by. autoloading libraries (which is stronger than pcm!),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2255
https://github.com/root-project/root/pull/2258:31,availability,error,error,31,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2258
https://github.com/root-project/root/pull/2258:37,integrability,messag,message,37,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2258
https://github.com/root-project/root/pull/2258:37,interoperability,messag,message,37,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2258
https://github.com/root-project/root/pull/2258:31,performance,error,error,31,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2258
https://github.com/root-project/root/pull/2258:31,safety,error,error,31,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2258
https://github.com/root-project/root/pull/2258:31,usability,error,error,31,[DF] Remove obsolete part from error message;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2258
https://github.com/root-project/root/pull/2259:440,availability,operat,operations,440,"[DF] Custom column managed locally ROOT-9465; Before this PR only the root node, the RLoopManager, kept the list of the custom columns defined by the user. This meant that it was not possible to define two columns with the same names in two different branches. After this PR, each node in the graph has a map with pointers to all the columns defined up to that node. Nodes in different branches may see different custom column. This allows operations like the following to work:. ```. ROOT::RDataFrame d(3);. auto branch1 = d.Define(""b2"", []() { return 1; });. auto branch2 = d.Define(""b2"", []() { return 2; });. ```. This commit resolves the issue ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2259
https://github.com/root-project/root/pull/2259:19,deployability,manag,managed,19,"[DF] Custom column managed locally ROOT-9465; Before this PR only the root node, the RLoopManager, kept the list of the custom columns defined by the user. This meant that it was not possible to define two columns with the same names in two different branches. After this PR, each node in the graph has a map with pointers to all the columns defined up to that node. Nodes in different branches may see different custom column. This allows operations like the following to work:. ```. ROOT::RDataFrame d(3);. auto branch1 = d.Define(""b2"", []() { return 1; });. auto branch2 = d.Define(""b2"", []() { return 2; });. ```. This commit resolves the issue ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2259
https://github.com/root-project/root/pull/2259:19,energy efficiency,manag,managed,19,"[DF] Custom column managed locally ROOT-9465; Before this PR only the root node, the RLoopManager, kept the list of the custom columns defined by the user. This meant that it was not possible to define two columns with the same names in two different branches. After this PR, each node in the graph has a map with pointers to all the columns defined up to that node. Nodes in different branches may see different custom column. This allows operations like the following to work:. ```. ROOT::RDataFrame d(3);. auto branch1 = d.Define(""b2"", []() { return 1; });. auto branch2 = d.Define(""b2"", []() { return 2; });. ```. This commit resolves the issue ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2259
https://github.com/root-project/root/pull/2259:19,safety,manag,managed,19,"[DF] Custom column managed locally ROOT-9465; Before this PR only the root node, the RLoopManager, kept the list of the custom columns defined by the user. This meant that it was not possible to define two columns with the same names in two different branches. After this PR, each node in the graph has a map with pointers to all the columns defined up to that node. Nodes in different branches may see different custom column. This allows operations like the following to work:. ```. ROOT::RDataFrame d(3);. auto branch1 = d.Define(""b2"", []() { return 1; });. auto branch2 = d.Define(""b2"", []() { return 2; });. ```. This commit resolves the issue ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2259
https://github.com/root-project/root/pull/2259:5,usability,Custom,Custom,5,"[DF] Custom column managed locally ROOT-9465; Before this PR only the root node, the RLoopManager, kept the list of the custom columns defined by the user. This meant that it was not possible to define two columns with the same names in two different branches. After this PR, each node in the graph has a map with pointers to all the columns defined up to that node. Nodes in different branches may see different custom column. This allows operations like the following to work:. ```. ROOT::RDataFrame d(3);. auto branch1 = d.Define(""b2"", []() { return 1; });. auto branch2 = d.Define(""b2"", []() { return 2; });. ```. This commit resolves the issue ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2259
https://github.com/root-project/root/pull/2259:120,usability,custom,custom,120,"[DF] Custom column managed locally ROOT-9465; Before this PR only the root node, the RLoopManager, kept the list of the custom columns defined by the user. This meant that it was not possible to define two columns with the same names in two different branches. After this PR, each node in the graph has a map with pointers to all the columns defined up to that node. Nodes in different branches may see different custom column. This allows operations like the following to work:. ```. ROOT::RDataFrame d(3);. auto branch1 = d.Define(""b2"", []() { return 1; });. auto branch2 = d.Define(""b2"", []() { return 2; });. ```. This commit resolves the issue ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2259
https://github.com/root-project/root/pull/2259:150,usability,user,user,150,"[DF] Custom column managed locally ROOT-9465; Before this PR only the root node, the RLoopManager, kept the list of the custom columns defined by the user. This meant that it was not possible to define two columns with the same names in two different branches. After this PR, each node in the graph has a map with pointers to all the columns defined up to that node. Nodes in different branches may see different custom column. This allows operations like the following to work:. ```. ROOT::RDataFrame d(3);. auto branch1 = d.Define(""b2"", []() { return 1; });. auto branch2 = d.Define(""b2"", []() { return 2; });. ```. This commit resolves the issue ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2259
https://github.com/root-project/root/pull/2259:413,usability,custom,custom,413,"[DF] Custom column managed locally ROOT-9465; Before this PR only the root node, the RLoopManager, kept the list of the custom columns defined by the user. This meant that it was not possible to define two columns with the same names in two different branches. After this PR, each node in the graph has a map with pointers to all the columns defined up to that node. Nodes in different branches may see different custom column. This allows operations like the following to work:. ```. ROOT::RDataFrame d(3);. auto branch1 = d.Define(""b2"", []() { return 1; });. auto branch2 = d.Define(""b2"", []() { return 2; });. ```. This commit resolves the issue ROOT-9465.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2259
https://github.com/root-project/root/pull/2260:17,deployability,updat,update,17,schema evolution update for v614; backport of Fix for https://github.com/cms-sw/cmssw/pull/22594,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2260
https://github.com/root-project/root/pull/2260:0,integrability,schema,schema,0,schema evolution update for v614; backport of Fix for https://github.com/cms-sw/cmssw/pull/22594,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2260
https://github.com/root-project/root/pull/2260:17,safety,updat,update,17,schema evolution update for v614; backport of Fix for https://github.com/cms-sw/cmssw/pull/22594,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2260
https://github.com/root-project/root/pull/2260:17,security,updat,update,17,schema evolution update for v614; backport of Fix for https://github.com/cms-sw/cmssw/pull/22594,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2260
https://github.com/root-project/root/pull/2261:0,deployability,Updat,Update,0,Update RDF code owners;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2261
https://github.com/root-project/root/pull/2261:0,safety,Updat,Update,0,Update RDF code owners;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2261
https://github.com/root-project/root/pull/2261:0,security,Updat,Update,0,Update RDF code owners;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2261
https://github.com/root-project/root/pull/2263:214,availability,Error,Error,214,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:635,availability,error,error,635,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:324,deployability,build,builddir,324,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:333,deployability,build,build,333,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:339,deployability,BUILD,BUILD,339,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:404,deployability,build,builddir,404,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:413,deployability,build,build,413,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:419,deployability,BUILD,BUILD,419,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:554,deployability,build,builddir,554,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:563,deployability,build,build,563,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:569,deployability,BUILD,BUILD,569,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:771,deployability,build,builddir,771,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:780,deployability,build,build,780,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:786,deployability,BUILD,BUILD,786,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:898,deployability,build,builddir,898,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:907,deployability,build,build,907,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:913,deployability,BUILD,BUILD,913,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:220,integrability,messag,message,220,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:220,interoperability,messag,message,220,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1185,interoperability,convers,conversion,1185,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1473,interoperability,convers,conversion,1473,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:214,performance,Error,Error,214,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:635,performance,error,error,635,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:4,safety,test,test,4,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:68,safety,test,test,68,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:214,safety,Error,Error,214,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:373,safety,test,test,373,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:453,safety,test,test,453,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:539,safety,Test,TestBody,539,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:603,safety,test,test,603,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:635,safety,error,error,635,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:676,safety,test,testing,676,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:820,safety,test,test,820,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:947,safety,test,test,947,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1020,safety,test,testing,1020,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1297,safety,test,testing,1297,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1345,safety,test,testing,1345,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1544,safety,test,testing,1544,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:4,testability,test,test,4,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:68,testability,test,test,68,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:373,testability,test,test,373,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:453,testability,test,test,453,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:539,testability,Test,TestBody,539,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:603,testability,test,test,603,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:676,testability,test,testing,676,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:685,testability,Assert,AssertionResult,685,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:702,testability,Assert,AssertionResult,702,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:820,testability,test,test,820,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:947,testability,test,test,947,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1020,testability,test,testing,1020,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1029,testability,Assert,AssertionResult,1029,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1046,testability,Assert,AssertionResult,1046,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1078,testability,Assert,AssertionResult,1078,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1297,testability,test,testing,1297,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1306,testability,Assert,AssertionResult,1306,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1323,testability,Assert,AssertionResult,1323,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1345,testability,test,testing,1345,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1354,testability,Assert,AssertionResult,1354,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1373,testability,Assert,AssertionResult,1373,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1395,testability,Assert,AssertionResult,1395,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1544,testability,test,testing,1544,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:1553,testability,Assert,AssertionResult,1553,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:214,usability,Error,Error,214,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2263:635,usability,error,error,635,"Fix test compilation on RHEL 7; Fix compilation of dataframe-resptr test on RHEL/EPEL 7 using the default compiler (gcc 4.8.5) and the gtest library provided by the system (gtest 1.6.0) instead of the bundled one. Error message with the original sources:. ````. In file included from /usr/include/gtest/gtest.h:57:0,. from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx: In member function 'virtual void RResultPtr_ImplConv_Test::TestBody()':. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: error: no matching function for call to 'testing::AssertionResult::AssertionResult(ROOT::RDF::RResultPtr<TH1D>&)'. EXPECT_TRUE(m);. ^. /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:48:4: note: candidates are:. In file included from /builddir/build/BUILD/root-6.14.00/tree/dataframe/test/dataframe_resptr.cxx:2:0:. /usr/include/gtest/gtest.h:271:12: note: testing::AssertionResult::AssertionResult(bool). explicit AssertionResult(bool success) : success_(success) {}. ^. /usr/include/gtest/gtest.h:271:12: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'bool'. /usr/include/gtest/gtest.h:269:3: note: testing::AssertionResult::AssertionResult(const testing::AssertionResult&). AssertionResult(const AssertionResult& other);. ^. /usr/include/gtest/gtest.h:269:3: note: no known conversion for argument 1 from 'ROOT::RDF::RResultPtr<TH1D>' to 'const testing::AssertionResult&'. ````",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2263
https://github.com/root-project/root/pull/2264:44,deployability,configurat,configuration,44,Remove remaining references to removed Java configuration variables; These are no longer in use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2264
https://github.com/root-project/root/pull/2264:44,integrability,configur,configuration,44,Remove remaining references to removed Java configuration variables; These are no longer in use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2264
https://github.com/root-project/root/pull/2264:44,modifiability,configur,configuration,44,Remove remaining references to removed Java configuration variables; These are no longer in use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2264
https://github.com/root-project/root/pull/2264:58,modifiability,variab,variables,58,Remove remaining references to removed Java configuration variables; These are no longer in use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2264
https://github.com/root-project/root/pull/2264:44,security,configur,configuration,44,Remove remaining references to removed Java configuration variables; These are no longer in use.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2264
https://github.com/root-project/root/pull/2266:220,deployability,build,build,220,"[Experimental PyROOT] Generic pythonizations; There are generic pythonizations to be added. I've started with the pretty printing feature. Additionally, I've added a `test` directory with the unit-tests and adjusted the build system. More generic possible pythonizations:. - `__cppname__` attribute (It vanished in the new cppyy? I could do a workaround/pythonization.). - Ideas?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2266
https://github.com/root-project/root/pull/2266:167,safety,test,test,167,"[Experimental PyROOT] Generic pythonizations; There are generic pythonizations to be added. I've started with the pretty printing feature. Additionally, I've added a `test` directory with the unit-tests and adjusted the build system. More generic possible pythonizations:. - `__cppname__` attribute (It vanished in the new cppyy? I could do a workaround/pythonization.). - Ideas?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2266
https://github.com/root-project/root/pull/2266:197,safety,test,tests,197,"[Experimental PyROOT] Generic pythonizations; There are generic pythonizations to be added. I've started with the pretty printing feature. Additionally, I've added a `test` directory with the unit-tests and adjusted the build system. More generic possible pythonizations:. - `__cppname__` attribute (It vanished in the new cppyy? I could do a workaround/pythonization.). - Ideas?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2266
https://github.com/root-project/root/pull/2266:167,testability,test,test,167,"[Experimental PyROOT] Generic pythonizations; There are generic pythonizations to be added. I've started with the pretty printing feature. Additionally, I've added a `test` directory with the unit-tests and adjusted the build system. More generic possible pythonizations:. - `__cppname__` attribute (It vanished in the new cppyy? I could do a workaround/pythonization.). - Ideas?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2266
https://github.com/root-project/root/pull/2266:192,testability,unit,unit-tests,192,"[Experimental PyROOT] Generic pythonizations; There are generic pythonizations to be added. I've started with the pretty printing feature. Additionally, I've added a `test` directory with the unit-tests and adjusted the build system. More generic possible pythonizations:. - `__cppname__` attribute (It vanished in the new cppyy? I could do a workaround/pythonization.). - Ideas?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2266
https://github.com/root-project/root/pull/2267:273,availability,error,error,273,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:38,deployability,Build,Build,38,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:44,deployability,fail,fails,44,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:83,deployability,build,builddir,83,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:92,deployability,build,build,92,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:98,deployability,BUILD,BUILD,98,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:194,deployability,build,builddir,194,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:203,deployability,build,build,203,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:209,deployability,BUILD,BUILD,209,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:273,performance,error,error,273,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:44,reliability,fail,fails,44,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:127,safety,test,test,127,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:132,safety,Test,TestRandomGenerator,132,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:238,safety,test,test,238,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:243,safety,Test,TestRandomGenerator,243,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:273,safety,error,error,273,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:127,testability,test,test,127,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:132,testability,Test,TestRandomGenerator,132,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:238,testability,test,test,238,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:243,testability,Test,TestRandomGenerator,243,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2267:273,usability,error,error,273,"Add missing include for std::shuffle; Build fails on Fedora 27 (gcc 7.3.1):. ```. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx: In function 'void test_example()':. /builddir/build/BUILD/root-6.14.00/tmva/tmva/test/TestRandomGenerator.cxx:88:9: error: 'shuffle' is not a member of 'std'. std::shuffle(v.begin(), v.end(), rng);. ^~~~~~~. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2267
https://github.com/root-project/root/pull/2269:16,interoperability,Standard,Standard,16,[DF] Introduced Standard Deviation Action;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2269
https://github.com/root-project/root/pull/2273:1137,availability,error,error,1137,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:65,deployability,fail,fails,65,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1092,deployability,fail,failed,1092,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1219,deployability,fail,fail,1219,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1137,performance,error,error,1137,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:65,reliability,fail,fails,65,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1092,reliability,fail,failed,1092,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1210,reliability,doe,does,1210,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1219,reliability,fail,fail,1219,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:15,safety,test,test,15,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:60,safety,test,test,60,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:100,safety,Test,Testing,100,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1099,safety,test,tests,1099,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1137,safety,error,error,1137,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1205,safety,test,test,1205,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:15,testability,test,test,15,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:60,testability,test,test,60,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:100,testability,Test,Testing,100,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1099,testability,test,tests,1099,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1205,testability,test,test,1205,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2273:1137,usability,error,error,1137,"Adjust allowed test difference for 32-bit ix86; The Vavilov test fails on 32 bit ix86 in Fedora 29. Testing Cdf and Cdf_c. kappa = 0.01. Max abs diff: 1.2e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 6.2e-15, pass=0. kappa = 0.04. Max abs diff: 1.4e-07, max rel diff: 1.9e-05, max diff cdf+cdf_c-1: 1.1e-15, pass=1. kappa = 0.07. Max abs diff: 2.5e-06, max rel diff: 0.061, max diff cdf+cdf_c-1: 1.6e-15, pass=1. kappa = 0.1. Max abs diff: 6.1e-06, max rel diff: 0.038, max diff cdf+cdf_c-1: 6.7e-16, pass=1. kappa = 0.4. Max abs diff: 1.7e-06, max rel diff: 0.018, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 0.7. Max abs diff: 2.5e-06, max rel diff: 0.0092, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 1. Max abs diff: 1.9e-06, max rel diff: 0.0072, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 4. Max abs diff: 2e-06, max rel diff: 0.15, max diff cdf+cdf_c-1: 2.2e-16, pass=1. kappa = 7. Max abs diff: 1.7e-06, max rel diff: 0.026, max diff cdf+cdf_c-1: 4.4e-16, pass=1. kappa = 10. Max abs diff: 2.2e-06, max rel diff: 0.033, max diff cdf+cdf_c-1: 2.2e-16, pass=1. Number of failed tests: 1. This PR changes the allowed error from 5e-15 to 7e-15, allowing the 6.2e-15 result to pass. The test does not fail on Fedora 28 which uses the same compiler (gcc 8.1.1). The default compiler flags for ix86 have changed between Fedora 28 and 29 though, and the following flags were added: -msse2 -mfpmath=sse -mstackrealign. The -msse2 flag is my guess for the cause of the difference.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2273
https://github.com/root-project/root/pull/2276:165,availability,error,error,165,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:288,deployability,version,version,288,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:356,deployability,modul,modules,356,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:680,deployability,probe,probelem,680,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:288,integrability,version,version,288,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:288,modifiability,version,version,288,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:346,modifiability,extens,extension,346,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:356,modifiability,modul,modules,356,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:165,performance,error,error,165,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:165,safety,error,error,165,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:356,safety,modul,modules,356,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:427,safety,avoid,avoids,427,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:543,safety,avoid,avoid,543,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:165,usability,error,error,165,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:228,usability,document,documentation,228,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2276:510,usability,indicat,indicates,510,"Fixes for Python 3.7; Fixes for Python 3.7. * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the undefined symbol. I also removed the #ifndef R__WIN32 since the comment indicates that this was added to avoid the same issue on windows. Calling PyObject_GC_Track instead of using the _PyObject_GC_TRACK macro should fix the undefined symbol probelem on windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2276
https://github.com/root-project/root/pull/2277:32,performance,cach,cache,32,[DF] Add tutorial to illustrate cache;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2277
https://github.com/root-project/root/pull/2278:552,availability,consist,consists,552,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:202,interoperability,format,format,202,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:382,interoperability,Architectur,Architecture,382,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:664,interoperability,distribut,distribution,664,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:124,safety,test,tests,124,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:513,safety,test,testing,513,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:4,testability,regress,regression,4,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:56,testability,regress,regression,56,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:124,testability,test,tests,124,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:134,testability,Regress,Regression,134,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:191,testability,regress,regression,191,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:513,testability,test,testing,513,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:15,usability,support,support,15,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:96,usability,support,support,96,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2278:552,usability,consist,consists,552,"Add regression support for MethodDL; - Add multivariate regression for MethodDL . - Multi class support for MethodDL. - Add tests for Regression. Targets can be added according to usual TMVA regression format:. ```. dataloader->AddTarget(""uniform1"");. dataloader->AddTarget(""uniform2"");. dataloader->AddTarget(""uniform_add"");. dataloader->AddTarget(""uniform_sub"");. ```. An example Architecture: . ``` TString layoutString(""Layout=RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID,DENSE|4|LINEAR"");```. ~~The root file used for testing has been added to the PR.~~ It consists of 10,000 examples with a dimensionality of 4. The first two features have been sampled from a uniform distribution (0,1). The third and fourth features are the sum and differences of the first two features respectively.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2278
https://github.com/root-project/root/pull/2279:7,deployability,Updat,Update,7,[DOCS] Update RDF release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2279
https://github.com/root-project/root/pull/2279:18,deployability,releas,release,18,[DOCS] Update RDF release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2279
https://github.com/root-project/root/pull/2279:7,safety,Updat,Update,7,[DOCS] Update RDF release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2279
https://github.com/root-project/root/pull/2279:7,security,Updat,Update,7,[DOCS] Update RDF release notes;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2279
https://github.com/root-project/root/pull/2280:26,availability,error,error,26,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:256,deployability,version,versions,256,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:384,deployability,patch,patch,384,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:12,integrability,buffer,buffer,12,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:114,integrability,buffer,buffer,114,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:256,integrability,version,versions,256,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:256,modifiability,version,versions,256,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:26,performance,error,error,26,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:465,reliability,doe,does,465,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:26,safety,error,error,26,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:384,safety,patch,patch,384,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:384,security,patch,patch,384,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2280:26,usability,error,error,26,"TXMLEngine: buffer expand error; In TXMLEngine in several places pointer was not recalculated after expand of the buffer. Problem exists since the beginning (approx 2004), but never appeared before. Most probably while realloc() function in previous glibc versions was always returning same pointer value. Now it is not a case. Reported here: https://root-forum.cern.ch/t/29593. Same patch can be applied to all previous ROOT branches. P.S. cmake /path/to/roottest does not work for me",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2280
https://github.com/root-project/root/pull/2284:0,interoperability,Xml,Xml,0,Xml expand 614; Copy of https://github.com/root-project/root/pull/2280,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2284
https://github.com/root-project/root/pull/2285:0,interoperability,Xml,Xml,0,Xml expand 612; Copy of https://github.com/root-project/root/pull/2280,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2285
https://github.com/root-project/root/pull/2286:50,energy efficiency,reduc,reducing,50,Geom fixes; Fixes for ROOT-9402 and ROOT-7917 and reducing printout verbosity in the GDML parser,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2286
https://github.com/root-project/root/pull/2289:181,modifiability,variab,variable,181,"[DF] Throw if name of defined column is not a valid C++ var name; Since v6.14, `RDataFrame` chokes (while jitting) in case the name of a `Define`'s custom column is not a valid C++ variable name. It didn't before, by accident. This PR adds a check for this pre-condition and makes sure that an exception is thrown if it's not respected.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2289
https://github.com/root-project/root/pull/2289:46,safety,valid,valid,46,"[DF] Throw if name of defined column is not a valid C++ var name; Since v6.14, `RDataFrame` chokes (while jitting) in case the name of a `Define`'s custom column is not a valid C++ variable name. It didn't before, by accident. This PR adds a check for this pre-condition and makes sure that an exception is thrown if it's not respected.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2289
https://github.com/root-project/root/pull/2289:171,safety,valid,valid,171,"[DF] Throw if name of defined column is not a valid C++ var name; Since v6.14, `RDataFrame` chokes (while jitting) in case the name of a `Define`'s custom column is not a valid C++ variable name. It didn't before, by accident. This PR adds a check for this pre-condition and makes sure that an exception is thrown if it's not respected.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2289
https://github.com/root-project/root/pull/2289:217,safety,accid,accident,217,"[DF] Throw if name of defined column is not a valid C++ var name; Since v6.14, `RDataFrame` chokes (while jitting) in case the name of a `Define`'s custom column is not a valid C++ variable name. It didn't before, by accident. This PR adds a check for this pre-condition and makes sure that an exception is thrown if it's not respected.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2289
https://github.com/root-project/root/pull/2289:294,safety,except,exception,294,"[DF] Throw if name of defined column is not a valid C++ var name; Since v6.14, `RDataFrame` chokes (while jitting) in case the name of a `Define`'s custom column is not a valid C++ variable name. It didn't before, by accident. This PR adds a check for this pre-condition and makes sure that an exception is thrown if it's not respected.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2289
https://github.com/root-project/root/pull/2289:148,usability,custom,custom,148,"[DF] Throw if name of defined column is not a valid C++ var name; Since v6.14, `RDataFrame` chokes (while jitting) in case the name of a `Define`'s custom column is not a valid C++ variable name. It didn't before, by accident. This PR adds a check for this pre-condition and makes sure that an exception is thrown if it's not respected.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2289
https://github.com/root-project/root/pull/2291:0,deployability,updat,updated,0,updated doxygen list; Fixed doxygen list here: https://root.cern.ch/doc/master/classROOT_1_1RDF_1_1RDataSource.html,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2291
https://github.com/root-project/root/pull/2291:0,safety,updat,updated,0,updated doxygen list; Fixed doxygen list here: https://root.cern.ch/doc/master/classROOT_1_1RDF_1_1RDataSource.html,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2291
https://github.com/root-project/root/pull/2291:0,security,updat,updated,0,updated doxygen list; Fixed doxygen list here: https://root.cern.ch/doc/master/classROOT_1_1RDF_1_1RDataSource.html,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2291
https://github.com/root-project/root/pull/2292:46,safety,valid,valid,46,[DF] Throw if name of defined column is not a valid C++ var name; @etejedor @dpiparo do we agree that this is the behavior we want?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2292
https://github.com/root-project/root/pull/2292:114,usability,behavi,behavior,114,[DF] Throw if name of defined column is not a valid C++ var name; @etejedor @dpiparo do we agree that this is the behavior we want?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2292
https://github.com/root-project/root/pull/2293:90,availability,avail,available,90,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2293:67,modifiability,variab,variable,67,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2293:90,reliability,availab,available,90,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2293:108,reliability,doe,doesn,108,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2293:90,safety,avail,available,90,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2293:160,safety,test,tests,160,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2293:90,security,availab,available,90,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2293:160,testability,test,tests,160,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2293:167,usability,Help,Help,167,"Add R__HAS_VARIABLE_TEMPLATES macro; It should be defined if C++14 variable templates are available, but it doesn't seem to work -- it's always undefined in my tests. Help? :sweat_smile:",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2293
https://github.com/root-project/root/pull/2294:129,deployability,contain,containing,129,"[DF] Add MakeVec<N,T> helper function; MakeVec<N, T> is a callable that takes N arguments of type T and returns a std::vector<T> containing copies of the arguments. Example usage:. ```c++. df.Define(""all_triggers"", MakeVec<3, bool>, {""trigger1"", ""trigger2"", ""trigger3""}). ```. @stwunsch might find this useful.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2294
https://github.com/root-project/root/pull/2294:22,usability,help,helper,22,"[DF] Add MakeVec<N,T> helper function; MakeVec<N, T> is a callable that takes N arguments of type T and returns a std::vector<T> containing copies of the arguments. Example usage:. ```c++. df.Define(""all_triggers"", MakeVec<3, bool>, {""trigger1"", ""trigger2"", ""trigger3""}). ```. @stwunsch might find this useful.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2294
https://github.com/root-project/root/pull/2295:441,integrability,Filter,Filter,441,"[DF] Add PassAsVec helper function; Thanks @amadio for the suggestion! `PassAsVec<N, T>(func)` is a callable that takes N arguments of type T,. passes them to func as a collection (`func({v1, v2, ...}`) and returns. the result of the call to `func`. This helper makes it possible to pass several columns of the same. type to a callable that accepts a vector of that type. Example usage:. ```c++. bool myVecFunc(std::vector<float> args);. df.Filter(PassAsVec<3, float>(myVecFunc), {""var1"", ""var2"", ""var3""});. ```. @stwunsch could this be interesting for the new TMVA interfaces?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2295
https://github.com/root-project/root/pull/2295:566,integrability,interfac,interfaces,566,"[DF] Add PassAsVec helper function; Thanks @amadio for the suggestion! `PassAsVec<N, T>(func)` is a callable that takes N arguments of type T,. passes them to func as a collection (`func({v1, v2, ...}`) and returns. the result of the call to `func`. This helper makes it possible to pass several columns of the same. type to a callable that accepts a vector of that type. Example usage:. ```c++. bool myVecFunc(std::vector<float> args);. df.Filter(PassAsVec<3, float>(myVecFunc), {""var1"", ""var2"", ""var3""});. ```. @stwunsch could this be interesting for the new TMVA interfaces?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2295
https://github.com/root-project/root/pull/2295:566,interoperability,interfac,interfaces,566,"[DF] Add PassAsVec helper function; Thanks @amadio for the suggestion! `PassAsVec<N, T>(func)` is a callable that takes N arguments of type T,. passes them to func as a collection (`func({v1, v2, ...}`) and returns. the result of the call to `func`. This helper makes it possible to pass several columns of the same. type to a callable that accepts a vector of that type. Example usage:. ```c++. bool myVecFunc(std::vector<float> args);. df.Filter(PassAsVec<3, float>(myVecFunc), {""var1"", ""var2"", ""var3""});. ```. @stwunsch could this be interesting for the new TMVA interfaces?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2295
https://github.com/root-project/root/pull/2295:566,modifiability,interfac,interfaces,566,"[DF] Add PassAsVec helper function; Thanks @amadio for the suggestion! `PassAsVec<N, T>(func)` is a callable that takes N arguments of type T,. passes them to func as a collection (`func({v1, v2, ...}`) and returns. the result of the call to `func`. This helper makes it possible to pass several columns of the same. type to a callable that accepts a vector of that type. Example usage:. ```c++. bool myVecFunc(std::vector<float> args);. df.Filter(PassAsVec<3, float>(myVecFunc), {""var1"", ""var2"", ""var3""});. ```. @stwunsch could this be interesting for the new TMVA interfaces?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2295
https://github.com/root-project/root/pull/2295:19,usability,help,helper,19,"[DF] Add PassAsVec helper function; Thanks @amadio for the suggestion! `PassAsVec<N, T>(func)` is a callable that takes N arguments of type T,. passes them to func as a collection (`func({v1, v2, ...}`) and returns. the result of the call to `func`. This helper makes it possible to pass several columns of the same. type to a callable that accepts a vector of that type. Example usage:. ```c++. bool myVecFunc(std::vector<float> args);. df.Filter(PassAsVec<3, float>(myVecFunc), {""var1"", ""var2"", ""var3""});. ```. @stwunsch could this be interesting for the new TMVA interfaces?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2295
https://github.com/root-project/root/pull/2295:255,usability,help,helper,255,"[DF] Add PassAsVec helper function; Thanks @amadio for the suggestion! `PassAsVec<N, T>(func)` is a callable that takes N arguments of type T,. passes them to func as a collection (`func({v1, v2, ...}`) and returns. the result of the call to `func`. This helper makes it possible to pass several columns of the same. type to a callable that accepts a vector of that type. Example usage:. ```c++. bool myVecFunc(std::vector<float> args);. df.Filter(PassAsVec<3, float>(myVecFunc), {""var1"", ""var2"", ""var3""});. ```. @stwunsch could this be interesting for the new TMVA interfaces?",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2295
https://github.com/root-project/root/pull/2296:57,deployability,releas,releases,57,jsroot 5.5.0; See https://github.com/root-project/jsroot/releases/tag/5.5.0 for details.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2296
https://github.com/root-project/root/pull/2299:25,usability,help,helpers,25,[DF] Add tutorial on RDF helpers;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2299
https://github.com/root-project/root/pull/2301:104,deployability,log,logically,104,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:325,energy efficiency,reduc,reduction,325,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:612,energy efficiency,CPU,CPU,612,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:374,interoperability,architectur,architecture,374,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:14,modifiability,Layer,Layer,14,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:87,modifiability,layer,layer,87,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:612,performance,CPU,CPU,612,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:104,safety,log,logically,104,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:191,safety,compl,complex,191,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:422,safety,test,test,422,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:104,security,log,logically,104,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:191,security,compl,complex,191,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:104,testability,log,logically,104,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:258,testability,simpl,simpler,258,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:422,testability,test,test,422,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:649,testability,assert,asserted,649,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:258,usability,simpl,simpler,258,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2301:276,usability,Help,Helper,276,"Convolutional Layer in CUDA; This is a PR including a lot of work on the convolutional layer. It can be logically divided the following parts:. 1. Back propagation in CUDA (this was the most complex). 2. Forward propagation in CUDA (this was a comparatively simpler task). 3. Helper static methods such as a column based sum reduction and matrix equality checkers for every architecture. 4. An element by element detailed test case for backward and forward propagation. The one on forward propagation was designed and solved on paper. I was not able to do the same for back-propagation, instead I considered the CPU implementation to be correct and asserted that the CUDA outputs the same result. 5. Trivial bug fixes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2301
https://github.com/root-project/root/pull/2302:62,testability,Simpl,Simplifies,62,"[v7hist] Make STORAGE a type template arg, remove from STAT.; Simplifies types, considerably.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2302
https://github.com/root-project/root/pull/2302:62,usability,Simpl,Simplifies,62,"[v7hist] Make STORAGE a type template arg, remove from STAT.; Simplifies types, considerably.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2302
https://github.com/root-project/root/pull/2305:148,performance,synch,synchronized,148,"use printLevel for MN_INFO_MSG calls; The macros MN_INFO_MSG and friends check the global MnPrint::Level() before they print anything, which is not synchronized with the local printLevel in the class. The expected behavior is that MN_INFO_MSG and friends correspond to the local printLevel in the class. A call was added to synchronize MnPrint::Level() with the local printLevel.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2305
https://github.com/root-project/root/pull/2305:324,performance,synch,synchronize,324,"use printLevel for MN_INFO_MSG calls; The macros MN_INFO_MSG and friends check the global MnPrint::Level() before they print anything, which is not synchronized with the local printLevel in the class. The expected behavior is that MN_INFO_MSG and friends correspond to the local printLevel in the class. A call was added to synchronize MnPrint::Level() with the local printLevel.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2305
https://github.com/root-project/root/pull/2305:214,usability,behavi,behavior,214,"use printLevel for MN_INFO_MSG calls; The macros MN_INFO_MSG and friends check the global MnPrint::Level() before they print anything, which is not synchronized with the local printLevel in the class. The expected behavior is that MN_INFO_MSG and friends correspond to the local printLevel in the class. A call was added to synchronize MnPrint::Level() with the local printLevel.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2305
https://github.com/root-project/root/pull/2306:55,energy efficiency,CPU,CPU,55,Matrix compatibility; Make sure that our matrix types (CPU and GPU) use the same method name so that we can call them in a polymorphic manner from templates.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2306
https://github.com/root-project/root/pull/2306:63,energy efficiency,GPU,GPU,63,Matrix compatibility; Make sure that our matrix types (CPU and GPU) use the same method name so that we can call them in a polymorphic manner from templates.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2306
https://github.com/root-project/root/pull/2306:7,interoperability,compatib,compatibility,7,Matrix compatibility; Make sure that our matrix types (CPU and GPU) use the same method name so that we can call them in a polymorphic manner from templates.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2306
https://github.com/root-project/root/pull/2306:123,modifiability,polymorph,polymorphic,123,Matrix compatibility; Make sure that our matrix types (CPU and GPU) use the same method name so that we can call them in a polymorphic manner from templates.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2306
https://github.com/root-project/root/pull/2306:55,performance,CPU,CPU,55,Matrix compatibility; Make sure that our matrix types (CPU and GPU) use the same method name so that we can call them in a polymorphic manner from templates.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2306
https://github.com/root-project/root/pull/2306:63,performance,GPU,GPU,63,Matrix compatibility; Make sure that our matrix types (CPU and GPU) use the same method name so that we can call them in a polymorphic manner from templates.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2306
https://github.com/root-project/root/pull/2307:35,deployability,modul,modulemaps,35,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:75,deployability,patch,patch,75,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:180,deployability,modul,modules,180,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:348,deployability,Modul,ModuleName,348,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:548,deployability,modul,modulemaps,548,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:688,deployability,Modul,ModuleName,688,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:298,energy efficiency,load,loadSubdirectoryModuleMaps,298,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:543,energy efficiency,load,load,543,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:22,integrability,sub,subdirectory,22,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:447,integrability,sub,subdirectories,447,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:562,integrability,sub,subdirectories,562,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:51,interoperability,specif,specific,51,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:35,modifiability,modul,modulemaps,35,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:180,modifiability,modul,modules,180,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:348,modifiability,Modul,ModuleName,348,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:548,modifiability,modul,modulemaps,548,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:688,modifiability,Modul,ModuleName,688,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:298,performance,load,loadSubdirectoryModuleMaps,298,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:543,performance,load,load,543,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:603,performance,disk,disk,603,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:35,safety,modul,modulemaps,35,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:75,safety,patch,patch,75,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:107,safety,review,reviews,107,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:180,safety,modul,modules,180,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:348,safety,Modul,ModuleName,348,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:548,safety,modul,modulemaps,548,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:688,safety,Modul,ModuleName,688,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:75,security,patch,patch,75,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:608,security,access,access,608,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:107,testability,review,reviews,107,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2307:515,usability,User,Users,515,"[cxxmodules] Autoload subdirectory modulemaps with specific LangOpts; This patch already landed in https://reviews.llvm.org/rL336660 in Clang. This was biting us to enable runtime modules in CMSSW. Detailed desciption:. https://bugs.llvm.org/show_bug.cgi?id=37878. lookupModule was falling back to loadSubdirectoryModuleMaps when it couldn't. find ModuleName in (proper) search paths. This was causing iteration over all. files in the search path subdirectories for example ""/usr/include/foobar"" in. bugzilla case. Users don't expect Clang to load modulemaps in subdirectories implicitly, and. also the disk access is not cheap. if (AllowExtraModuleMapSearch) true with ObjC with @import ModuleName.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2307
https://github.com/root-project/root/pull/2308:23,energy efficiency,draw,draw,23,Add the possibility to draw filled areas in NDC ; as requested here: https://sft.its.cern.ch/jira/browse/ROOT-9523,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2308
https://github.com/root-project/root/pull/2309:7,deployability,API,API-Support,7,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:46,deployability,API,API-Support,46,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:27,energy efficiency,Optim,Optimizer,27,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:66,energy efficiency,Optim,Optimizer,66,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:187,energy efficiency,Optim,Optimizer,187,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:257,energy efficiency,Optim,Optimizer,257,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:344,energy efficiency,Optim,Optimizer,344,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:7,integrability,API,API-Support,7,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:46,integrability,API,API-Support,46,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:409,integrability,Batch,BatchSize,409,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:7,interoperability,API,API-Support,7,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:46,interoperability,API,API-Support,46,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:27,performance,Optimiz,Optimizer,27,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:66,performance,Optimiz,Optimizer,66,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:187,performance,Optimiz,Optimizer,187,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:257,performance,Optimiz,Optimizer,257,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:344,performance,Optimiz,Optimizer,344,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:409,performance,Batch,BatchSize,409,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:173,safety,Test,Tests,173,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:220,safety,Test,Tests,220,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:423,safety,Test,TestRepetitions,423,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:200,security,Modif,Modify,200,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:168,testability,Unit,Unit,168,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:173,testability,Test,Tests,173,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:220,testability,Test,Tests,220,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:423,testability,Test,TestRepetitions,423,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:11,usability,Support,Support,11,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:50,usability,Support,Support,50,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2309:326,usability,Learn,LearningRate,326,"[TMVA] API-Support for SGD Optimizer:; [TMVA] API-Support for SGD Optimizer:. * Add Base Class VOptimizer. * Add Derived Class TSGD with Momentum implementation. * Add Unit Tests for SGD Optimizer. * Modify the MethodDL Tests to include parsing options for Optimizer. An example Training Strategy string may look like,. ```. ""LearningRate=1e-1,Optimizer=SGD,Momentum=0.9,Repetitions=1,"". ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,"". ""WeightDecay=1e-4,Regularization=L2,"". ""DropConfig=0.0+0.5+0.5+0.5, Multithreading=True"". ```. Reference Implementation: Tensorflow. Blog Post: https://www.sravikiran.com/GSOC18//2018/07/09/sgd/",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2309
https://github.com/root-project/root/pull/2313:65,availability,Operat,Operation,65,"[DF] [ROOT-9465] [ROOT-9457] [ROOT-9464] Multiple Custom Column, Operation Graph, GetDefinedColumns; This PR introduces the possibility to obtain a dot representation of the operations graph in the Dataframe. This PR depends the the [PR-2259](https://github.com/root-project/root/pull/2259). This PR is related to the Jira issue ROOT-9458",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2313
https://github.com/root-project/root/pull/2313:174,availability,operat,operations,174,"[DF] [ROOT-9465] [ROOT-9457] [ROOT-9464] Multiple Custom Column, Operation Graph, GetDefinedColumns; This PR introduces the possibility to obtain a dot representation of the operations graph in the Dataframe. This PR depends the the [PR-2259](https://github.com/root-project/root/pull/2259). This PR is related to the Jira issue ROOT-9458",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2313
https://github.com/root-project/root/pull/2313:217,deployability,depend,depends,217,"[DF] [ROOT-9465] [ROOT-9457] [ROOT-9464] Multiple Custom Column, Operation Graph, GetDefinedColumns; This PR introduces the possibility to obtain a dot representation of the operations graph in the Dataframe. This PR depends the the [PR-2259](https://github.com/root-project/root/pull/2259). This PR is related to the Jira issue ROOT-9458",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2313
https://github.com/root-project/root/pull/2313:217,integrability,depend,depends,217,"[DF] [ROOT-9465] [ROOT-9457] [ROOT-9464] Multiple Custom Column, Operation Graph, GetDefinedColumns; This PR introduces the possibility to obtain a dot representation of the operations graph in the Dataframe. This PR depends the the [PR-2259](https://github.com/root-project/root/pull/2259). This PR is related to the Jira issue ROOT-9458",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2313
https://github.com/root-project/root/pull/2313:217,modifiability,depend,depends,217,"[DF] [ROOT-9465] [ROOT-9457] [ROOT-9464] Multiple Custom Column, Operation Graph, GetDefinedColumns; This PR introduces the possibility to obtain a dot representation of the operations graph in the Dataframe. This PR depends the the [PR-2259](https://github.com/root-project/root/pull/2259). This PR is related to the Jira issue ROOT-9458",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2313
https://github.com/root-project/root/pull/2313:217,safety,depend,depends,217,"[DF] [ROOT-9465] [ROOT-9457] [ROOT-9464] Multiple Custom Column, Operation Graph, GetDefinedColumns; This PR introduces the possibility to obtain a dot representation of the operations graph in the Dataframe. This PR depends the the [PR-2259](https://github.com/root-project/root/pull/2259). This PR is related to the Jira issue ROOT-9458",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2313
https://github.com/root-project/root/pull/2313:217,testability,depend,depends,217,"[DF] [ROOT-9465] [ROOT-9457] [ROOT-9464] Multiple Custom Column, Operation Graph, GetDefinedColumns; This PR introduces the possibility to obtain a dot representation of the operations graph in the Dataframe. This PR depends the the [PR-2259](https://github.com/root-project/root/pull/2259). This PR is related to the Jira issue ROOT-9458",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2313
https://github.com/root-project/root/pull/2313:50,usability,Custom,Custom,50,"[DF] [ROOT-9465] [ROOT-9457] [ROOT-9464] Multiple Custom Column, Operation Graph, GetDefinedColumns; This PR introduces the possibility to obtain a dot representation of the operations graph in the Dataframe. This PR depends the the [PR-2259](https://github.com/root-project/root/pull/2259). This PR is related to the Jira issue ROOT-9458",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2313
https://github.com/root-project/root/pull/2314:129,integrability,event,eventSample,129,"[TMVA] ROOT-9495 BDTG erroneous output; Typo, parallel loop should have been looping over fEventSample instead of local variable eventSample.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2314
https://github.com/root-project/root/pull/2314:120,modifiability,variab,variable,120,"[TMVA] ROOT-9495 BDTG erroneous output; Typo, parallel loop should have been looping over fEventSample instead of local variable eventSample.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2314
https://github.com/root-project/root/pull/2314:46,performance,parallel,parallel,46,"[TMVA] ROOT-9495 BDTG erroneous output; Typo, parallel loop should have been looping over fEventSample instead of local variable eventSample.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2314
https://github.com/root-project/root/pull/2315:11,availability,operat,operations,11,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:54,deployability,updat,updates,54,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:44,energy efficiency,optim,optimizer,44,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:158,energy efficiency,CPU,CPU,158,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:163,energy efficiency,GPU,GPU,163,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:225,energy efficiency,CPU,CPU,225,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:230,energy efficiency,GPU,GPU,230,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:181,interoperability,architectur,architectures,181,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:234,interoperability,architectur,architecture,234,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:280,interoperability,format,format,280,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:33,performance,performing optim,performing optimizer,33,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:158,performance,CPU,CPU,158,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:163,performance,GPU,GPU,163,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:225,performance,CPU,CPU,225,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:230,performance,GPU,GPU,230,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:54,safety,updat,updates,54,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:207,safety,Test,Tests,207,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:54,security,updat,updates,54,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:202,testability,Unit,Unit,202,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:207,testability,Test,Tests,207,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2315:33,usability,perform,performing,33,"[TMVA] Add operations needed for performing optimizer updates:; * Implement ConstAdd, ConstMult, ReciprocalElementWise, SquareElementWise, SqrtElementWise in CPU, GPU and Reference architectures. * Add Unit Tests for them in CPU, GPU architecture. * Add ROOT Style docs and clang format the code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2315
https://github.com/root-project/root/pull/2316:283,energy efficiency,draw,drawing,283,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2316
https://github.com/root-project/root/pull/2316:338,interoperability,specif,specify,338,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2316
https://github.com/root-project/root/pull/2316:41,usability,custom,custom,41,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2316
https://github.com/root-project/root/pull/2316:67,usability,custom,custom,67,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2316
https://github.com/root-project/root/pull/2316:161,usability,help,helper,161,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2316
https://github.com/root-project/root/pull/2317:283,energy efficiency,draw,drawing,283,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2317
https://github.com/root-project/root/pull/2317:338,interoperability,specif,specify,338,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2317
https://github.com/root-project/root/pull/2317:41,usability,custom,custom,41,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2317
https://github.com/root-project/root/pull/2317:67,usability,custom,custom,67,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2317
https://github.com/root-project/root/pull/2317:161,usability,help,helper,161,"[DF] Introduced tutorial for Fill TGraph custom action; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2317
https://github.com/root-project/root/pull/2318:293,energy efficiency,draw,drawing,293,"[DF] Introduced tutorial for Fill TGraph custom action ROOT-9462; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2318
https://github.com/root-project/root/pull/2318:348,interoperability,specif,specify,348,"[DF] Introduced tutorial for Fill TGraph custom action ROOT-9462; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2318
https://github.com/root-project/root/pull/2318:41,usability,custom,custom,41,"[DF] Introduced tutorial for Fill TGraph custom action ROOT-9462; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2318
https://github.com/root-project/root/pull/2318:77,usability,custom,custom,77,"[DF] Introduced tutorial for Fill TGraph custom action ROOT-9462; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2318
https://github.com/root-project/root/pull/2318:171,usability,help,helper,171,"[DF] Introduced tutorial for Fill TGraph custom action ROOT-9462; Using this custom action it is possible to fill a TGraph starting from two Dataframe columns. Using this helper in Multithread, the order in which points are connected can't be forseen and may result in unexpected noise in the drawing. However, in the constructor it is possible to specify if a sort on the x axis is needed. This commit solves the Jira issue ROOT-9462",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2318
https://github.com/root-project/root/pull/2319:555,deployability,build,build,555,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:744,deployability,upgrad,upgraded,744,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:102,energy efficiency,reduc,reduce,102,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:590,energy efficiency,reduc,reduce,590,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:54,modifiability,inherit,inheritance,54,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:279,modifiability,inherit,inheritance,279,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:533,modifiability,inherit,inheritance,533,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:744,modifiability,upgrad,upgraded,744,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:113,performance,content,contention,113,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:395,performance,lock,lock,395,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:561,performance,time,time,561,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:612,performance,content,contention,612,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:662,performance,cach,caches,662,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:332,security,access,accessing,332,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:395,security,lock,lock,395,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2319:630,security,access,accesses,630,"[WIP] TClass knows if it or any ancestor has multiple inheritance when it has a dict; this PR aims to reduce the contention in TClass::GetBaseClassOffset. Zero is returned as offset value if the class that the TClass instance represents. and all of its ancestors has no multiple inheritance. This information can be checked without accessing the interpreter, and therewith. acquiring the global lock, because it now originally resides in the dictionaries. The information is put in the dictionaries by rootcling, which explores the. inheritance chain *at build time*. The expectation is to reduce *considerably* contention due to accesses to the interpreter and caches of offsets in the TClassInfo instances. Potentially, this mechanism can be upgraded inserting in the dictionaries. not only the aforementioned information, but also the offsets to all the. bases.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2319
https://github.com/root-project/root/pull/2321:633,availability,error,error,633,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:319,deployability,updat,update,319,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:656,deployability,fail,fails,656,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:179,integrability,pub,publishing,179,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:865,interoperability,architectur,architectures,865,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:6,modifiability,Refact,Refactor,6,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:942,modifiability,paramet,parameters,942,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1037,modifiability,paramet,parameters,1037,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1321,modifiability,layer,layer,1321,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1458,modifiability,paramet,parameters,1458,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1523,modifiability,paramet,parameters,1523,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1651,modifiability,layer,layer,1651,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:6,performance,Refactor,Refactor,6,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:633,performance,error,error,633,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:656,reliability,fail,fails,656,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:319,safety,updat,update,319,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:588,safety,test,tests,588,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:633,safety,error,error,633,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:651,safety,test,test,651,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:972,safety,input,inputs,972,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1601,safety,input,input,1601,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:63,security,ident,identified,63,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:319,security,updat,update,319,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:572,security,control,control,572,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:823,security,sign,signature,823,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:992,security,stride,strides,992,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1263,security,sign,signatures,1263,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1535,security,stride,stride,1535,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:534,testability,assert,asserted,534,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:572,testability,control,control,572,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:583,testability,unit,unit,583,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:588,testability,test,tests,588,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:597,testability,simpl,simplified,597,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:651,testability,test,test,651,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1248,testability,simpl,simplifies,1248,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1732,testability,simpl,simplify,1732,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:597,usability,simpl,simplified,597,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:633,usability,error,error,633,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:972,usability,input,inputs,972,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1248,usability,simpl,simplifies,1248,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1601,usability,input,input,1601,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2321:1732,usability,simpl,simplify,1732,"[DNM] Refactor TMVA; This PR addresses the code quality issues identified by @Axel-Naumann and @lmoneta . One of the points raised can be tackled in multiple ways. The purpose of publishing this PR now with the **[Do Not Merge]** tag is to facilitate discussion (see last section). I can then apply what we agree upon, update the PR and Lorenzo can merge. . ## Solved. The following issues have been addressed (1 commit per issue more or less): . * Compilation warnings are solved. * Assumptions about the size of passed matrices are asserted where appropriate. * Flow of control in unit tests is simplified by exiting early with an error code when a test fails. * Floating point arithmetic issue solved when checking whether a float is actually an int. ## Discussion. One of the comments has to do with the overly verbose signature of propagation functions in all architectures. This is caused because we always need to pass the convolution parameters besides the actual inputs and outputs (strides, padding, kernel sizes are already 7 parameters that are often passed together). . One proposed solution is to capture them in a struct (perhaps called `ConvParams`) and then pass this struct around instead. I like this approach because it greatly simplifies the signatures. . An alternative approach could be to let the layer methods pass a pointer (or reference) to the calling object (`this`). The advantage is that the verbose functions accept different parameters, for example the forward prop expects the convolution parameters (stride, padding, kernel size) while the backward pass expects the input and output size. Passing a reference to the layer itself will allow each function to pick what it needs, thus allowing us to simplify all calls with a common strategy. I would like to hear opinions on which method to choose.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2321
https://github.com/root-project/root/pull/2322:203,availability,monitor,monitoring,203,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:203,deployability,monitor,monitoring,203,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:653,deployability,integr,integration,653,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1147,deployability,API,API,1147,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:203,energy efficiency,monitor,monitoring,203,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1718,energy efficiency,current,current,1718,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:653,integrability,integration test,integration tests,653,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:919,integrability,wrap,wrap,919,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1068,integrability,interfac,interface,1068,"ataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something t",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1147,integrability,API,API,1147,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1648,integrability,sub,subsequent,1648,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:433,interoperability,distribut,distribution,433,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:653,interoperability,integr,integration,653,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1068,interoperability,interfac,interface,1068,"ataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something t",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1147,interoperability,API,API,1147,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:653,modifiability,integr,integration,653,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:990,modifiability,reu,reusing,990,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1068,modifiability,interfac,interface,1068,"ataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something t",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:203,reliability,monitor,monitoring,203,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:653,reliability,integr,integration,653,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1739,reliability,pra,practical,1739,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:203,safety,monitor,monitoring,203,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:665,safety,test,tests,665,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:857,safety,safe,safe,857,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1816,safety,avoid,avoid,1816,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:653,security,integr,integration,653,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:203,testability,monitor,monitoring,203,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:644,testability,Unit,Unit,644,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:653,testability,integr,integration,653,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:665,testability,test,tests,665,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:564,usability,progress,progress,564,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:612,usability,feedback,feedback,612,"SQlite data source for RDataFrame; This PR adds a new data source for `RDataFrame` that is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1775,usability,user,user,1775,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1869,usability,support,support,1869,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2322:1937,usability,custom,custom,1937,"is able to provide data from SQlite `SELECT` queries. It will be useful for cvmfs, where we have file catalogs and monitoring information in sqlite files. For instance, one can do. auto rdf = ROOT::RDF::MakeSqliteDataFrame(""catalog.sqlite"", ""select * from catalog"");. auto h = rdf.Define(""lname"", ""name.length()"").Histo1D(""lname"");. to show the distribution of file name sizes. There are probably more use cases, for instance sqlite export of conditions data. This is work in progress, I'm posting it for early comments and feedback. My open points are. - Unit and integration tests are yet to be done. - The data source work single-threaded only at the moment. I initially thought it might be enough to return only a single row in `GetEntryRanges()` to make it thread-safe but that's apparently not enough. So I'm now thinking to wrap `SetEntry()` and `GetEntryRanges()` in a mutex. - The code is not reusing `TSQLiteServer`. It felt like it requires stretching the `TSQLServer` interface in perhaps unwanted ways, for saving only a handful of direct sqlite API calls. On the other hand, with (probably quite a bit) more work it might be possible to have a more general data source that works on any SQL result set. - Determining column types in SQlite is tricky as it is dynamically typed and in principle each row can have different column types. If a table column is queried as is, I can use the default/declared column type. For expressions, I'll use the type of the first row of the result set. Still it can result in a column to be of type `NULL` where subsequent rows actually have meaningful values. My guess is that the current heuristic is practical enough, and of course the user can formulate the `SELECT` query to avoid ambiguity. - It would not be impossible to add support for remote reading of sqlite files. To do so, one can add a custom implementation of an sqlite virtual file system to have data pouring in directly from HTTP or XRootD. This is perhaps something to keep in mind.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2322
https://github.com/root-project/root/pull/2323:208,deployability,depend,dependent,208,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:302,deployability,depend,dependency,302,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:471,deployability,build,build,471,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:208,integrability,depend,dependent,208,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:302,integrability,depend,dependency,302,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:208,modifiability,depend,dependent,208,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:302,modifiability,depend,dependency,302,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:208,safety,depend,dependent,208,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:240,safety,avoid,avoid,240,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:302,safety,depend,dependency,302,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:285,security,SSL,SSL,285,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:208,testability,depend,dependent,208,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2323:302,testability,depend,dependency,302,"[ROOT-9532] Use all-keyword style in target_link_libraries(); When using `builtin_openssl=ON`, CMake erroneously exports the builtin static libraries in `ROOTConfig-targets.cmake`, which causes problems with dependent projects. In order to avoid this, we need `Net` and `RHTTP` to use SSL as a private dependency. Since CMake requires `target_link_libraries()`. to either be all-plain (as before) or all-keyword (required to allow `PRIVATE` linking), we need to move the build system to use all-keyword linking only. Fixes: [ROOT-9532](https://sft.its.cern.ch/jira/browse/ROOT-9532).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2323
https://github.com/root-project/root/pull/2324:4,availability,repair,repair,4,v7: repair many scripts from tutorials/v7 folder; After renaming of v7 classes not all places in JSROOT were fixed (or I lost them). Plus most of tutorials were not adjusted. Now all tutorials are working. Plus RCanvas title now shown as web-page title,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2324
https://github.com/root-project/root/pull/2324:4,reliability,repair,repair,4,v7: repair many scripts from tutorials/v7 folder; After renaming of v7 classes not all places in JSROOT were fixed (or I lost them). Plus most of tutorials were not adjusted. Now all tutorials are working. Plus RCanvas title now shown as web-page title,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2324
https://github.com/root-project/root/pull/2324:211,reliability,RCa,RCanvas,211,v7: repair many scripts from tutorials/v7 folder; After renaming of v7 classes not all places in JSROOT were fixed (or I lost them). Plus most of tutorials were not adjusted. Now all tutorials are working. Plus RCanvas title now shown as web-page title,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2324
https://github.com/root-project/root/pull/2325:9,usability,Document,Document,9,[sqlite] Document that SQLite will always return -1 as rowCount.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2325
https://github.com/root-project/root/pull/2326:17,deployability,build,build,17,"Remove genvector build option, will be always ON from now on; The CMake build system is broken with genvector=OFF (does not really disable genvector, and tests fail if that is fixed). Therefore, it was decided that it's better to just make this always ON and remove the option for disabling it altogether. See #2155 for more details.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2326
https://github.com/root-project/root/pull/2326:72,deployability,build,build,72,"Remove genvector build option, will be always ON from now on; The CMake build system is broken with genvector=OFF (does not really disable genvector, and tests fail if that is fixed). Therefore, it was decided that it's better to just make this always ON and remove the option for disabling it altogether. See #2155 for more details.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2326
https://github.com/root-project/root/pull/2326:160,deployability,fail,fail,160,"Remove genvector build option, will be always ON from now on; The CMake build system is broken with genvector=OFF (does not really disable genvector, and tests fail if that is fixed). Therefore, it was decided that it's better to just make this always ON and remove the option for disabling it altogether. See #2155 for more details.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2326
https://github.com/root-project/root/pull/2326:115,reliability,doe,does,115,"Remove genvector build option, will be always ON from now on; The CMake build system is broken with genvector=OFF (does not really disable genvector, and tests fail if that is fixed). Therefore, it was decided that it's better to just make this always ON and remove the option for disabling it altogether. See #2155 for more details.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2326
https://github.com/root-project/root/pull/2326:160,reliability,fail,fail,160,"Remove genvector build option, will be always ON from now on; The CMake build system is broken with genvector=OFF (does not really disable genvector, and tests fail if that is fixed). Therefore, it was decided that it's better to just make this always ON and remove the option for disabling it altogether. See #2155 for more details.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2326
https://github.com/root-project/root/pull/2326:154,safety,test,tests,154,"Remove genvector build option, will be always ON from now on; The CMake build system is broken with genvector=OFF (does not really disable genvector, and tests fail if that is fixed). Therefore, it was decided that it's better to just make this always ON and remove the option for disabling it altogether. See #2155 for more details.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2326
https://github.com/root-project/root/pull/2326:154,testability,test,tests,154,"Remove genvector build option, will be always ON from now on; The CMake build system is broken with genvector=OFF (does not really disable genvector, and tests fail if that is fixed). Therefore, it was decided that it's better to just make this always ON and remove the option for disabling it altogether. See #2155 for more details.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2326
https://github.com/root-project/root/pull/2327:213,integrability,sub,subset,213,"[VecOps,RVec] Proposal for sorting utils; Making a proposal to add utils for sorting of collections. The use-case is to get the indices of the largset/smallest elements of a vector, e.g., `Muon_pt` and retrieve a subset of other vectors from these, e.g., `Muon_eta`, ... . Already solved by @amadio in [this](https://root-forum.cern.ch/t/sort-struct-of-array-format-with-rvec/29485) forum post. ```cpp. root [1] using namespace ROOT::VecOps;. root [2] RVec<float> x({3, 1, 2}). (ROOT::VecOps::RVec<float> &) { 3, 1, 2 }. root [3] auto i = Argsort(x). (ROOT::VecOps::RVec<unsigned long> &) { 1, 2, 0 }. root [4] auto y = ByIndices(x, i). (ROOT::VecOps::RVec<float> &) { 1, 2, 3 }. ```. Functionalities and naming TBD.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2327
https://github.com/root-project/root/pull/2327:359,interoperability,format,format-with-rvec,359,"[VecOps,RVec] Proposal for sorting utils; Making a proposal to add utils for sorting of collections. The use-case is to get the indices of the largset/smallest elements of a vector, e.g., `Muon_pt` and retrieve a subset of other vectors from these, e.g., `Muon_eta`, ... . Already solved by @amadio in [this](https://root-forum.cern.ch/t/sort-struct-of-array-format-with-rvec/29485) forum post. ```cpp. root [1] using namespace ROOT::VecOps;. root [2] RVec<float> x({3, 1, 2}). (ROOT::VecOps::RVec<float> &) { 3, 1, 2 }. root [3] auto i = Argsort(x). (ROOT::VecOps::RVec<unsigned long> &) { 1, 2, 0 }. root [4] auto y = ByIndices(x, i). (ROOT::VecOps::RVec<float> &) { 1, 2, 3 }. ```. Functionalities and naming TBD.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2327
https://github.com/root-project/root/pull/2328:121,testability,simpl,simple,121,"[DF] [ROOT-9461] Introduced Kahan Sum as a custom action; Introduced tutorial on how to create a Kahan summation, with a simple example on how it can give better results than the classical one. This issue solves the Jira issue ROOT-9461",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2328
https://github.com/root-project/root/pull/2328:43,usability,custom,custom,43,"[DF] [ROOT-9461] Introduced Kahan Sum as a custom action; Introduced tutorial on how to create a Kahan summation, with a simple example on how it can give better results than the classical one. This issue solves the Jira issue ROOT-9461",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2328
https://github.com/root-project/root/pull/2328:121,usability,simpl,simple,121,"[DF] [ROOT-9461] Introduced Kahan Sum as a custom action; Introduced tutorial on how to create a Kahan summation, with a simple example on how it can give better results than the classical one. This issue solves the Jira issue ROOT-9461",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2328
https://github.com/root-project/root/pull/2329:16,usability,help,helper,16,[VecOps] Rename helper ByIndices to Take; @dpiparo and I agreed that the naming of `ByIndices` is not very lucky. I would propose to use `Take` following `numpy.take` as described [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.take.html).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2329
https://github.com/root-project/root/pull/2331:114,availability,failur,failures,114,[cxxmodules] Add a RAII because we can trigger deserialization.; This patch fixes a few recent runtime_cxxmodules failures.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2331
https://github.com/root-project/root/pull/2331:70,deployability,patch,patch,70,[cxxmodules] Add a RAII because we can trigger deserialization.; This patch fixes a few recent runtime_cxxmodules failures.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2331
https://github.com/root-project/root/pull/2331:114,deployability,fail,failures,114,[cxxmodules] Add a RAII because we can trigger deserialization.; This patch fixes a few recent runtime_cxxmodules failures.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2331
https://github.com/root-project/root/pull/2331:114,performance,failur,failures,114,[cxxmodules] Add a RAII because we can trigger deserialization.; This patch fixes a few recent runtime_cxxmodules failures.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2331
https://github.com/root-project/root/pull/2331:114,reliability,fail,failures,114,[cxxmodules] Add a RAII because we can trigger deserialization.; This patch fixes a few recent runtime_cxxmodules failures.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2331
https://github.com/root-project/root/pull/2331:70,safety,patch,patch,70,[cxxmodules] Add a RAII because we can trigger deserialization.; This patch fixes a few recent runtime_cxxmodules failures.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2331
https://github.com/root-project/root/pull/2331:70,security,patch,patch,70,[cxxmodules] Add a RAII because we can trigger deserialization.; This patch fixes a few recent runtime_cxxmodules failures.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2331
https://github.com/root-project/root/pull/2332:23,deployability,Depend,Dependent,23,[Vecops] Sorting docs; Dependent on #2329,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2332
https://github.com/root-project/root/pull/2332:23,integrability,Depend,Dependent,23,[Vecops] Sorting docs; Dependent on #2329,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2332
https://github.com/root-project/root/pull/2332:23,modifiability,Depend,Dependent,23,[Vecops] Sorting docs; Dependent on #2329,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2332
https://github.com/root-project/root/pull/2332:23,safety,Depend,Dependent,23,[Vecops] Sorting docs; Dependent on #2329,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2332
https://github.com/root-project/root/pull/2332:23,testability,Depend,Dependent,23,[Vecops] Sorting docs; Dependent on #2329,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2332
https://github.com/root-project/root/pull/2334:89,deployability,build,builds,89,"[StreamerAction] Do not rely on RVO returning an owning SequencePtr:; On certain Windows builds, `return {..., true}` would create a local SequencePtr, return a copy, then destruct the local - which in turn would delete fSequence. Instead, move the local and remember that the local has lost ownership.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2334
https://github.com/root-project/root/pull/2334:259,safety,reme,remember,259,"[StreamerAction] Do not rely on RVO returning an owning SequencePtr:; On certain Windows builds, `return {..., true}` would create a local SequencePtr, return a copy, then destruct the local - which in turn would delete fSequence. Instead, move the local and remember that the local has lost ownership.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2334
https://github.com/root-project/root/pull/2336:78,integrability,transform,transformations,78,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2336
https://github.com/root-project/root/pull/2336:78,interoperability,transform,transformations,78,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2336
https://github.com/root-project/root/pull/2336:13,safety,safe,safe,13,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2336
https://github.com/root-project/root/pull/2337:282,safety,valid,valid,282,Flag in Minuit2 when the covariance matrix is not positive defined at the first iteration (after MnSeed). ; In this case we return an invalid function minimum. . This should fix ROOT-9522 which shows that when minimizing concave functions (like -x^2) we returned a wrong minimum as valid,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2337
https://github.com/root-project/root/pull/2337:151,usability,minim,minimum,151,Flag in Minuit2 when the covariance matrix is not positive defined at the first iteration (after MnSeed). ; In this case we return an invalid function minimum. . This should fix ROOT-9522 which shows that when minimizing concave functions (like -x^2) we returned a wrong minimum as valid,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2337
https://github.com/root-project/root/pull/2337:210,usability,minim,minimizing,210,Flag in Minuit2 when the covariance matrix is not positive defined at the first iteration (after MnSeed). ; In this case we return an invalid function minimum. . This should fix ROOT-9522 which shows that when minimizing concave functions (like -x^2) we returned a wrong minimum as valid,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2337
https://github.com/root-project/root/pull/2337:271,usability,minim,minimum,271,Flag in Minuit2 when the covariance matrix is not positive defined at the first iteration (after MnSeed). ; In this case we return an invalid function minimum. . This should fix ROOT-9522 which shows that when minimizing concave functions (like -x^2) we returned a wrong minimum as valid,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2337
https://github.com/root-project/root/pull/2338:131,deployability,fail,fail,131,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:264,deployability,version,version,264,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:310,deployability,releas,released,310,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:264,integrability,version,version,264,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:194,interoperability,compatib,compatibility,194,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:94,modifiability,exten,extended,94,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:264,modifiability,version,version,264,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:131,reliability,fail,fail,131,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:255,testability,verif,verified,255,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2338:103,usability,minim,minimum,103,"Working around MSVC 15 CMake bug; There's a bug in the server-mode reader in MSVC that causes extended minimum required strings to fail. This hopefully will be fixed, but for now, this improves compatibility with MSVC in server mode. Also bumping maximum verified version of CMake to 3.12, since that has been released.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2338
https://github.com/root-project/root/pull/2339:5,usability,help,help,5,"[man,help] Add script to generate man, .h page. Use for `root`.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2339
https://github.com/root-project/root/pull/2340:6,integrability,Abstract,Abstract,6,[WIP] Abstract TFormula from ROOT::Double_v;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2340
https://github.com/root-project/root/pull/2340:6,modifiability,Abstract,Abstract,6,[WIP] Abstract TFormula from ROOT::Double_v;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2340
https://github.com/root-project/root/pull/2341:6,deployability,updat,updates,6,Small updates to GenVector;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2341
https://github.com/root-project/root/pull/2341:6,safety,updat,updates,6,Small updates to GenVector;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2341
https://github.com/root-project/root/pull/2341:6,security,updat,updates,6,Small updates to GenVector;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2341
https://github.com/root-project/root/pull/2342:179,availability,error,error,179,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:338,deployability,version,version,338,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:338,integrability,version,version,338,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:501,interoperability,specif,specific,501,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:338,modifiability,version,version,338,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:179,performance,error,error,179,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:179,safety,error,error,179,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:27,security,polic,policy,27,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:99,security,polic,policy,99,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:293,security,polic,policy,293,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:367,security,polic,policies,367,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:426,security,polic,policies,426,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:461,security,polic,policy,461,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:609,security,polic,policy,609,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:8,usability,behavi,behavior,8,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:87,usability,behavi,behavior,87,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:179,usability,error,error,179,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:280,usability,behavi,behavior,280,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:408,usability,behavi,behaviors,408,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2342:574,usability,behavi,behavior,574,Use NEW behavior for CMake policy CMP0051; We need to move LLVM in ROOT to use the new behavior of policy CMP0051. as it will be removed soon and the warning below will become an error:. CMake Deprecation Warning at interpreter/llvm/src/CMakeLists.txt:15 (cmake_policy):. The OLD behavior for policy CMP0051 will be removed from a future version. of CMake. The cmake-policies(7) manual explains that the OLD behaviors of all. policies are deprecated and that a policy should be set to OLD only under. specific short-term circumstances. Projects should be ported to the NEW. behavior and not rely on setting a policy to OLD.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2342
https://github.com/root-project/root/pull/2343:143,deployability,depend,dependencies,143,"[ROOT-9541] Do not require Libxml2 and OpenSSL when looking for Davix; When Davix is not builtin to ROOT, it's not necessary to propagate link dependencies on Libxml2 and OpenSSL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2343
https://github.com/root-project/root/pull/2343:143,integrability,depend,dependencies,143,"[ROOT-9541] Do not require Libxml2 and OpenSSL when looking for Davix; When Davix is not builtin to ROOT, it's not necessary to propagate link dependencies on Libxml2 and OpenSSL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2343
https://github.com/root-project/root/pull/2343:143,modifiability,depend,dependencies,143,"[ROOT-9541] Do not require Libxml2 and OpenSSL when looking for Davix; When Davix is not builtin to ROOT, it's not necessary to propagate link dependencies on Libxml2 and OpenSSL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2343
https://github.com/root-project/root/pull/2343:143,safety,depend,dependencies,143,"[ROOT-9541] Do not require Libxml2 and OpenSSL when looking for Davix; When Davix is not builtin to ROOT, it's not necessary to propagate link dependencies on Libxml2 and OpenSSL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2343
https://github.com/root-project/root/pull/2343:143,testability,depend,dependencies,143,"[ROOT-9541] Do not require Libxml2 and OpenSSL when looking for Davix; When Davix is not builtin to ROOT, it's not necessary to propagate link dependencies on Libxml2 and OpenSSL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2343
https://github.com/root-project/root/pull/2344:54,testability,understand,understand,54,[Doc] Fix friend trees example of RDF; @bluehood Do I understand this correctly?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2344
https://github.com/root-project/root/pull/2346:63,deployability,modul,module,63,[cxxmodules] TCling needs to autoparse headers which it has no module for:; Fixes roottest/cling/reflex/classVersion_rflx.log.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2346
https://github.com/root-project/root/pull/2346:122,deployability,log,log,122,[cxxmodules] TCling needs to autoparse headers which it has no module for:; Fixes roottest/cling/reflex/classVersion_rflx.log.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2346
https://github.com/root-project/root/pull/2346:63,modifiability,modul,module,63,[cxxmodules] TCling needs to autoparse headers which it has no module for:; Fixes roottest/cling/reflex/classVersion_rflx.log.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2346
https://github.com/root-project/root/pull/2346:63,safety,modul,module,63,[cxxmodules] TCling needs to autoparse headers which it has no module for:; Fixes roottest/cling/reflex/classVersion_rflx.log.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2346
https://github.com/root-project/root/pull/2346:122,safety,log,log,122,[cxxmodules] TCling needs to autoparse headers which it has no module for:; Fixes roottest/cling/reflex/classVersion_rflx.log.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2346
https://github.com/root-project/root/pull/2346:122,security,log,log,122,[cxxmodules] TCling needs to autoparse headers which it has no module for:; Fixes roottest/cling/reflex/classVersion_rflx.log.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2346
https://github.com/root-project/root/pull/2346:122,testability,log,log,122,[cxxmodules] TCling needs to autoparse headers which it has no module for:; Fixes roottest/cling/reflex/classVersion_rflx.log.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2346
https://github.com/root-project/root/pull/2347:78,integrability,transform,transformations,78,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2347
https://github.com/root-project/root/pull/2347:78,interoperability,transform,transformations,78,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2347
https://github.com/root-project/root/pull/2347:13,safety,safe,safe,13,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2347
https://github.com/root-project/root/pull/2348:50,deployability,modul,modules,50,[Experimental PyROOT] Fix pretty-printing for cxx modules;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2348
https://github.com/root-project/root/pull/2348:50,modifiability,modul,modules,50,[Experimental PyROOT] Fix pretty-printing for cxx modules;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2348
https://github.com/root-project/root/pull/2348:50,safety,modul,modules,50,[Experimental PyROOT] Fix pretty-printing for cxx modules;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2348
https://github.com/root-project/root/pull/2349:78,integrability,transform,transformations,78,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2349
https://github.com/root-project/root/pull/2349:78,interoperability,transform,transformations,78,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2349
https://github.com/root-project/root/pull/2349:13,safety,safe,safe,13,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2349
https://github.com/root-project/root/pull/2350:78,integrability,transform,transformations,78,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2350
https://github.com/root-project/root/pull/2350:78,interoperability,transform,transformations,78,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2350
https://github.com/root-project/root/pull/2350:13,safety,safe,safe,13,Added thread safe method GetInverse to compute the inverse matrix for…; … all transformations. (cherry picked from commit 36a3c2a5ee1852e6bc20d4273e9a2281043bd70d),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2350
https://github.com/root-project/root/pull/2351:273,availability,operat,operator,273,"[VecOps] Add RVec helpers for sorting and selecting elements; Introduce helpers for `VecOps::RVec` to sort and select the elements:. 1. `Take`: Take elements either at given indices or first/last elements. 4. `Sorted`: Sort either in ascending order or by given comparison operator, e.g., `std::greater<T>`. 5. `Reversed`: Reverse elements of `RVec`. On purpose, all operations are designed to be *not* inplace (in comparison to the STL API), because we want most often a copy, e.g., for `RDataFrame::Define` calls. Main target is to support convenient selection of elements in physics object selection. E.g. sort the pt vector, and take the two largest values in descending order:. ```cpp. >>> using namespace ROOT::VecOps;. >>> RVec<float> pt = {200, 50, 20, 100, 10};. >>> auto selection = Reversed(Take(Sorted(pt), 2)). (ROOT::VecOps::RVec<float> &) { 200, 100 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2351
https://github.com/root-project/root/pull/2351:367,availability,operat,operations,367,"[VecOps] Add RVec helpers for sorting and selecting elements; Introduce helpers for `VecOps::RVec` to sort and select the elements:. 1. `Take`: Take elements either at given indices or first/last elements. 4. `Sorted`: Sort either in ascending order or by given comparison operator, e.g., `std::greater<T>`. 5. `Reversed`: Reverse elements of `RVec`. On purpose, all operations are designed to be *not* inplace (in comparison to the STL API), because we want most often a copy, e.g., for `RDataFrame::Define` calls. Main target is to support convenient selection of elements in physics object selection. E.g. sort the pt vector, and take the two largest values in descending order:. ```cpp. >>> using namespace ROOT::VecOps;. >>> RVec<float> pt = {200, 50, 20, 100, 10};. >>> auto selection = Reversed(Take(Sorted(pt), 2)). (ROOT::VecOps::RVec<float> &) { 200, 100 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2351
https://github.com/root-project/root/pull/2351:437,deployability,API,API,437,"[VecOps] Add RVec helpers for sorting and selecting elements; Introduce helpers for `VecOps::RVec` to sort and select the elements:. 1. `Take`: Take elements either at given indices or first/last elements. 4. `Sorted`: Sort either in ascending order or by given comparison operator, e.g., `std::greater<T>`. 5. `Reversed`: Reverse elements of `RVec`. On purpose, all operations are designed to be *not* inplace (in comparison to the STL API), because we want most often a copy, e.g., for `RDataFrame::Define` calls. Main target is to support convenient selection of elements in physics object selection. E.g. sort the pt vector, and take the two largest values in descending order:. ```cpp. >>> using namespace ROOT::VecOps;. >>> RVec<float> pt = {200, 50, 20, 100, 10};. >>> auto selection = Reversed(Take(Sorted(pt), 2)). (ROOT::VecOps::RVec<float> &) { 200, 100 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2351
https://github.com/root-project/root/pull/2351:437,integrability,API,API,437,"[VecOps] Add RVec helpers for sorting and selecting elements; Introduce helpers for `VecOps::RVec` to sort and select the elements:. 1. `Take`: Take elements either at given indices or first/last elements. 4. `Sorted`: Sort either in ascending order or by given comparison operator, e.g., `std::greater<T>`. 5. `Reversed`: Reverse elements of `RVec`. On purpose, all operations are designed to be *not* inplace (in comparison to the STL API), because we want most often a copy, e.g., for `RDataFrame::Define` calls. Main target is to support convenient selection of elements in physics object selection. E.g. sort the pt vector, and take the two largest values in descending order:. ```cpp. >>> using namespace ROOT::VecOps;. >>> RVec<float> pt = {200, 50, 20, 100, 10};. >>> auto selection = Reversed(Take(Sorted(pt), 2)). (ROOT::VecOps::RVec<float> &) { 200, 100 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2351
https://github.com/root-project/root/pull/2351:437,interoperability,API,API,437,"[VecOps] Add RVec helpers for sorting and selecting elements; Introduce helpers for `VecOps::RVec` to sort and select the elements:. 1. `Take`: Take elements either at given indices or first/last elements. 4. `Sorted`: Sort either in ascending order or by given comparison operator, e.g., `std::greater<T>`. 5. `Reversed`: Reverse elements of `RVec`. On purpose, all operations are designed to be *not* inplace (in comparison to the STL API), because we want most often a copy, e.g., for `RDataFrame::Define` calls. Main target is to support convenient selection of elements in physics object selection. E.g. sort the pt vector, and take the two largest values in descending order:. ```cpp. >>> using namespace ROOT::VecOps;. >>> RVec<float> pt = {200, 50, 20, 100, 10};. >>> auto selection = Reversed(Take(Sorted(pt), 2)). (ROOT::VecOps::RVec<float> &) { 200, 100 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2351
https://github.com/root-project/root/pull/2351:18,usability,help,helpers,18,"[VecOps] Add RVec helpers for sorting and selecting elements; Introduce helpers for `VecOps::RVec` to sort and select the elements:. 1. `Take`: Take elements either at given indices or first/last elements. 4. `Sorted`: Sort either in ascending order or by given comparison operator, e.g., `std::greater<T>`. 5. `Reversed`: Reverse elements of `RVec`. On purpose, all operations are designed to be *not* inplace (in comparison to the STL API), because we want most often a copy, e.g., for `RDataFrame::Define` calls. Main target is to support convenient selection of elements in physics object selection. E.g. sort the pt vector, and take the two largest values in descending order:. ```cpp. >>> using namespace ROOT::VecOps;. >>> RVec<float> pt = {200, 50, 20, 100, 10};. >>> auto selection = Reversed(Take(Sorted(pt), 2)). (ROOT::VecOps::RVec<float> &) { 200, 100 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2351
https://github.com/root-project/root/pull/2351:72,usability,help,helpers,72,"[VecOps] Add RVec helpers for sorting and selecting elements; Introduce helpers for `VecOps::RVec` to sort and select the elements:. 1. `Take`: Take elements either at given indices or first/last elements. 4. `Sorted`: Sort either in ascending order or by given comparison operator, e.g., `std::greater<T>`. 5. `Reversed`: Reverse elements of `RVec`. On purpose, all operations are designed to be *not* inplace (in comparison to the STL API), because we want most often a copy, e.g., for `RDataFrame::Define` calls. Main target is to support convenient selection of elements in physics object selection. E.g. sort the pt vector, and take the two largest values in descending order:. ```cpp. >>> using namespace ROOT::VecOps;. >>> RVec<float> pt = {200, 50, 20, 100, 10};. >>> auto selection = Reversed(Take(Sorted(pt), 2)). (ROOT::VecOps::RVec<float> &) { 200, 100 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2351
https://github.com/root-project/root/pull/2351:534,usability,support,support,534,"[VecOps] Add RVec helpers for sorting and selecting elements; Introduce helpers for `VecOps::RVec` to sort and select the elements:. 1. `Take`: Take elements either at given indices or first/last elements. 4. `Sorted`: Sort either in ascending order or by given comparison operator, e.g., `std::greater<T>`. 5. `Reversed`: Reverse elements of `RVec`. On purpose, all operations are designed to be *not* inplace (in comparison to the STL API), because we want most often a copy, e.g., for `RDataFrame::Define` calls. Main target is to support convenient selection of elements in physics object selection. E.g. sort the pt vector, and take the two largest values in descending order:. ```cpp. >>> using namespace ROOT::VecOps;. >>> RVec<float> pt = {200, 50, 20, 100, 10};. >>> auto selection = Reversed(Take(Sorted(pt), 2)). (ROOT::VecOps::RVec<float> &) { 200, 100 }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2351
https://github.com/root-project/root/pull/2352:96,interoperability,convers,conversion,96,"[TMVA] CVSplit -- Improve fold assignment for large values; The fold assignment is relying on a conversion from a double to an integer (due to using a TTreeFormula). It was previously enforced that the formula output should be ""close"" to an integer. The semantics are now changed to assume the output is an integer (and truncate any fractional part). This should ensure that the valid range is increased significantly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2352
https://github.com/root-project/root/pull/2352:254,interoperability,semant,semantics,254,"[TMVA] CVSplit -- Improve fold assignment for large values; The fold assignment is relying on a conversion from a double to an integer (due to using a TTreeFormula). It was previously enforced that the formula output should be ""close"" to an integer. The semantics are now changed to assume the output is an integer (and truncate any fractional part). This should ensure that the valid range is increased significantly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2352
https://github.com/root-project/root/pull/2352:379,safety,valid,valid,379,"[TMVA] CVSplit -- Improve fold assignment for large values; The fold assignment is relying on a conversion from a double to an integer (due to using a TTreeFormula). It was previously enforced that the formula output should be ""close"" to an integer. The semantics are now changed to assume the output is an integer (and truncate any fractional part). This should ensure that the valid range is increased significantly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2352
https://github.com/root-project/root/pull/2352:404,security,sign,significantly,404,"[TMVA] CVSplit -- Improve fold assignment for large values; The fold assignment is relying on a conversion from a double to an integer (due to using a TTreeFormula). It was previously enforced that the formula output should be ""close"" to an integer. The semantics are now changed to assume the output is an integer (and truncate any fractional part). This should ensure that the valid range is increased significantly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2352
https://github.com/root-project/root/pull/2352:228,usability,close,close,228,"[TMVA] CVSplit -- Improve fold assignment for large values; The fold assignment is relying on a conversion from a double to an integer (due to using a TTreeFormula). It was previously enforced that the formula output should be ""close"" to an integer. The semantics are now changed to assume the output is an integer (and truncate any fractional part). This should ensure that the valid range is increased significantly.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2352
https://github.com/root-project/root/pull/2353:16,deployability,Updat,Update,16,[TMVA] JsMVA -- Update d3 lib; The name of the d3 library included with jsroot was changed from d3.v3. to just d3. This updates JsMVA to use the new name. Note: Not sure if more changes are required to propagate to SWAN and notebook environment. Please advise :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2353
https://github.com/root-project/root/pull/2353:120,deployability,updat,updates,120,[TMVA] JsMVA -- Update d3 lib; The name of the d3 library included with jsroot was changed from d3.v3. to just d3. This updates JsMVA to use the new name. Note: Not sure if more changes are required to propagate to SWAN and notebook environment. Please advise :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2353
https://github.com/root-project/root/pull/2353:16,safety,Updat,Update,16,[TMVA] JsMVA -- Update d3 lib; The name of the d3 library included with jsroot was changed from d3.v3. to just d3. This updates JsMVA to use the new name. Note: Not sure if more changes are required to propagate to SWAN and notebook environment. Please advise :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2353
https://github.com/root-project/root/pull/2353:120,safety,updat,updates,120,[TMVA] JsMVA -- Update d3 lib; The name of the d3 library included with jsroot was changed from d3.v3. to just d3. This updates JsMVA to use the new name. Note: Not sure if more changes are required to propagate to SWAN and notebook environment. Please advise :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2353
https://github.com/root-project/root/pull/2353:16,security,Updat,Update,16,[TMVA] JsMVA -- Update d3 lib; The name of the d3 library included with jsroot was changed from d3.v3. to just d3. This updates JsMVA to use the new name. Note: Not sure if more changes are required to propagate to SWAN and notebook environment. Please advise :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2353
https://github.com/root-project/root/pull/2353:120,security,updat,updates,120,[TMVA] JsMVA -- Update d3 lib; The name of the d3 library included with jsroot was changed from d3.v3. to just d3. This updates JsMVA to use the new name. Note: Not sure if more changes are required to propagate to SWAN and notebook environment. Please advise :),MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2353
https://github.com/root-project/root/pull/2354:0,deployability,Updat,Update,0,Update TBranchElement docs;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2354
https://github.com/root-project/root/pull/2354:0,safety,Updat,Update,0,Update TBranchElement docs;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2354
https://github.com/root-project/root/pull/2354:0,security,Updat,Update,0,Update TBranchElement docs;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2354
https://github.com/root-project/root/pull/2356:49,deployability,modul,module,49,[cxxmodules] Remove the lib prefix only when the module name starts w…; …ith it.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2356
https://github.com/root-project/root/pull/2356:49,modifiability,modul,module,49,[cxxmodules] Remove the lib prefix only when the module name starts w…; …ith it.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2356
https://github.com/root-project/root/pull/2356:49,safety,modul,module,49,[cxxmodules] Remove the lib prefix only when the module name starts w…; …ith it.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2356
https://github.com/root-project/root/pull/2358:33,deployability,depend,depends,33,"[test,cxxmodules] stressMathMore depends on Smatrix.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2358
https://github.com/root-project/root/pull/2358:33,integrability,depend,depends,33,"[test,cxxmodules] stressMathMore depends on Smatrix.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2358
https://github.com/root-project/root/pull/2358:33,modifiability,depend,depends,33,"[test,cxxmodules] stressMathMore depends on Smatrix.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2358
https://github.com/root-project/root/pull/2358:1,safety,test,test,1,"[test,cxxmodules] stressMathMore depends on Smatrix.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2358
https://github.com/root-project/root/pull/2358:33,safety,depend,depends,33,"[test,cxxmodules] stressMathMore depends on Smatrix.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2358
https://github.com/root-project/root/pull/2358:1,testability,test,test,1,"[test,cxxmodules] stressMathMore depends on Smatrix.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2358
https://github.com/root-project/root/pull/2358:33,testability,depend,depends,33,"[test,cxxmodules] stressMathMore depends on Smatrix.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2358
https://github.com/root-project/root/pull/2359:38,performance,time,timeout,38,"[tutorial,pythia8] Try to work around timeout on Fed28.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2359
https://github.com/root-project/root/pull/2359:38,safety,timeout,timeout,38,"[tutorial,pythia8] Try to work around timeout on Fed28.;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2359
https://github.com/root-project/root/pull/2361:414,integrability,Event,Events,414,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:434,integrability,Filter,Filter,434,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:492,integrability,Filter,Filter,492,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:519,integrability,Event,Events,519,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:4,performance,memor,memory,4,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:164,performance,memor,memory,164,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:237,performance,memor,memory,237,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:499,performance,memor,memory,499,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:348,safety,test,test,348,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:569,safety,test,test,569,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:348,testability,test,test,348,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:569,testability,test,test,569,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:4,usability,memor,memory,4,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:164,usability,memor,memory,164,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:237,usability,memor,memory,237,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2361:499,usability,memor,memory,499,"Fix memory leak in GetStreamInfoListImpl; The program used for finding the leaks is as follows:. ```cpp. #include ""ROOT/RDataFrame.hxx"". #include ""TSystem.h"". bool memory() {. ProcInfo_t info;. gSystem->GetProcInfo(&info);. printf("" res memory = %g Mbytes\n"", info.fMemResident / 1024.);. return true;. }. constexpr auto file = ""small.root"";. void test() {. ROOT::EnableImplicitMT(1);. auto df = ROOT::RDataFrame(""Events"", file);. df.Filter(""Muon_pt.size()>0""). .Define(""pt"", ""Muon_pt[0]""). .Filter(memory). .Snapshot(""Events"", ""output.root"", {""pt""});. }. int main() { test(); }. ```. The valgrind output before the fix:. ```. ==20802== LEAK SUMMARY:. ==20802== definitely lost: 54,488 bytes in 222 blocks. ==20802== indirectly lost: 23,816 bytes in 199 blocks. ==20802== possibly lost: 71,130 bytes in 610 blocks. ==20802== still reachable: 74,920,340 bytes in 100,971 blocks. ==20802== of which reachable via heuristic:. ==20802== newarray : 25,424 bytes in 49 blocks. ==20802== multipleinheritance: 1,048 bytes in 3 blocks. ==20802== suppressed: 6,366,063 bytes in 65,508 blocks. ```. And the valgrind output after the fix:. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2361
https://github.com/root-project/root/pull/2362:4,performance,memor,memory,4,"Fix memory leak in TTreeReaderArray; Second fix after #2361 with same test program. Valgrind output before fix (but with fix of #2361):. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```. Valgrind output after fix:. ```. ==5872== LEAK SUMMARY:. ==5872== definitely lost: 24 bytes in 2 blocks. ==5872== indirectly lost: 56 bytes in 1 blocks. ==5872== possibly lost: 60,990 bytes in 608 blocks. ==5872== still reachable: 74,909,841 bytes in 100,876 blocks. ==5872== of which reachable via heuristic:. ==5872== newarray : 25,424 bytes in 49 blocks. ==5872== multipleinheritance: 2,136 bytes in 3 blocks. ==5872== suppressed: 6,378,087 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2362
https://github.com/root-project/root/pull/2362:70,safety,test,test,70,"Fix memory leak in TTreeReaderArray; Second fix after #2361 with same test program. Valgrind output before fix (but with fix of #2361):. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```. Valgrind output after fix:. ```. ==5872== LEAK SUMMARY:. ==5872== definitely lost: 24 bytes in 2 blocks. ==5872== indirectly lost: 56 bytes in 1 blocks. ==5872== possibly lost: 60,990 bytes in 608 blocks. ==5872== still reachable: 74,909,841 bytes in 100,876 blocks. ==5872== of which reachable via heuristic:. ==5872== newarray : 25,424 bytes in 49 blocks. ==5872== multipleinheritance: 2,136 bytes in 3 blocks. ==5872== suppressed: 6,378,087 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2362
https://github.com/root-project/root/pull/2362:70,testability,test,test,70,"Fix memory leak in TTreeReaderArray; Second fix after #2361 with same test program. Valgrind output before fix (but with fix of #2361):. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```. Valgrind output after fix:. ```. ==5872== LEAK SUMMARY:. ==5872== definitely lost: 24 bytes in 2 blocks. ==5872== indirectly lost: 56 bytes in 1 blocks. ==5872== possibly lost: 60,990 bytes in 608 blocks. ==5872== still reachable: 74,909,841 bytes in 100,876 blocks. ==5872== of which reachable via heuristic:. ==5872== newarray : 25,424 bytes in 49 blocks. ==5872== multipleinheritance: 2,136 bytes in 3 blocks. ==5872== suppressed: 6,378,087 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2362
https://github.com/root-project/root/pull/2362:4,usability,memor,memory,4,"Fix memory leak in TTreeReaderArray; Second fix after #2361 with same test program. Valgrind output before fix (but with fix of #2361):. ```. ==22182== LEAK SUMMARY:. ==22182== definitely lost: 6,424 bytes in 202 blocks. ==22182== indirectly lost: 23,936 bytes in 200 blocks. ==22182== possibly lost: 61,230 bytes in 610 blocks. ==22182== still reachable: 74,911,268 bytes in 100,857 blocks. ==22182== of which reachable via heuristic:. ==22182== newarray : 25,424 bytes in 49 blocks. ==22182== multipleinheritance: 928 bytes in 2 blocks. ==22182== suppressed: 6,374,775 bytes in 65,619 blocks. ```. Valgrind output after fix:. ```. ==5872== LEAK SUMMARY:. ==5872== definitely lost: 24 bytes in 2 blocks. ==5872== indirectly lost: 56 bytes in 1 blocks. ==5872== possibly lost: 60,990 bytes in 608 blocks. ==5872== still reachable: 74,909,841 bytes in 100,876 blocks. ==5872== of which reachable via heuristic:. ==5872== newarray : 25,424 bytes in 49 blocks. ==5872== multipleinheritance: 2,136 bytes in 3 blocks. ==5872== suppressed: 6,378,087 bytes in 65,619 blocks. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2362
https://github.com/root-project/root/pull/2363:1,energy efficiency,core,core,1,"[core,zip] Initialize rootrc Root.CompressionAlgorithm with old Root.…; …ZipMode:. If the old name is used, warn if the usage is dangerous (==0). Else use it to provide a default value for Root.CompressionAlgorithm if the latter isn not set.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2363
https://github.com/root-project/root/pull/2364:104,availability,consist,consistently,104,"Matrix updates; This fixes a thread safety issue in the matrix package [ROOT-9547], but also implements consistently operators +, +=, ==. = for matrix classes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2364
https://github.com/root-project/root/pull/2364:117,availability,operat,operators,117,"Matrix updates; This fixes a thread safety issue in the matrix package [ROOT-9547], but also implements consistently operators +, +=, ==. = for matrix classes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2364
https://github.com/root-project/root/pull/2364:7,deployability,updat,updates,7,"Matrix updates; This fixes a thread safety issue in the matrix package [ROOT-9547], but also implements consistently operators +, +=, ==. = for matrix classes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2364
https://github.com/root-project/root/pull/2364:63,modifiability,pac,package,63,"Matrix updates; This fixes a thread safety issue in the matrix package [ROOT-9547], but also implements consistently operators +, +=, ==. = for matrix classes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2364
https://github.com/root-project/root/pull/2364:7,safety,updat,updates,7,"Matrix updates; This fixes a thread safety issue in the matrix package [ROOT-9547], but also implements consistently operators +, +=, ==. = for matrix classes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2364
https://github.com/root-project/root/pull/2364:36,safety,safe,safety,36,"Matrix updates; This fixes a thread safety issue in the matrix package [ROOT-9547], but also implements consistently operators +, +=, ==. = for matrix classes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2364
https://github.com/root-project/root/pull/2364:7,security,updat,updates,7,"Matrix updates; This fixes a thread safety issue in the matrix package [ROOT-9547], but also implements consistently operators +, +=, ==. = for matrix classes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2364
https://github.com/root-project/root/pull/2364:104,usability,consist,consistently,104,"Matrix updates; This fixes a thread safety issue in the matrix package [ROOT-9547], but also implements consistently operators +, +=, ==. = for matrix classes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2364
https://github.com/root-project/root/pull/2365:71,availability,consist,consistent,71,Matrix updates; Fixes thread safety issues in the matrix package. Adds consistent operators for TGeoMatrix-derived types.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2365
https://github.com/root-project/root/pull/2365:82,availability,operat,operators,82,Matrix updates; Fixes thread safety issues in the matrix package. Adds consistent operators for TGeoMatrix-derived types.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2365
https://github.com/root-project/root/pull/2365:7,deployability,updat,updates,7,Matrix updates; Fixes thread safety issues in the matrix package. Adds consistent operators for TGeoMatrix-derived types.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2365
https://github.com/root-project/root/pull/2365:57,modifiability,pac,package,57,Matrix updates; Fixes thread safety issues in the matrix package. Adds consistent operators for TGeoMatrix-derived types.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2365
https://github.com/root-project/root/pull/2365:7,safety,updat,updates,7,Matrix updates; Fixes thread safety issues in the matrix package. Adds consistent operators for TGeoMatrix-derived types.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2365
https://github.com/root-project/root/pull/2365:29,safety,safe,safety,29,Matrix updates; Fixes thread safety issues in the matrix package. Adds consistent operators for TGeoMatrix-derived types.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2365
https://github.com/root-project/root/pull/2365:7,security,updat,updates,7,Matrix updates; Fixes thread safety issues in the matrix package. Adds consistent operators for TGeoMatrix-derived types.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2365
https://github.com/root-project/root/pull/2365:71,usability,consist,consistent,71,Matrix updates; Fixes thread safety issues in the matrix package. Adds consistent operators for TGeoMatrix-derived types.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2365
https://github.com/root-project/root/pull/2366:201,availability,echo,echo,201,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:324,availability,echo,echo,324,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:516,availability,echo,echo,516,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:981,availability,echo,echo,981,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1462,availability,echo,echo,1462,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:792,deployability,build,build,792,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:822,deployability,patch,patch,822,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1254,deployability,patch,patch,1254,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:107,performance,time,time,107,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:822,safety,patch,patch,822,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:873,safety,test,test,873,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:902,safety,test,test,902,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:924,safety,test,test,924,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:974,safety,test,test,974,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1024,safety,test,test,1024,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1152,safety,test,test,1152,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1254,safety,patch,patch,1254,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1354,safety,test,test,1354,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1383,safety,test,test,1383,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1405,safety,test,test,1405,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1455,safety,test,test,1455,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1505,safety,test,test,1505,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1593,safety,test,test,1593,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:822,security,patch,patch,822,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1254,security,patch,patch,1254,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:873,testability,test,test,873,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:902,testability,test,test,902,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:924,testability,test,test,924,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:974,testability,test,test,974,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1024,testability,test,test,1024,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1152,testability,test,test,1152,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1354,testability,test,test,1354,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1383,testability,test,test,1383,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1405,testability,test,test,1405,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1455,testability,test,test,1455,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1505,testability,test,test,1505,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:1593,testability,test,test,1593,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:566,usability,behavi,behavior,566,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2366:578,usability,user,users,578,"Overwrite ROOTSYS by ROOT location path, not where ROOT was built; SetRootSys is overwriting ROOTSYS every time:. ```. [yuka@yuka-arch normalroot]$ source bin/thisroot.sh. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /home/yuka/normalroot. [yuka@yuka-arch normalroot]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch normalroot]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"". root [1] .q. [yuka@yuka-arch normalroot]$ echo $ROOTSYS. /hoge/huga. ```. This is the right behavior as users can choose different ROOT binary in. cvmfs enviroment. However, as SetROOTSYS is reading the binary header. which stores the information of where the binary was built, ROOTSYS is. messed up when you relocate build directory (Without this patch):. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/normalroot"" // It should be ""/home/yuka/test""! ```. ROOTSYS can be overwritten, but it should point to the actual binary. location. With this patch, ROOTSYS can point to the correct binary location:. ```. [yuka@yuka-arch ~]$ cp -r normalroot test. [yuka@yuka-arch ~]$ cd test. [yuka@yuka-arch test]$ export ROOTSYS=/hoge/huga. [yuka@yuka-arch test]$ echo $ROOTSYS. /hoge/huga. [yuka@yuka-arch test]$ bin/root.exe -l. root [0] gSystem->Getenv(""ROOTSYS""). (const char *) ""/home/yuka/test"" // Which is correct. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2366
https://github.com/root-project/root/pull/2367:0,deployability,Updat,Updating,0,"Updating root-configure.in, since ROOT doesn't support anymore config…; …uration with autotools",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2367
https://github.com/root-project/root/pull/2367:14,integrability,configur,configure,14,"Updating root-configure.in, since ROOT doesn't support anymore config…; …uration with autotools",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2367
https://github.com/root-project/root/pull/2367:14,modifiability,configur,configure,14,"Updating root-configure.in, since ROOT doesn't support anymore config…; …uration with autotools",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2367
https://github.com/root-project/root/pull/2367:39,reliability,doe,doesn,39,"Updating root-configure.in, since ROOT doesn't support anymore config…; …uration with autotools",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2367
https://github.com/root-project/root/pull/2367:0,safety,Updat,Updating,0,"Updating root-configure.in, since ROOT doesn't support anymore config…; …uration with autotools",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2367
https://github.com/root-project/root/pull/2367:0,security,Updat,Updating,0,"Updating root-configure.in, since ROOT doesn't support anymore config…; …uration with autotools",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2367
https://github.com/root-project/root/pull/2367:14,security,configur,configure,14,"Updating root-configure.in, since ROOT doesn't support anymore config…; …uration with autotools",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2367
https://github.com/root-project/root/pull/2367:47,usability,support,support,47,"Updating root-configure.in, since ROOT doesn't support anymore config…; …uration with autotools",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2367
https://github.com/root-project/root/pull/2368:23,usability,document,documented,23,[DF][Doc] Graph action documented;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2368
https://github.com/root-project/root/pull/2369:9,deployability,patch,patches,9,V6 14 00 patches;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2369
https://github.com/root-project/root/pull/2369:9,safety,patch,patches,9,V6 14 00 patches;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2369
https://github.com/root-project/root/pull/2369:9,security,patch,patches,9,V6 14 00 patches;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2369
https://github.com/root-project/root/pull/2371:462,deployability,patch,patch,462,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:886,deployability,version,version,886,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:1103,deployability,Patch,Patch,1103,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:555,energy efficiency,load,loading,555,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:886,integrability,version,version,886,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:24,interoperability,plug,plugin,24,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:66,interoperability,share,shared,66,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:347,interoperability,plug,plugins,347,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:431,interoperability,plug,plugin,431,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:476,interoperability,plug,plugins,476,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:566,interoperability,plug,plugins,566,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:579,interoperability,share,shared,579,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:720,interoperability,share,shared,720,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:1041,interoperability,plug,plugins,1041,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:108,modifiability,extens,extensions,108,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:497,modifiability,exten,extends,497,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:886,modifiability,version,version,886,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:929,modifiability,pac,package,929,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:1065,modifiability,scenario,scenario,1065,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:555,performance,load,loading,555,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:378,reliability,doe,does,378,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:462,safety,patch,patch,462,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:844,safety,avoid,avoid,844,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:1103,safety,Patch,Patch,1103,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:462,security,patch,patch,462,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:1103,security,Patch,Patch,1103,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:31,usability,support,support,31,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:94,usability,user,user-defined,94,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:135,usability,custom,custom,135,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:175,usability,visual,visualize,175,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:751,usability,User,Users,751,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2371:784,usability,prefer,prefer,784,"[cling] Implement clang plugin support.; Clang allows third party shared libraries to provide user-defined. extensions. For example, a custom libTemplateInstantiation.so can. visualize all template instantiation chains in clang. To enable it. one needs to pass a set of options such as -fplugin. Cling should be able to inherently work with clang plugins. However,. cling still does not make full use of the clang driver where the plugin. setup is handled. This patch enables plugins in cling and extends them in some aspects. In particular, cling allows loading of plugins from shared libraries. but also if they are linked to the same library where cling is. This is. very useful in cases where cling runs itself in a shared library (eg. libCling). Users of libCling (such as ROOT) prefer to keep all llvm and. clang related symbols local to avoid symbol clashes if there is another. version of clang and llvm linked against a package. This can be done by. dlopen-ing libCling with RTLD_LOCAL visibility mode. Then the only way. for clang plugins to work in this scenario is to be linked to libCling. Patch by Aleksandr Efremov (@efremale) and me.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2371
https://github.com/root-project/root/pull/2372:160,deployability,version,versions,160,"[ROOT-9550] Revert ""Do not include internal PostgreSQL header""; This reverts commit 6fa43c88b40e058f0c1dfeed9f25f852ee57a6ea. That commit breaks support of old versions of PostgreSQL (9.x). Since the classic build has been removed, this change should be reverted. More information: [ROOT-9550](https://sft.its.cern.ch/jira/browse/ROOT-9550).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2372
https://github.com/root-project/root/pull/2372:208,deployability,build,build,208,"[ROOT-9550] Revert ""Do not include internal PostgreSQL header""; This reverts commit 6fa43c88b40e058f0c1dfeed9f25f852ee57a6ea. That commit breaks support of old versions of PostgreSQL (9.x). Since the classic build has been removed, this change should be reverted. More information: [ROOT-9550](https://sft.its.cern.ch/jira/browse/ROOT-9550).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2372
https://github.com/root-project/root/pull/2372:160,integrability,version,versions,160,"[ROOT-9550] Revert ""Do not include internal PostgreSQL header""; This reverts commit 6fa43c88b40e058f0c1dfeed9f25f852ee57a6ea. That commit breaks support of old versions of PostgreSQL (9.x). Since the classic build has been removed, this change should be reverted. More information: [ROOT-9550](https://sft.its.cern.ch/jira/browse/ROOT-9550).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2372
https://github.com/root-project/root/pull/2372:160,modifiability,version,versions,160,"[ROOT-9550] Revert ""Do not include internal PostgreSQL header""; This reverts commit 6fa43c88b40e058f0c1dfeed9f25f852ee57a6ea. That commit breaks support of old versions of PostgreSQL (9.x). Since the classic build has been removed, this change should be reverted. More information: [ROOT-9550](https://sft.its.cern.ch/jira/browse/ROOT-9550).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2372
https://github.com/root-project/root/pull/2372:145,usability,support,support,145,"[ROOT-9550] Revert ""Do not include internal PostgreSQL header""; This reverts commit 6fa43c88b40e058f0c1dfeed9f25f852ee57a6ea. That commit breaks support of old versions of PostgreSQL (9.x). Since the classic build has been removed, this change should be reverted. More information: [ROOT-9550](https://sft.its.cern.ch/jira/browse/ROOT-9550).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2372
https://github.com/root-project/root/pull/2373:32,deployability,fail,fails,32,[DF] Fix ROOT-9555: Compilation fails for Reduce on a bool column due to std::vector<bool>;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2373
https://github.com/root-project/root/pull/2373:42,energy efficiency,Reduc,Reduce,42,[DF] Fix ROOT-9555: Compilation fails for Reduce on a bool column due to std::vector<bool>;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2373
https://github.com/root-project/root/pull/2373:32,reliability,fail,fails,32,[DF] Fix ROOT-9555: Compilation fails for Reduce on a bool column due to std::vector<bool>;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2373
https://github.com/root-project/root/pull/2374:32,deployability,fail,fails,32,[DF] Fix ROOT-9555: Compilation fails for Reduce on a bool column due to std::vector<bool> in v6.14;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2374
https://github.com/root-project/root/pull/2374:42,energy efficiency,Reduc,Reduce,42,[DF] Fix ROOT-9555: Compilation fails for Reduce on a bool column due to std::vector<bool> in v6.14;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2374
https://github.com/root-project/root/pull/2374:32,reliability,fail,fails,32,[DF] Fix ROOT-9555: Compilation fails for Reduce on a bool column due to std::vector<bool> in v6.14;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2374
https://github.com/root-project/root/pull/2375:18,deployability,modul,modulemap,18,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:107,deployability,modul,module,107,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:114,deployability,modul,modulemap,114,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:204,deployability,modul,modulemap,204,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:13,energy efficiency,Load,Load,13,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:31,energy efficiency,current,current,31,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:98,energy efficiency,Load,Load,98,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:179,energy efficiency,Load,LoadModule,179,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:190,energy efficiency,load,loads,190,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:18,modifiability,modul,modulemap,18,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:107,modifiability,modul,module,107,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:114,modifiability,modul,modulemap,114,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:204,modifiability,modul,modulemap,204,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:13,performance,Load,Load,13,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:98,performance,Load,Load,98,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:179,performance,Load,LoadModule,179,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:190,performance,load,loads,190,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:18,safety,modul,modulemap,18,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:107,safety,modul,module,107,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:114,safety,modul,modulemap,114,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:204,safety,modul,modulemap,204,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2375:139,usability,User,User,139,"[cxxmodules] Load modulemap in current directory; This is related to the fix of includeInLinkdef. Load. ""./module.modulemap"" if it exists. User can cd to different directory. and LoadModule loads the new modulemap.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2375
https://github.com/root-project/root/pull/2376:304,availability,error,errors,304,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:361,availability,error,error,361,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:435,availability,error,error,435,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:572,availability,error,error,572,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:647,availability,error,error,647,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1220,availability,error,error,1220,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1455,availability,error,errors,1455,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1495,availability,error,error,1495,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1591,availability,error,error,1591,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:390,deployability,depend,depends,390,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:160,energy efficiency,optim,optimizer,160,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:693,energy efficiency,optim,optimizer,693,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1723,energy efficiency,optim,optimizers,1723,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:390,integrability,depend,depends,390,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:390,modifiability,depend,depends,390,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:742,modifiability,layer,layer,742,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:160,performance,optimiz,optimizer,160,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:304,performance,error,errors,304,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:361,performance,error,error,361,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:435,performance,error,error,435,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:572,performance,error,error,572,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:647,performance,error,error,647,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:693,performance,optimiz,optimizer,693,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1220,performance,error,error,1220,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1455,performance,error,errors,1455,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1495,performance,error,error,1495,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1591,performance,error,error,1591,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1723,performance,optimiz,optimizers,1723,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:129,safety,test,testing,129,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:304,safety,error,errors,304,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:361,safety,error,error,361,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:390,safety,depend,depends,390,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:435,safety,error,error,435,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:572,safety,error,error,572,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:647,safety,error,error,647,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:683,safety,test,tests,683,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:919,safety,Input,Input,919,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1220,safety,error,error,1220,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1455,safety,error,errors,1455,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1495,safety,error,error,1495,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1591,safety,error,error,1591,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1715,safety,test,testing,1715,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:803,security,Ident,Identity,803,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1229,security,sign,significantly,1229,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:129,testability,test,testing,129,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:390,testability,depend,depends,390,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:678,testability,unit,unit,678,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:683,testability,test,tests,683,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1715,testability,test,testing,1715,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:304,usability,error,errors,304,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:361,usability,error,error,361,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:435,usability,error,error,435,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:572,usability,error,error,572,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:647,usability,error,error,647,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:757,usability,learn,learn,757,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:919,usability,Input,Input,919,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1220,usability,error,error,1220,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1455,usability,error,errors,1455,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1495,usability,error,error,1495,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2376:1591,usability,error,error,1591,"[TMVA] Add new Evaluation Metric ( meanAbsoluteError between two matrices ); **Need**:. The need for a new evaluation metric for testing the convergence of the optimizer is essential. The already existing metric was maximumRelativeError() between two matrices which takes the maximum of all the relative errors between its individual elements. But the relative error between these elements depends on the element values. i.e. Relative error between a and b = abs(a-b)/(abs(a)+abs(b)). Let use consider 2 cases,. case a) If two values are a = 0.0001 , b = 0.0002, relative error = 0.3333. case b) If two values are a = 10.0001 b = 10.0002 relative error = 4.99992e-6 . Since the unit tests for optimizer is written in a way so that a sample 3 layer DNN will learn this function Y = K * X. So, If X = I ( Identity matrix ), then Y = K * I = K. This should be equivalent to the output of the trained DNN when I is feed as Input. Let Y' be the output of the trained DNN. So I need to compare the matrices K and Y' for approximate equality with a certain threshold. So If I use maximumRelativeError for comparing the approximate equality for two matrices, then even though the difference is small for two cases, the relative error is significantly different. So there is a need for a new evaluation metric. . **Goal**:. The goal of this PR is to implement new evaluation metric meanAbsoluteError() between two matrices which takes the mean of all the absolute errors of individual elements. Absolute error between a and b = abs(a-b). So both the cases described above will have the same absolute error. So I propose this would be a good choice of metric for comparing two matrices for approximate equality as needed for testing optimizers.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2376
https://github.com/root-project/root/pull/2377:186,integrability,filter,filter,186,[DF] Introducing Visitor Pattern; - [x] Visitor skeleton not supporting jitting or virtual calls. - [ ] Visitor skeleton supporting jitting and virtual calls. - [ ] Use visitor to print filter names,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2377
https://github.com/root-project/root/pull/2377:48,interoperability,skeleton,skeleton,48,[DF] Introducing Visitor Pattern; - [x] Visitor skeleton not supporting jitting or virtual calls. - [ ] Visitor skeleton supporting jitting and virtual calls. - [ ] Use visitor to print filter names,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2377
https://github.com/root-project/root/pull/2377:112,interoperability,skeleton,skeleton,112,[DF] Introducing Visitor Pattern; - [x] Visitor skeleton not supporting jitting or virtual calls. - [ ] Visitor skeleton supporting jitting and virtual calls. - [ ] Use visitor to print filter names,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2377
https://github.com/root-project/root/pull/2377:61,usability,support,supporting,61,[DF] Introducing Visitor Pattern; - [x] Visitor skeleton not supporting jitting or virtual calls. - [ ] Visitor skeleton supporting jitting and virtual calls. - [ ] Use visitor to print filter names,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2377
https://github.com/root-project/root/pull/2377:121,usability,support,supporting,121,[DF] Introducing Visitor Pattern; - [x] Visitor skeleton not supporting jitting or virtual calls. - [ ] Visitor skeleton supporting jitting and virtual calls. - [ ] Use visitor to print filter names,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2377
https://github.com/root-project/root/pull/2378:595,availability,operat,operations,595,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:764,availability,operat,operations,764,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:931,availability,operat,operations,931,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1305,availability,error,errors,1305,"nd computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1461,availability,error,errors,1461,"ogram. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksan",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1514,availability,slo,slow,1514,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:11,deployability,automat,automatic,11,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:117,deployability,automat,automatic,117,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:329,deployability,automat,automatic,329,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:694,deployability,log,log,694,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:824,deployability,automat,automatically,824,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1658,deployability,Automat,Automatic,1658,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1760,deployability,depend,dependencies,1760,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1779,deployability,patch,patch,1779,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:2448,deployability,Patch,Patch,2448,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1633,energy efficiency,optim,optimization,1633,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:228,integrability,coupl,coupled,228,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:253,integrability,transform,transformation,253,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:617,integrability,sub,subtraction,617,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1760,integrability,depend,dependencies,1760,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:74,interoperability,plug,plugin,74,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:253,interoperability,transform,transformation,253,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:438,interoperability,specif,specified,438,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1800,interoperability,interop,interoperate,1800,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:228,modifiability,coupl,coupled,228,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1760,modifiability,depend,dependencies,1760,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1800,modifiability,interop,interoperate,1800,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1305,performance,error,errors,1305,"nd computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1461,performance,error,errors,1461,"ogram. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksan",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1633,performance,optimiz,optimization,1633,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1514,reliability,slo,slow,1514,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:536,safety,compl,complicated,536,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:694,safety,log,log,694,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1305,safety,error,errors,1305,"nd computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1446,safety,compl,complexity,1446,"a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Pat",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1461,safety,error,errors,1461,"ogram. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksan",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1592,safety,input,inputs,1592,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1760,safety,depend,dependencies,1760,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1779,safety,patch,patch,1779,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:2448,safety,Patch,Patch,2448,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:536,security,compl,complicated,536,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:694,security,log,log,694,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1446,security,compl,complexity,1446,"a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Pat",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1779,security,patch,patch,1779,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:2448,security,Patch,Patch,2448,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:11,testability,automat,automatic,11,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:117,testability,automat,automatic,117,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:228,testability,coupl,coupled,228,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:329,testability,automat,automatic,329,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:694,testability,log,log,694,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:824,testability,automat,automatically,824,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1658,testability,Automat,Automatic,1658,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1760,testability,depend,dependencies,1760,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:147,usability,user,user-defined,147,"Enable the automatic differentiation library clad in ROOT.; clad is a C++ plugin for clang and cling that implements automatic. differentiation of user-defined functions by employing the chain rule in. forward and reverse mode, coupled with source code transformation and AST. constant fold. In mathematics and computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""M",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1305,usability,error,errors,1305,"nd computer algebra, automatic differentiation (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1347,usability,cancel,cancellation,1347,"n (AD) is a. set of techniques to numerically evaluate the derivative of a function. specified by a computer program. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of f",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1461,usability,error,errors,1461,"ogram. AD exploits the fact that every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksan",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1592,usability,input,inputs,1592,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:1832,usability,user,users,1832,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2378:2387,usability,Learn,Learn,2387,"hat every computer. program, no matter how complicated, executes a sequence of elementary. arithmetic operations (addition, subtraction, multiplication, division, etc.). and elementary functions (exp, log, sin, cos, etc.). By applying the chain. rule repeatedly to these operations, derivatives of arbitrary order can. be computed automatically, accurately to working precision, and using at. most a small constant factor more arithmetic operations than the original. program. AD is an alternative technique to symbolic and numerical differentiation. These classical methods run into problems: symbolic differentiation leads. to inefficient code (unless done carefully) and faces the difficulty of. converting a computer program into a single expression, while numerical. differentiation can introduce round-off errors in the discretization. process and cancellation. Both classical methods have problems with. calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial. derivatives of a function with respect to many inputs, as is needed for. gradient-based optimization algorithms. Automatic differentiation solves. all of these problems, at the expense of introducing more software. dependencies. This patch allows ROOT to interoperate with clad. Namely, users can ask. the interpreter to produce a derivative or a gradient to a known function. An illustrative example code for first order derivative:. root [0] #include ""Math/CladDerivator.h"". root [1] double my_pow2(double x) { return x*x; }. root [2] auto derivative_meta_obj = clad::differentiate(my_pow2, /*wrt 1-st argument*/0);. root [3] derivative_meta_obj.dump();. The code is: double my_pow2_darg0(double x) {. return (1. * x + x * 1.);. }. root [5] derivative_meta_obj.execute(1) // no iterations, at the cost of function call. (double) 2.0000000. Learn more about clad at https://github.com/vgvassilev/clad. Patch by Aleksandr Efremov(@efremale) and me!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2378
https://github.com/root-project/root/pull/2379:35,energy efficiency,Optim,Optimization,35,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:125,energy efficiency,optim,optimizers,125,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:7,modifiability,Refact,Refactor,7,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:59,modifiability,refact,refactored,59,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:7,performance,Refactor,Refactor,7,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:35,performance,Optimiz,Optimization,35,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:59,performance,refactor,refactored,59,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:125,performance,optimiz,optimizers,125,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:25,safety,Test,Tests,25,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:92,safety,test,tests,92,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:109,safety,test,testing,109,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:25,testability,Test,Tests,25,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:92,testability,test,tests,92,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:109,testability,test,testing,109,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2379:101,usability,support,support,101,[TMVA] Refactor MethodDL Tests for Optimization.; * I have refactored the existing methodDL tests to support testing various optimizers.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2379
https://github.com/root-project/root/pull/2381:10,modifiability,scal,scaling,10,TRef weak scaling; Improve thread scability of TRef. Creating and looking up a lot of TRef from the same processID now has practically perfect weak scaling. Use Read/Write lock where relevant. . Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2381
https://github.com/root-project/root/pull/2381:148,modifiability,scal,scaling,148,TRef weak scaling; Improve thread scability of TRef. Creating and looking up a lot of TRef from the same processID now has practically perfect weak scaling. Use Read/Write lock where relevant. . Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2381
https://github.com/root-project/root/pull/2381:172,performance,lock,lock,172,TRef weak scaling; Improve thread scability of TRef. Creating and looking up a lot of TRef from the same processID now has practically perfect weak scaling. Use Read/Write lock where relevant. . Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2381
https://github.com/root-project/root/pull/2381:195,performance,Cach,Cache,195,TRef weak scaling; Improve thread scability of TRef. Creating and looking up a lot of TRef from the same processID now has practically perfect weak scaling. Use Read/Write lock where relevant. . Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2381
https://github.com/root-project/root/pull/2381:123,reliability,pra,practically,123,TRef weak scaling; Improve thread scability of TRef. Creating and looking up a lot of TRef from the same processID now has practically perfect weak scaling. Use Read/Write lock where relevant. . Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2381
https://github.com/root-project/root/pull/2381:172,security,lock,lock,172,TRef weak scaling; Improve thread scability of TRef. Creating and looking up a lot of TRef from the same processID now has practically perfect weak scaling. Use Read/Write lock where relevant. . Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2381
https://github.com/root-project/root/pull/2382:0,deployability,Build,Build,0,Build system updates; * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2382
https://github.com/root-project/root/pull/2382:13,deployability,updat,updates,13,Build system updates; * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2382
https://github.com/root-project/root/pull/2382:162,deployability,version,version,162,Build system updates; * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2382
https://github.com/root-project/root/pull/2382:162,integrability,version,version,162,Build system updates; * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2382
https://github.com/root-project/root/pull/2382:162,modifiability,version,version,162,Build system updates; * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2382
https://github.com/root-project/root/pull/2382:13,safety,updat,updates,13,Build system updates; * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2382
https://github.com/root-project/root/pull/2382:13,security,updat,updates,13,Build system updates; * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2382
https://github.com/root-project/root/pull/2384:0,deployability,Build,Build,0,Build system updates (retry); * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2384
https://github.com/root-project/root/pull/2384:13,deployability,updat,updates,13,Build system updates (retry); * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2384
https://github.com/root-project/root/pull/2384:170,deployability,version,version,170,Build system updates (retry); * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2384
https://github.com/root-project/root/pull/2384:170,integrability,version,version,170,Build system updates (retry); * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2384
https://github.com/root-project/root/pull/2384:170,modifiability,version,version,170,Build system updates (retry); * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2384
https://github.com/root-project/root/pull/2384:13,safety,updat,updates,13,Build system updates (retry); * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2384
https://github.com/root-project/root/pull/2384:13,security,updat,updates,13,Build system updates (retry); * Fix builtin Davix by setting `$DAVIX_FOUND` to `TRUE`. * Fix [ROOT-9493](https://sft.its.cern.ch/jira/browse/ROOT-9493) by requiring same version of CMake as ROOT requires in `RooTUseFile.cmake`,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2384
https://github.com/root-project/root/pull/2386:86,deployability,updat,update,86,"Significantly speed-up back-propagation; This is achieved by parallelizing the weight update process. This PR does not fix the code quality issues mentioned by @Axel-Naumann or the warnings, since these issues are handled by #2321",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2386
https://github.com/root-project/root/pull/2386:61,performance,parallel,parallelizing,61,"Significantly speed-up back-propagation; This is achieved by parallelizing the weight update process. This PR does not fix the code quality issues mentioned by @Axel-Naumann or the warnings, since these issues are handled by #2321",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2386
https://github.com/root-project/root/pull/2386:110,reliability,doe,does,110,"Significantly speed-up back-propagation; This is achieved by parallelizing the weight update process. This PR does not fix the code quality issues mentioned by @Axel-Naumann or the warnings, since these issues are handled by #2321",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2386
https://github.com/root-project/root/pull/2386:86,safety,updat,update,86,"Significantly speed-up back-propagation; This is achieved by parallelizing the weight update process. This PR does not fix the code quality issues mentioned by @Axel-Naumann or the warnings, since these issues are handled by #2321",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2386
https://github.com/root-project/root/pull/2386:0,security,Sign,Significantly,0,"Significantly speed-up back-propagation; This is achieved by parallelizing the weight update process. This PR does not fix the code quality issues mentioned by @Axel-Naumann or the warnings, since these issues are handled by #2321",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2386
https://github.com/root-project/root/pull/2386:86,security,updat,update,86,"Significantly speed-up back-propagation; This is achieved by parallelizing the weight update process. This PR does not fix the code quality issues mentioned by @Axel-Naumann or the warnings, since these issues are handled by #2321",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2386
https://github.com/root-project/root/pull/2387:11,deployability,build,build,11,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2387
https://github.com/root-project/root/pull/2387:0,safety,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2387
https://github.com/root-project/root/pull/2387:0,testability,Test,Testing,0,Testing PR build.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2387
https://github.com/root-project/root/pull/2388:34,deployability,infrastructur,infrastructure,34,Add change with no effect to test infrastructure (not meant for merging);,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2388
https://github.com/root-project/root/pull/2388:29,safety,test,test,29,Add change with no effect to test infrastructure (not meant for merging);,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2388
https://github.com/root-project/root/pull/2388:29,testability,test,test,29,Add change with no effect to test infrastructure (not meant for merging);,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2388
https://github.com/root-project/root/pull/2389:2968,availability,operat,operations,2968," limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::Enabl",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3313,availability,operat,operations,3313,"xistance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3474,availability,operat,operations,3474,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:160,deployability,depend,dependencies,160,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2639,deployability,manag,manages,2639,"he scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager o",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3669,deployability,manag,manage,3669,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:921,energy efficiency,schedul,scheduler,921,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1091,energy efficiency,schedul,scheduler,1091," singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit paral",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1148,energy efficiency,schedul,scheduler,1148,"f) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1255,energy efficiency,schedul,scheduler,1255," co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1359,energy efficiency,schedul,scheduler,1359," . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each han",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1646,energy efficiency,schedul,scheduler,1646," x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2421,energy efficiency,schedul,scheduler,2421,"2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2639,energy efficiency,manag,manages,2639,"he scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager o",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3598,energy efficiency,schedul,scheduler,3598,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3669,energy efficiency,manag,manage,3669,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3914,energy efficiency,schedul,scheduler,3914,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3930,energy efficiency,schedul,scheduler,3930,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:4044,energy efficiency,Schedul,Scheduler,4044,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:4120,energy efficiency,schedul,scheduler,4120,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:4162,energy efficiency,schedul,scheduler,4162,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:4356,energy efficiency,schedul,scheduler,4356,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:160,integrability,depend,dependencies,160,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2141,integrability,event,event,2141,"The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default nu",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2584,integrability,sub,subset,2584,"till two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still d",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3043,interoperability,compatib,compatibility,3043,"viours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Schedu",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:160,modifiability,depend,dependencies,160,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:665,modifiability,interm,intermediate,665,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:611,performance,time,times,611,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:921,performance,schedul,scheduler,921,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1091,performance,schedul,scheduler,1091," singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit paral",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1148,performance,schedul,scheduler,1148,"f) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1255,performance,schedul,scheduler,1255," co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1359,performance,schedul,scheduler,1359," . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each han",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:1646,performance,schedul,scheduler,1646," x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2091,performance,parallel,parallelism,2091,"ler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2421,performance,schedul,scheduler,2421,"2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3598,performance,schedul,scheduler,3598,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3914,performance,schedul,scheduler,3914,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3930,performance,schedul,scheduler,3930,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:4044,performance,Schedul,Scheduler,4044,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:4120,performance,schedul,scheduler,4120,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:4162,performance,schedul,scheduler,4162,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:4356,performance,schedul,scheduler,4356,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:370,reliability,doe,doesn,370,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3488,reliability,Doe,Doesn,3488,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:160,safety,depend,dependencies,160,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2030,safety,avoid,avoid,2030,"ue set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for ba",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2639,safety,manag,manages,2639,"he scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager o",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3669,safety,manag,manage,3669,"izing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will destroy the IMT reference to the scheduler. The reference count of. // the scheduler reaches zero and it gets destroyed. ROOT::DisableIMT();. ROOT::EnableIMT(4);. TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. //The scheduler is still alive here because of executor3. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:160,testability,depend,dependencies,160,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:395,usability,behavi,behaviour,395,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:424,usability,user,user,424,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:492,usability,interact,interactions,492,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:824,usability,behavi,behaviour,824,"[WIP] Make multithreading work with independent tbb::task_arena instead of a tbb::task_scheduler singleton; ***tl;dr;*** This PR eliminates (almost all of) the dependencies between the implicit and explicit multithreading execution modes in ROOT and allows the co-existance of several TThreadExecutor instances, each one executing on a different number of threads. . It doesn't change any other behaviour in the eyes of the user. TO DO:. - [ ] Decide on explicit-implicit MT execution modes' interactions. - [ ] Rename TPoolManager. - [ ] Decide if allowing change of the number of threads when calling several times EnableImplicitMT(x) with a varying x without an intermediate call to DisableImplicitMT(). - [ ] Add warnings . ***********************************************************************************. # Previous behaviour. Previous to this PR, the number of threads was limited during the lifetime of the tbb scheduler, kept alive by TPoolManager as a ```std::shared_ptr``` as long as references to it existed, to the value set on its initialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! `",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2046,usability,behavi,behaviours,2046,"ialization. ```cpp. //We initialize the scheduler with 4 threads. ROOT::EnableIMT(4);. { . //The scheduler is active, so the value passed to TThreadExecutor. //is overriden with the number of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatib",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2229,usability,behavi,behaviour,2229,"umber of threads the scheduler has been. //initialized with (4). TThreadExecutor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 thr",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:2302,usability,support,support,2302,"Executor executor(9);. }. ROOT::DisableIMT();. //The scheduler is not alive at this point, . //so we initialize it with 2 threads. ROOT::EnableIMT(2);. ROOT::TThreadExecutor executor(8);. ROOT::DisableIMT();. executor.MapReduce(...); //Runs on two threads! ROOT::EnableIMT(3);. //Still two threads! TThreadExecutor instance was keeping the scheduler alive. ```. This also implies that given two co-existent instances of TThreadExecutor initialized with a different number of threads, the first one to be initialized forces on the second one the number of threads to work with. ```cpp. ROOT::TThreadExecutor executor(4);. ROOT::TThreadExecutor executor2(8); //will run limited to two threads!! ```. This setup was useful [to avoid undefined behaviours between the implicit and explicit parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2389:3083,usability,interact,interaction,3083,"t parallelism modes of ROOT](https://indico.cern.ch/event/607814/contributions/2466931/attachments/1409778/2155811/TScheduler.pdf). . # New behaviour. With this PR, we can separate this two modes of execution. We support the co-existance of several TThreadExecutors, each handling a different number of threads, by initializing the scheduler with the [default number of threads](tbb::task_scheduler_init::default_num_threads) and using a ```tbb::task_arena``` per TThreadExecutor to work with a subset of them instead. ```cpp. //Each of the executor manages its own tbb::task_arena,. //which allows the co-existance of TThreadExecutors. //handling different number of threads. TThreadExecutor executor1(8); //will run on 8 threads. TThreadExecutor executor2(4); //will run on 4 threads. //IMT keeps a different task Arena too! ROOT::EnableIMT(4); //4 threads will be used in IMT operations. //executor3 will be initialized with 4 threads for backward. //compatibility. Should we not allow this interaction? . //Should it be initialized with the default number of threads? TThreadExecutor executor3; //Implicit constructor. Initialized with 4 threads. ROOT::DisableIMT();. ROOT::EnableIMT(2); //2 threads will be used in IMT operations. ROOT::TThreadExecutor executor(8); //Explicit number of threads. . // Will execute on 8 threads. ROOT::EnableIMT(4); //2 threads will be used in IMT operations. //Doesn't change until disabled! . //Should we allow it instead? ROOT::DisableIMT();. ```. We still destroy the scheduler when not in use, and TPoolManager only reason to exist is to manage it's lifetime (needs a name change):. ```cpp. {. //TThreadExecutor holds a shared_ptr to the tbb::task_scheduler. TThreadExecutor executor1(8); //will run on 8 threads. }. //executor1 went out of scope and was destroyed together with the scheduler. //No scheduler active at this point. ROOT::EnableIMT(4);. //""IMT"" holds holds a shared_ptr to the tbb::task_scheduler. Scheduler alive here. // DisableIMT() will de",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2389
https://github.com/root-project/root/pull/2391:10,modifiability,scal,scaling,10,TRef weak scaling v614; Use Read/Write lock where relevant. Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime. This addresses: https://root-forum.cern.ch/t/copying-trefs-and-accessing-tref-data-from-multiple-threads/29417,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2391
https://github.com/root-project/root/pull/2391:39,performance,lock,lock,39,TRef weak scaling v614; Use Read/Write lock where relevant. Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime. This addresses: https://root-forum.cern.ch/t/copying-trefs-and-accessing-tref-data-from-multiple-threads/29417,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2391
https://github.com/root-project/root/pull/2391:60,performance,Cach,Cache,60,TRef weak scaling v614; Use Read/Write lock where relevant. Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime. This addresses: https://root-forum.cern.ch/t/copying-trefs-and-accessing-tref-data-from-multiple-threads/29417,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2391
https://github.com/root-project/root/pull/2391:39,security,lock,lock,39,TRef weak scaling v614; Use Read/Write lock where relevant. Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime. This addresses: https://root-forum.cern.ch/t/copying-trefs-and-accessing-tref-data-from-multiple-threads/29417,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2391
https://github.com/root-project/root/pull/2391:277,security,access,accessing-tref-data-from-multiple-threads,277,TRef weak scaling v614; Use Read/Write lock where relevant. Cache the last result of TProcessID::IsValid and TProcessID::GetProcessWithUID as. most often the same PID will be used for most of the process lifetime. This addresses: https://root-forum.cern.ch/t/copying-trefs-and-accessing-tref-data-from-multiple-threads/29417,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2391
https://github.com/root-project/root/pull/2392:52,interoperability,conflict,conflicts,52,[DF] Minor fixes; To be merged after #2313 to avoid conflicts for @imaxoi,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2392
https://github.com/root-project/root/pull/2392:46,safety,avoid,avoid,46,[DF] Minor fixes; To be merged after #2313 to avoid conflicts for @imaxoi,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2392
https://github.com/root-project/root/pull/2393:834,availability,Ping,Pinging,834,"[DF] Add GetColumnType utility to RInterface; This is just a proposal for a simple but hopefully useful feature. The idea is to let people check the type of a given RDF column:. ```cpp. df.GetColumnType(""x""); // returns e.g. ""int"". ```. Some usecases in which `GetColumnType` might be useful:. 1. a help to use RDF in interpreted C++ (check the type of a column, then call `df.Take<T>(""x"")` with the right type. 2. a help to write PyROOT utilities on top of RDF: often such utilities need to jit some RDF call but have no way to know what are the required template types. 3. easy way to programmatically get the type of a TTree branch (`TTree::Print` is easy but does not return anything, `TLeaf::GetTypeName` is less straightforward to use correctly, e.g. if I'm not mistaken it returns `int` for an `int[n]`). What do people think? Pinging @stwunsch @amadio @dpiparo @etejedor @Axel-Naumann",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2393
https://github.com/root-project/root/pull/2393:663,reliability,doe,does,663,"[DF] Add GetColumnType utility to RInterface; This is just a proposal for a simple but hopefully useful feature. The idea is to let people check the type of a given RDF column:. ```cpp. df.GetColumnType(""x""); // returns e.g. ""int"". ```. Some usecases in which `GetColumnType` might be useful:. 1. a help to use RDF in interpreted C++ (check the type of a column, then call `df.Take<T>(""x"")` with the right type. 2. a help to write PyROOT utilities on top of RDF: often such utilities need to jit some RDF call but have no way to know what are the required template types. 3. easy way to programmatically get the type of a TTree branch (`TTree::Print` is easy but does not return anything, `TLeaf::GetTypeName` is less straightforward to use correctly, e.g. if I'm not mistaken it returns `int` for an `int[n]`). What do people think? Pinging @stwunsch @amadio @dpiparo @etejedor @Axel-Naumann",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2393
https://github.com/root-project/root/pull/2393:76,testability,simpl,simple,76,"[DF] Add GetColumnType utility to RInterface; This is just a proposal for a simple but hopefully useful feature. The idea is to let people check the type of a given RDF column:. ```cpp. df.GetColumnType(""x""); // returns e.g. ""int"". ```. Some usecases in which `GetColumnType` might be useful:. 1. a help to use RDF in interpreted C++ (check the type of a column, then call `df.Take<T>(""x"")` with the right type. 2. a help to write PyROOT utilities on top of RDF: often such utilities need to jit some RDF call but have no way to know what are the required template types. 3. easy way to programmatically get the type of a TTree branch (`TTree::Print` is easy but does not return anything, `TLeaf::GetTypeName` is less straightforward to use correctly, e.g. if I'm not mistaken it returns `int` for an `int[n]`). What do people think? Pinging @stwunsch @amadio @dpiparo @etejedor @Axel-Naumann",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2393
https://github.com/root-project/root/pull/2393:76,usability,simpl,simple,76,"[DF] Add GetColumnType utility to RInterface; This is just a proposal for a simple but hopefully useful feature. The idea is to let people check the type of a given RDF column:. ```cpp. df.GetColumnType(""x""); // returns e.g. ""int"". ```. Some usecases in which `GetColumnType` might be useful:. 1. a help to use RDF in interpreted C++ (check the type of a column, then call `df.Take<T>(""x"")` with the right type. 2. a help to write PyROOT utilities on top of RDF: often such utilities need to jit some RDF call but have no way to know what are the required template types. 3. easy way to programmatically get the type of a TTree branch (`TTree::Print` is easy but does not return anything, `TLeaf::GetTypeName` is less straightforward to use correctly, e.g. if I'm not mistaken it returns `int` for an `int[n]`). What do people think? Pinging @stwunsch @amadio @dpiparo @etejedor @Axel-Naumann",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2393
https://github.com/root-project/root/pull/2393:299,usability,help,help,299,"[DF] Add GetColumnType utility to RInterface; This is just a proposal for a simple but hopefully useful feature. The idea is to let people check the type of a given RDF column:. ```cpp. df.GetColumnType(""x""); // returns e.g. ""int"". ```. Some usecases in which `GetColumnType` might be useful:. 1. a help to use RDF in interpreted C++ (check the type of a column, then call `df.Take<T>(""x"")` with the right type. 2. a help to write PyROOT utilities on top of RDF: often such utilities need to jit some RDF call but have no way to know what are the required template types. 3. easy way to programmatically get the type of a TTree branch (`TTree::Print` is easy but does not return anything, `TLeaf::GetTypeName` is less straightforward to use correctly, e.g. if I'm not mistaken it returns `int` for an `int[n]`). What do people think? Pinging @stwunsch @amadio @dpiparo @etejedor @Axel-Naumann",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2393
https://github.com/root-project/root/pull/2393:417,usability,help,help,417,"[DF] Add GetColumnType utility to RInterface; This is just a proposal for a simple but hopefully useful feature. The idea is to let people check the type of a given RDF column:. ```cpp. df.GetColumnType(""x""); // returns e.g. ""int"". ```. Some usecases in which `GetColumnType` might be useful:. 1. a help to use RDF in interpreted C++ (check the type of a column, then call `df.Take<T>(""x"")` with the right type. 2. a help to write PyROOT utilities on top of RDF: often such utilities need to jit some RDF call but have no way to know what are the required template types. 3. easy way to programmatically get the type of a TTree branch (`TTree::Print` is easy but does not return anything, `TLeaf::GetTypeName` is less straightforward to use correctly, e.g. if I'm not mistaken it returns `int` for an `int[n]`). What do people think? Pinging @stwunsch @amadio @dpiparo @etejedor @Axel-Naumann",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2393
https://github.com/root-project/root/pull/2395:156,availability,error,error,156,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:279,deployability,version,version,279,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:348,deployability,modul,modules,348,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:676,deployability,probe,probelem,676,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:279,integrability,version,version,279,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:279,modifiability,version,version,279,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:338,modifiability,extens,extension,338,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:348,modifiability,modul,modules,348,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:156,performance,error,error,156,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:156,safety,error,error,156,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:348,safety,modul,modules,348,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:419,safety,avoid,avoids,419,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:537,safety,avoid,avoid,537,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:156,usability,error,error,156,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:219,usability,document,documentation,219,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2395:504,usability,indicat,indicates,504,"[PyROOT] Port fixes for Python3.7; * PyROOT_PyUnicode_AsString changed return type from char* to const char*. * Using _PyObject_GC_TRACK results in linking error:. undefined reference to `_PyGC_generation0'. The python documentation says this about _PyObject_GC_TRACK:. ""A macro version of PyObject_GC_Track(). It should not be used for. extension modules."". So it should not be used. Calling PyObject_GC_Track instead avoids the. undefined symbol. I also removed the #ifndef R__WIN32 since the. comment indicates that this was added to avoid the same issue on. windows. Calling PyObject_GC_Track instead of using the. _PyObject_GC_TRACK macro should fix the undefined symbol probelem on. windows too.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2395
https://github.com/root-project/root/pull/2396:429,availability,consist,consistent,429,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:102,deployability,contain,contains,102,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:111,deployability,depend,dependency,111,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:305,deployability,modul,modules,305,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:348,deployability,Modul,Module,348,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:371,deployability,contain,contains,371,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:407,deployability,modul,modulename,407,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:111,integrability,depend,dependency,111,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:111,modifiability,depend,dependency,111,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:305,modifiability,modul,modules,305,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:348,modifiability,Modul,Module,348,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:407,modifiability,modul,modulename,407,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:111,safety,depend,dependency,111,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:280,safety,input,input,280,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:305,safety,modul,modules,305,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:348,safety,Modul,Module,348,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:407,safety,modul,modulename,407,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:194,security,ident,identifier,194,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:111,testability,depend,dependency,111,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:17,usability,support,support,17,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:280,usability,input,input,280,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2396:429,usability,consist,consistent,429,"[cxxmodules] Add support for ACLiC with cxxmodules; For ACLiC, we have to generate "".out"" files which contains dependency. library name separated by "" "". This was done by creating. std::vector<(identifier from rootmap file), (library name)> and comparing this with decls from the input file. However with modules, we want not to use rootmap files. Module's name. already contains information about library (modulename should be. consistent to the library) so we need not to store all information in. the vector anymore.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2396
https://github.com/root-project/root/pull/2397:16,usability,Custom,Custom,16,"Fix ROOT-9533 - Custom colors are not saved into .root and .json output; Custom colors were not saved in the root files, as described here https://sft.its.cern.ch/jira/browse/ROOT-9533",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2397
https://github.com/root-project/root/pull/2397:73,usability,Custom,Custom,73,"Fix ROOT-9533 - Custom colors are not saved into .root and .json output; Custom colors were not saved in the root files, as described here https://sft.its.cern.ch/jira/browse/ROOT-9533",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2397
https://github.com/root-project/root/pull/2398:511,deployability,log,logic,511,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:746,deployability,log,logic,746,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:58,energy efficiency,Current,Currently,58,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:26,integrability,wrap,wrapper,26,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:122,integrability,event,event,122,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:332,integrability,event,event,332,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:529,integrability,filter,filters,529,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:26,interoperability,wrapper,wrapper,26,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:146,performance,time,time,146,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:729,performance,time,time,729,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:842,performance,time,time,842,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:195,safety,compl,complex,195,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:283,safety,detect,detect,283,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:511,safety,log,logic,511,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:746,safety,log,logic,746,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:878,safety,valid,valid,878,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:195,security,compl,complex,195,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:283,security,detect,detect,283,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:511,security,log,logic,511,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:746,security,log,logic,746,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:511,testability,log,logic,511,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:746,testability,log,logic,746,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:768,testability,simpl,simplified,768,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2398:768,usability,simpl,simplified,768,"[DF] Add RJittedAction, a wrapper around jitted RActions; Currently, jitted actions spawn into existence right before the event. loop (at jitting time). This makes it impossible or unnecessarily complex to:. 1) let RResultPtrs own actions. 2) let actions own their previous node. 3) detect that an action has been booked before the event loop has run. Points 1 and 2 are required by ROOT-9416. Point 3 is required by several graph-traversing features, e.g. ROOT-9458. The solution is to align the jitted action logic with jitted filters and. defines, and use a placeholder RJittedAction object that sits into the. computation graph and forwards all relevant calls to the concrete, jitted. action which will be created at a later time. RResultPtr logic is also greatly simplified since its action pointer can now always be set at construction time and is always guaranteed to be valid (it points to the RJittedAction owned by the RLoopManager).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2398
https://github.com/root-project/root/pull/2399:7,deployability,Updat,Update,7,[TMVA] Update User's Guide authors;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2399
https://github.com/root-project/root/pull/2399:7,safety,Updat,Update,7,[TMVA] Update User's Guide authors;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2399
https://github.com/root-project/root/pull/2399:7,security,Updat,Update,7,[TMVA] Update User's Guide authors;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2399
https://github.com/root-project/root/pull/2399:27,security,auth,authors,27,[TMVA] Update User's Guide authors;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2399
https://github.com/root-project/root/pull/2399:14,usability,User,User,14,[TMVA] Update User's Guide authors;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2399
https://github.com/root-project/root/pull/2399:21,usability,Guid,Guide,21,[TMVA] Update User's Guide authors;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2399
https://github.com/root-project/root/pull/2400:270,safety,test,tests,270,"Add TGraph2D::GetPoint; Implementation and behavior is the same as `TGraph::GetPoint`. PR was motivated by a friend from ATLAS who asked why `TGraph` had a `GetPoint` method but `TGraph2D` didn't. If this feature is undesired feel free to close the PR. I could not find tests for `TGraph2D`'s methods anywhere, so I attach a minimal test for this method here:. ```cpp. #include <TGraph2D.h>. #include <TError.h>. int main(). {. TGraph2D g;. g.SetPoint(0, 1., 2., 3.);. double x, y, z;. R__ASSERT(-1 == g.GetPoint(-3, x, y, z));. R__ASSERT(-1 == g.GetPoint(1, x, y, z));. R__ASSERT(0 == g.GetPoint(0, x, y, z));. R__ASSERT(1. == x);. R__ASSERT(2. == y);. R__ASSERT(3. == z);. return 0;. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2400
https://github.com/root-project/root/pull/2400:333,safety,test,test,333,"Add TGraph2D::GetPoint; Implementation and behavior is the same as `TGraph::GetPoint`. PR was motivated by a friend from ATLAS who asked why `TGraph` had a `GetPoint` method but `TGraph2D` didn't. If this feature is undesired feel free to close the PR. I could not find tests for `TGraph2D`'s methods anywhere, so I attach a minimal test for this method here:. ```cpp. #include <TGraph2D.h>. #include <TError.h>. int main(). {. TGraph2D g;. g.SetPoint(0, 1., 2., 3.);. double x, y, z;. R__ASSERT(-1 == g.GetPoint(-3, x, y, z));. R__ASSERT(-1 == g.GetPoint(1, x, y, z));. R__ASSERT(0 == g.GetPoint(0, x, y, z));. R__ASSERT(1. == x);. R__ASSERT(2. == y);. R__ASSERT(3. == z);. return 0;. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2400
https://github.com/root-project/root/pull/2400:270,testability,test,tests,270,"Add TGraph2D::GetPoint; Implementation and behavior is the same as `TGraph::GetPoint`. PR was motivated by a friend from ATLAS who asked why `TGraph` had a `GetPoint` method but `TGraph2D` didn't. If this feature is undesired feel free to close the PR. I could not find tests for `TGraph2D`'s methods anywhere, so I attach a minimal test for this method here:. ```cpp. #include <TGraph2D.h>. #include <TError.h>. int main(). {. TGraph2D g;. g.SetPoint(0, 1., 2., 3.);. double x, y, z;. R__ASSERT(-1 == g.GetPoint(-3, x, y, z));. R__ASSERT(-1 == g.GetPoint(1, x, y, z));. R__ASSERT(0 == g.GetPoint(0, x, y, z));. R__ASSERT(1. == x);. R__ASSERT(2. == y);. R__ASSERT(3. == z);. return 0;. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2400
https://github.com/root-project/root/pull/2400:333,testability,test,test,333,"Add TGraph2D::GetPoint; Implementation and behavior is the same as `TGraph::GetPoint`. PR was motivated by a friend from ATLAS who asked why `TGraph` had a `GetPoint` method but `TGraph2D` didn't. If this feature is undesired feel free to close the PR. I could not find tests for `TGraph2D`'s methods anywhere, so I attach a minimal test for this method here:. ```cpp. #include <TGraph2D.h>. #include <TError.h>. int main(). {. TGraph2D g;. g.SetPoint(0, 1., 2., 3.);. double x, y, z;. R__ASSERT(-1 == g.GetPoint(-3, x, y, z));. R__ASSERT(-1 == g.GetPoint(1, x, y, z));. R__ASSERT(0 == g.GetPoint(0, x, y, z));. R__ASSERT(1. == x);. R__ASSERT(2. == y);. R__ASSERT(3. == z);. return 0;. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2400
https://github.com/root-project/root/pull/2400:43,usability,behavi,behavior,43,"Add TGraph2D::GetPoint; Implementation and behavior is the same as `TGraph::GetPoint`. PR was motivated by a friend from ATLAS who asked why `TGraph` had a `GetPoint` method but `TGraph2D` didn't. If this feature is undesired feel free to close the PR. I could not find tests for `TGraph2D`'s methods anywhere, so I attach a minimal test for this method here:. ```cpp. #include <TGraph2D.h>. #include <TError.h>. int main(). {. TGraph2D g;. g.SetPoint(0, 1., 2., 3.);. double x, y, z;. R__ASSERT(-1 == g.GetPoint(-3, x, y, z));. R__ASSERT(-1 == g.GetPoint(1, x, y, z));. R__ASSERT(0 == g.GetPoint(0, x, y, z));. R__ASSERT(1. == x);. R__ASSERT(2. == y);. R__ASSERT(3. == z);. return 0;. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2400
https://github.com/root-project/root/pull/2400:239,usability,close,close,239,"Add TGraph2D::GetPoint; Implementation and behavior is the same as `TGraph::GetPoint`. PR was motivated by a friend from ATLAS who asked why `TGraph` had a `GetPoint` method but `TGraph2D` didn't. If this feature is undesired feel free to close the PR. I could not find tests for `TGraph2D`'s methods anywhere, so I attach a minimal test for this method here:. ```cpp. #include <TGraph2D.h>. #include <TError.h>. int main(). {. TGraph2D g;. g.SetPoint(0, 1., 2., 3.);. double x, y, z;. R__ASSERT(-1 == g.GetPoint(-3, x, y, z));. R__ASSERT(-1 == g.GetPoint(1, x, y, z));. R__ASSERT(0 == g.GetPoint(0, x, y, z));. R__ASSERT(1. == x);. R__ASSERT(2. == y);. R__ASSERT(3. == z);. return 0;. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2400
https://github.com/root-project/root/pull/2400:325,usability,minim,minimal,325,"Add TGraph2D::GetPoint; Implementation and behavior is the same as `TGraph::GetPoint`. PR was motivated by a friend from ATLAS who asked why `TGraph` had a `GetPoint` method but `TGraph2D` didn't. If this feature is undesired feel free to close the PR. I could not find tests for `TGraph2D`'s methods anywhere, so I attach a minimal test for this method here:. ```cpp. #include <TGraph2D.h>. #include <TError.h>. int main(). {. TGraph2D g;. g.SetPoint(0, 1., 2., 3.);. double x, y, z;. R__ASSERT(-1 == g.GetPoint(-3, x, y, z));. R__ASSERT(-1 == g.GetPoint(1, x, y, z));. R__ASSERT(0 == g.GetPoint(0, x, y, z));. R__ASSERT(1. == x);. R__ASSERT(2. == y);. R__ASSERT(3. == z);. return 0;. }. ```",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2400
https://github.com/root-project/root/pull/2401:48,availability,operat,operator,48,"Matrix patch; This promotes the return value of operator* to TGeoHMatrix, to be able to handle the result of the multiplication of e.g. translation * rotation correctly. Also adds Multiply and MultiplyLeft with const TGeoMatrix& signature",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2401
https://github.com/root-project/root/pull/2401:7,deployability,patch,patch,7,"Matrix patch; This promotes the return value of operator* to TGeoHMatrix, to be able to handle the result of the multiplication of e.g. translation * rotation correctly. Also adds Multiply and MultiplyLeft with const TGeoMatrix& signature",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2401
https://github.com/root-project/root/pull/2401:136,integrability,translat,translation,136,"Matrix patch; This promotes the return value of operator* to TGeoHMatrix, to be able to handle the result of the multiplication of e.g. translation * rotation correctly. Also adds Multiply and MultiplyLeft with const TGeoMatrix& signature",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2401
https://github.com/root-project/root/pull/2401:136,interoperability,translat,translation,136,"Matrix patch; This promotes the return value of operator* to TGeoHMatrix, to be able to handle the result of the multiplication of e.g. translation * rotation correctly. Also adds Multiply and MultiplyLeft with const TGeoMatrix& signature",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2401
https://github.com/root-project/root/pull/2401:7,safety,patch,patch,7,"Matrix patch; This promotes the return value of operator* to TGeoHMatrix, to be able to handle the result of the multiplication of e.g. translation * rotation correctly. Also adds Multiply and MultiplyLeft with const TGeoMatrix& signature",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2401
https://github.com/root-project/root/pull/2401:7,security,patch,patch,7,"Matrix patch; This promotes the return value of operator* to TGeoHMatrix, to be able to handle the result of the multiplication of e.g. translation * rotation correctly. Also adds Multiply and MultiplyLeft with const TGeoMatrix& signature",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2401
https://github.com/root-project/root/pull/2401:150,security,rotat,rotation,150,"Matrix patch; This promotes the return value of operator* to TGeoHMatrix, to be able to handle the result of the multiplication of e.g. translation * rotation correctly. Also adds Multiply and MultiplyLeft with const TGeoMatrix& signature",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2401
https://github.com/root-project/root/pull/2401:229,security,sign,signature,229,"Matrix patch; This promotes the return value of operator* to TGeoHMatrix, to be able to handle the result of the multiplication of e.g. translation * rotation correctly. Also adds Multiply and MultiplyLeft with const TGeoMatrix& signature",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2401
https://github.com/root-project/root/pull/2402:98,deployability,modul,modules,98,[cxxmodule] Fix outdated documentation & rename GenerateModule; GenerateModule was NOT generating modules since November 2017. It is. just checking the module validity.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2402
https://github.com/root-project/root/pull/2402:152,deployability,modul,module,152,[cxxmodule] Fix outdated documentation & rename GenerateModule; GenerateModule was NOT generating modules since November 2017. It is. just checking the module validity.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2402
https://github.com/root-project/root/pull/2402:98,modifiability,modul,modules,98,[cxxmodule] Fix outdated documentation & rename GenerateModule; GenerateModule was NOT generating modules since November 2017. It is. just checking the module validity.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2402
https://github.com/root-project/root/pull/2402:152,modifiability,modul,module,152,[cxxmodule] Fix outdated documentation & rename GenerateModule; GenerateModule was NOT generating modules since November 2017. It is. just checking the module validity.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2402
https://github.com/root-project/root/pull/2402:98,safety,modul,modules,98,[cxxmodule] Fix outdated documentation & rename GenerateModule; GenerateModule was NOT generating modules since November 2017. It is. just checking the module validity.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2402
https://github.com/root-project/root/pull/2402:152,safety,modul,module,152,[cxxmodule] Fix outdated documentation & rename GenerateModule; GenerateModule was NOT generating modules since November 2017. It is. just checking the module validity.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2402
https://github.com/root-project/root/pull/2402:159,safety,valid,validity,159,[cxxmodule] Fix outdated documentation & rename GenerateModule; GenerateModule was NOT generating modules since November 2017. It is. just checking the module validity.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2402
https://github.com/root-project/root/pull/2402:25,usability,document,documentation,25,[cxxmodule] Fix outdated documentation & rename GenerateModule; GenerateModule was NOT generating modules since November 2017. It is. just checking the module validity.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2402
https://github.com/root-project/root/pull/2403:25,availability,operat,operator,25,Extended return value of operator* to TGeoHMatrix.; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2403
https://github.com/root-project/root/pull/2403:80,availability,operat,operator,80,Extended return value of operator* to TGeoHMatrix.; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2403
https://github.com/root-project/root/pull/2403:148,integrability,translat,translation,148,Extended return value of operator* to TGeoHMatrix.; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2403
https://github.com/root-project/root/pull/2403:148,interoperability,translat,translation,148,Extended return value of operator* to TGeoHMatrix.; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2403
https://github.com/root-project/root/pull/2403:0,modifiability,Exten,Extended,0,Extended return value of operator* to TGeoHMatrix.; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2403
https://github.com/root-project/root/pull/2403:162,security,rotat,rotation,162,Extended return value of operator* to TGeoHMatrix.; This promotes the result of operator* for matrices to TGeoHMatrix to be able to handle properly translation * rotation. Added Multiply(Left) methods taking const matrix reference.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2403
https://github.com/root-project/root/pull/2404:18,deployability,build,build,18,Add default CLion build paths to gitignore;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2404
https://github.com/root-project/root/pull/2406:51,deployability,patch,patch,51,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:164,deployability,manag,manager,164,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:217,deployability,build,build,217,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:164,energy efficiency,manag,manager,164,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:12,integrability,wrap,wrapper,12,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:95,integrability,sub,subfolders,95,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:284,integrability,compon,components,284,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:12,interoperability,wrapper,wrapper,12,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:284,interoperability,compon,components,284,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:156,modifiability,pac,package,156,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:284,modifiability,compon,components,284,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:51,safety,patch,patch,51,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:164,safety,manag,manager,164,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:51,security,patch,patch,51,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
https://github.com/root-project/root/pull/2406:66,security,control,control,66,[WIP] Add a wrapper around add_subdirectory.; This patch gives us control over the addition in subfolders in cmake. It is necessary ingredient for the root package manager where it needs. to control precisely how the build is confugured and possibly ignore. some of the non-requested components.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2406
