id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/852:152,availability,fault,faulty,152,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:202,deployability,instal,installation,202,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,energy efficiency,fault,faulty,152,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,performance,fault,faulty,152,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,reliability,fault,faulty,152,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:278,reliability,Doe,Does,278,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,safety,fault,faulty,152,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:293,testability,simpl,simple,293,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:293,usability,simpl,simple,293,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:49,availability,error,error,49,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:70,deployability,version,version,70,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:70,integrability,version,version,70,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:70,modifiability,version,version,70,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:49,performance,error,error,49,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:49,safety,error,error,49,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:49,usability,error,error,49,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:106,usability,user,user-images,106,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:241,usability,user,user-images,241,Hello . I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:17,usability,user,user-images,17,![image](https://user-images.githubusercontent.com/48261734/65530790-340e0f80-debe-11e9-9208-61115d63d2f9.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:75,deployability,version,versions,75,This is a lot of warnings. @LuckyMD is this normal or is anything with the versions wrong?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:75,integrability,version,versions,75,This is a lot of warnings. @LuckyMD is this normal or is anything with the versions wrong?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:75,modifiability,version,versions,75,This is a lot of warnings. @LuckyMD is this normal or is anything with the versions wrong?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:150,availability,error,errors,150,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:184,availability,error,errors,184,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:291,availability,error,error,291,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:150,performance,error,errors,150,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:184,performance,error,errors,184,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:291,performance,error,error,291,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:150,safety,error,errors,150,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:184,safety,error,errors,184,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:291,safety,error,error,291,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:150,usability,error,errors,150,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:184,usability,error,errors,184,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:291,usability,error,error,291,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:174,reliability,doe,doesn,174,"My guess is that it has to do with the data passed to `posx` and `posy`, which cannot be interpreted by the plotting function, and therefore ignored... and then the plotting doesn't work because you do have a figure of dimension 0 in the end, as there is nothing to plot... as you suggested before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:348,availability,error,errors,348,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:478,availability,error,error,478,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:399,deployability,version,version,399,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:399,integrability,version,version,399,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:159,modifiability,scal,scalars,159,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:399,modifiability,version,version,399,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:348,performance,error,errors,348,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:478,performance,error,error,478,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:348,safety,error,errors,348,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:478,safety,error,error,478,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:348,usability,error,errors,348,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:478,usability,error,error,478,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:350,availability,error,errors,350,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:480,availability,error,error,480,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:401,deployability,version,version,401,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:401,integrability,version,version,401,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:161,modifiability,scal,scalars,161,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:401,modifiability,version,version,401,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:350,performance,error,errors,350,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:480,performance,error,error,480,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:350,safety,error,errors,350,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:480,safety,error,error,480,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:350,usability,error,errors,350,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:480,usability,error,error,480,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error? my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,availability,error,errors,152,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:190,availability,error,errors,190,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:297,availability,error,error,297,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:566,deployability,version,versions,566,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:566,integrability,version,versions,566,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:566,modifiability,version,versions,566,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,performance,error,errors,152,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:190,performance,error,errors,190,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:297,performance,error,error,297,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,safety,error,errors,152,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:190,safety,error,errors,190,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:297,safety,error,error,297,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,usability,error,errors,152,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:190,usability,error,errors,190,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:297,usability,error,error,297,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. > . > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows? matplotlib is working well with my windows and I tried to run . pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:8,deployability,instal,install,8,can you install matplotlib 3.0? We have seen an unrelated problem with matplotlib 3.1 and thus we recommend not to install it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:115,deployability,instal,install,115,can you install matplotlib 3.0? We have seen an unrelated problem with matplotlib 3.1 and thus we recommend not to install it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/853:18,deployability,modul,module,18,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:155,deployability,contain,contain,155,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:209,deployability,instal,installs,209,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:220,deployability,modul,module,220,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:274,deployability,log,logging,274,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:18,modifiability,modul,module,18,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:49,modifiability,pac,package,49,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:99,modifiability,pac,packages,99,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:141,modifiability,pac,packages,141,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:220,modifiability,modul,module,220,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:18,safety,modul,module,18,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:180,safety,except,exception,180,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:220,safety,modul,module,220,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:274,safety,log,logging,274,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:274,security,log,logging,274,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:274,testability,log,logging,274,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:60,availability,error,error,60,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:274,deployability,modul,module,274,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:355,deployability,modul,module,355,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:380,deployability,API,API,380,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:645,deployability,modul,module,645,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:908,deployability,modul,module,908,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1037,deployability,log,logging,1037,"it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(ta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1048,deployability,log,logg,1048,"shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.whic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1104,deployability,modul,module,1104,"rt scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: mod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1289,deployability,modul,module,1289,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1333,deployability,version,versions,1333,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1672,deployability,modul,module,1672,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1845,deployability,modul,modules,1845,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1923,deployability,modul,module,1923,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:2104,deployability,modul,module,2104,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:2197,deployability,updat,update,2197,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:66,integrability,messag,message,66,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:380,integrability,API,API,380,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1333,integrability,version,versions,1333,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1769,integrability,filter,filters,1769,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1784,integrability,Filter,Filters,1784,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1908,integrability,filter,filters,1908,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:66,interoperability,messag,message,66,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:380,interoperability,API,API,380,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:274,modifiability,modul,module,274,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:355,modifiability,modul,module,355,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:645,modifiability,modul,module,645,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:908,modifiability,modul,module,908,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1104,modifiability,modul,module,1104,"rt scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: mod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1257,modifiability,pac,packages,1257,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1289,modifiability,modul,module,1289,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1333,modifiability,version,versions,1333,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1363,modifiability,extens,extension,1363,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1633,modifiability,pac,packages,1633,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1672,modifiability,modul,module,1672,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1845,modifiability,modul,modules,1845,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1885,modifiability,pac,packages,1885,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1923,modifiability,modul,module,1923,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:2104,modifiability,modul,module,2104,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:2208,modifiability,pac,package,2208,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:60,performance,error,error,60,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:60,safety,error,error,60,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:248,safety,input,input-,248,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:274,safety,modul,module,274,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:355,safety,modul,module,355,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:645,safety,modul,module,645,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:908,safety,modul,module,908,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1037,safety,log,logging,1037,"it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(ta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1048,safety,log,logg,1048,"shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.whic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1104,safety,modul,module,1104,"rt scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: mod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1289,safety,modul,module,1289,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1672,safety,modul,module,1672,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1845,safety,modul,modules,1845,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1923,safety,modul,module,1923,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:2104,safety,modul,module,2104,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:2197,safety,updat,update,2197,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1037,security,log,logging,1037,"it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(ta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1048,security,log,logg,1048,"shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.whic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:2197,security,updat,update,2197,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:204,testability,Trace,Traceback,204,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1037,testability,log,logging,1037,"it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(ta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1048,testability,log,logg,1048,"shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.whic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:60,usability,error,error,60,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:248,usability,input,input-,248,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:308,usability,Document,Documents,308,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:464,usability,tool,tools,464,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:506,usability,tool,tools,506,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:592,usability,Document,Documents,592,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:623,usability,tool,tools,623,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:859,usability,Document,Documents,859,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:890,usability,tool,tools,890,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1056,usability,Document,Documents,1056,"rror message. . Here is my code. . ```py. import scanpy. ```. Here is what the computer showed after I ran this code:. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_versi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:1816,usability,user,user,1816,". <ipython-input-2-135279188441> in <module>. ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>. 34 # the actual API. 35 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 36 from . import tools as tl. 37 from . import preprocessing as pp. 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>. 15 from ._leiden import leiden. 16 from ._louvain import louvain. ---> 17 from ._sim import sim. 18 from ._score_genes import score_genes, score_genes_cell_cycle. 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>. 23 . 24 from .. import _utils. ---> 25 from .. import readwrite. 26 from .._settings import settings. 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>. 7 import numpy as np. 8 import pandas as pd. ----> 9 import tables. 10 import anndata. 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>. 91 . 92 # Necessary imports to get versions stored on the cython extension. ---> 93 from .utilsextension import (. 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,. 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>. 122 from .flavor import restrict_flavors. 123 from .description import *. --> 124 from .filters import Filters. 125 . 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>. 27 from tables.req_versions import min_blosc_bitshuffle_version. 28 . ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]). 30 . 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'. ```. I used Jupyter Notebook. Should I update any package? Thank you so much! .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:16,deployability,updat,updating,16,"well, generally updating everything could fix it, but another alternative is that you have a `tables.py` in your $PYTHONPATH (`sys.path`) which gets imported instead of the correct pytables version. See https://github.com/pandas-dev/pandas/issues/26052#issuecomment-482116173",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:190,deployability,version,version,190,"well, generally updating everything could fix it, but another alternative is that you have a `tables.py` in your $PYTHONPATH (`sys.path`) which gets imported instead of the correct pytables version. See https://github.com/pandas-dev/pandas/issues/26052#issuecomment-482116173",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:190,integrability,version,version,190,"well, generally updating everything could fix it, but another alternative is that you have a `tables.py` in your $PYTHONPATH (`sys.path`) which gets imported instead of the correct pytables version. See https://github.com/pandas-dev/pandas/issues/26052#issuecomment-482116173",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:190,modifiability,version,version,190,"well, generally updating everything could fix it, but another alternative is that you have a `tables.py` in your $PYTHONPATH (`sys.path`) which gets imported instead of the correct pytables version. See https://github.com/pandas-dev/pandas/issues/26052#issuecomment-482116173",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:16,safety,updat,updating,16,"well, generally updating everything could fix it, but another alternative is that you have a `tables.py` in your $PYTHONPATH (`sys.path`) which gets imported instead of the correct pytables version. See https://github.com/pandas-dev/pandas/issues/26052#issuecomment-482116173",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:16,security,updat,updating,16,"well, generally updating everything could fix it, but another alternative is that you have a `tables.py` in your $PYTHONPATH (`sys.path`) which gets imported instead of the correct pytables version. See https://github.com/pandas-dev/pandas/issues/26052#issuecomment-482116173",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/855:110,deployability,version,version,110,"Hi! In general, we support python 3.6+ (f-strings are just too nice to not have). Are you able to use a newer version of python instead?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:110,integrability,version,version,110,"Hi! In general, we support python 3.6+ (f-strings are just too nice to not have). Are you able to use a newer version of python instead?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:110,modifiability,version,version,110,"Hi! In general, we support python 3.6+ (f-strings are just too nice to not have). Are you able to use a newer version of python instead?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:19,usability,support,support,19,"Hi! In general, we support python 3.6+ (f-strings are just too nice to not have). Are you able to use a newer version of python instead?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:60,deployability,instal,installing,60,Thanks for the suggestion. I actually solved the problem by installing a local miniconda with newer version of Python 3. Thank you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:100,deployability,version,version,100,Thanks for the suggestion. I actually solved the problem by installing a local miniconda with newer version of Python 3. Thank you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:100,integrability,version,version,100,Thanks for the suggestion. I actually solved the problem by installing a local miniconda with newer version of Python 3. Thank you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:100,modifiability,version,version,100,Thanks for the suggestion. I actually solved the problem by installing a local miniconda with newer version of Python 3. Thank you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/856:87,energy efficiency,load,loaded,87,"Hi @k3yavi, I would be great to have a tutorial in which an alevin generated matrix is loaded into scanpy. Your suggestion is to have such a tutorial hosted by scanpy or you plan to add it to your list of tutorials?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:87,performance,load,loaded,87,"Hi @k3yavi, I would be great to have a tutorial in which an alevin generated matrix is loaded into scanpy. Your suggestion is to have such a tutorial hosted by scanpy or you plan to add it to your list of tutorials?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:174,testability,plan,plan,174,"Hi @k3yavi, I would be great to have a tutorial in which an alevin generated matrix is loaded into scanpy. Your suggestion is to have such a tutorial hosted by scanpy or you plan to add it to your list of tutorials?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:845,availability,down,downstream,845,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:348,energy efficiency,Current,Currently,348,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:545,energy efficiency,load,load,545,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:338,interoperability,format,formats,338,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:430,interoperability,format,format,430,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:495,interoperability,format,formats,495,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:662,modifiability,pac,package,662,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:545,performance,load,load,545,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:564,performance,memor,memory,564,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:112,usability,efficien,efficient,112,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:125,usability,interact,interact,125,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:229,usability,efficien,efficiently,229,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:300,usability,support,support,300,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:564,usability,memor,memory,564,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:597,usability,support,support,597,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:699,usability,support,support,699,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:711,usability,efficien,efficient,711,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:829,usability,interact,interaction,829,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:870,usability,efficien,efficient,870,"Hi @fidelram ,. Thanks for the response. I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ? Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:223,availability,avail,available,223,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:188,deployability,api,api,188,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:65,integrability,repositor,repository,65,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:188,integrability,api,api,188,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:65,interoperability,repositor,repository,65,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:188,interoperability,api,api,188,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:223,reliability,availab,available,223,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:223,safety,avail,available,223,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:223,security,availab,available,223,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/pull/857:210,deployability,depend,dependency,210,Remaining Qs:. * Does this need support for nans? * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:210,integrability,depend,dependency,210,Remaining Qs:. * Does this need support for nans? * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:210,modifiability,depend,dependency,210,Remaining Qs:. * Does this need support for nans? * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:17,reliability,Doe,Does,17,Remaining Qs:. * Does this need support for nans? * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:210,safety,depend,dependency,210,Remaining Qs:. * Does this need support for nans? * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:210,testability,depend,dependency,210,Remaining Qs:. * Does this need support for nans? * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:32,usability,support,support,32,Remaining Qs:. * Does this need support for nans? * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:123,deployability,log,logic,123,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:430,deployability,scale,scaler,430,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:499,deployability,scale,scaler,499,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:513,deployability,scale,scaler,513,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:810,deployability,scale,scaler,810,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:879,deployability,scale,scaler,879,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:893,deployability,scale,scaler,893,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:430,energy efficiency,scale,scaler,430,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:499,energy efficiency,scale,scaler,499,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:513,energy efficiency,scale,scaler,513,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:810,energy efficiency,scale,scaler,810,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:879,energy efficiency,scale,scaler,879,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:893,energy efficiency,scale,scaler,893,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:414,interoperability,Standard,StandardScaler,414,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:439,interoperability,Standard,StandardScaler,439,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:602,interoperability,Standard,StandardScaler,602,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:794,interoperability,Standard,StandardScaler,794,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:819,interoperability,Standard,StandardScaler,819,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:430,modifiability,scal,scaler,430,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:499,modifiability,scal,scaler,499,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:513,modifiability,scal,scaler,513,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:810,modifiability,scal,scaler,810,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:879,modifiability,scal,scaler,879,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:893,modifiability,scal,scaler,893,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:430,performance,scale,scaler,430,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:499,performance,scale,scaler,499,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:513,performance,scale,scaler,513,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:810,performance,scale,scaler,810,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:879,performance,scale,scaler,879,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:893,performance,scale,scaler,893,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:123,safety,log,logic,123,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:123,security,log,logic,123,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:123,testability,log,logic,123,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:38,usability,learn,learn,38,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:51,usability,learn,learn,51,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py. if not issparse(X):. mean = np.mean(X, axis=0, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64). var = mean_sq - mean ** 2. elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py. if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):. from sklearn.preprocessing import StandardScaler. scaler = StandardScaler(with_mean=False).partial_fit(X). mean, var = scaler.mean_, scaler.var_. else:. mean, var = sparse_mean_variance_axis(X, axis=0) . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:4,interoperability,Standard,StandardScaler,4,so `StandardScaler` only gives 32 bit floats and isn’t faster than your solution?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:247,deployability,scale,scaler,247,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:311,deployability,scale,scaler,311,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:325,deployability,scale,scaler,325,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:247,energy efficiency,scale,scaler,247,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:311,energy efficiency,scale,scaler,311,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:325,energy efficiency,scale,scaler,325,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:106,interoperability,Standard,StandardScaler,106,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:201,interoperability,format,format,201,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:256,interoperability,Standard,StandardScaler,256,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:247,modifiability,scal,scaler,247,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:311,modifiability,scal,scaler,311,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:325,modifiability,scal,scaler,325,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:247,performance,scale,scaler,247,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:311,performance,scale,scaler,311,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:325,performance,scale,scaler,325,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:339,performance,time,timeit,339,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:430,performance,time,timeit,430,"A little surprisingly, looks like it is! ```python. import numpy as np. from sklearn.preprocessing import StandardScaler. from scipy import sparse. import scanpy as sc. s = sparse.random(10000, 10000, format=""csr"", dtype=np.float32). def old(X):. scaler = StandardScaler(with_mean=False).partial_fit(X). return scaler.mean_, scaler.var_. %timeit old(s). # 10.6 ms ± 58.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). %timeit sc.pp._utils.sparse_mean_variance_axis(s, 0). # 5.8 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). ```. I think it'd be worth it to have better accuracy even if it's a bit more expensive though. I've definitely had variances be much less stable than I would like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/issues/859:191,deployability,API,API,191,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place! Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:191,integrability,API,API,191,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place! Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:191,interoperability,API,API,191,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place! Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:433,testability,simpl,simply,433,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place! Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:433,usability,simpl,simply,433,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place! Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:509,usability,help,help,509,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place! Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:290,deployability,integr,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:504,deployability,integr,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:290,integrability,integr,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:323,integrability,coupl,couple,323,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:454,integrability,interfac,interface,454,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:504,integrability,integr,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:290,interoperability,integr,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:454,interoperability,interfac,interface,454,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:504,interoperability,integr,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:290,modifiability,integr,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:323,modifiability,coupl,couple,323,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:454,modifiability,interfac,interface,454,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:504,modifiability,integr,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:290,reliability,integr,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:504,reliability,integr,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:290,security,integr,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:504,security,integr,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:290,testability,integr,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:323,testability,coupl,couple,323,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:504,testability,integr,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:. https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:207,availability,cluster,clusters,207,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:607,availability,cluster,clusters,607,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:207,deployability,cluster,clusters,207,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:607,deployability,cluster,clusters,607,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:367,integrability,batch,batch,367,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:587,interoperability,specif,specific,587,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:367,performance,batch,batch,367,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:620,performance,perform,perform,620,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:196,security,ident,identified,196,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:596,security,ident,identified,596,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:404,usability,command,command,404,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:620,usability,perform,perform,620,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that? Thanks again!! Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:126,deployability,continu,continue,126,"Hi Joao, . These types of questions should be addressed in the discourse group. Please move there and ask again. We shouldn't continue a thread here that was closed due to wrong posting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:158,usability,close,closed,158,"Hi Joao, . These types of questions should be addressed in the discourse group. Please move there and ask again. We shouldn't continue a thread here that was closed due to wrong posting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/pull/862:57,availability,error,errors,57,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:170,availability,error,errors,170,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:159,interoperability,format,formatting,159,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:57,performance,error,errors,57,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:170,performance,error,errors,170,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:57,safety,error,errors,57,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:170,safety,error,errors,170,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:57,usability,error,errors,57,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:170,usability,error,errors,170,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:75,modifiability,pac,packages,75,I guess this should probably be added to scanpy external as other external packages are?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:54,availability,error,errors,54,"Thanks for getting back to me. I fixed the formatting errors and moved trimap to scanpy external. . trimap is no longer imported by default, so the overall import time is unaffected. . I also added an example to the docstring. Please let me know if further fixes are required.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:43,interoperability,format,formatting,43,"Thanks for getting back to me. I fixed the formatting errors and moved trimap to scanpy external. . trimap is no longer imported by default, so the overall import time is unaffected. . I also added an example to the docstring. Please let me know if further fixes are required.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:54,performance,error,errors,54,"Thanks for getting back to me. I fixed the formatting errors and moved trimap to scanpy external. . trimap is no longer imported by default, so the overall import time is unaffected. . I also added an example to the docstring. Please let me know if further fixes are required.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:163,performance,time,time,163,"Thanks for getting back to me. I fixed the formatting errors and moved trimap to scanpy external. . trimap is no longer imported by default, so the overall import time is unaffected. . I also added an example to the docstring. Please let me know if further fixes are required.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:54,safety,error,errors,54,"Thanks for getting back to me. I fixed the formatting errors and moved trimap to scanpy external. . trimap is no longer imported by default, so the overall import time is unaffected. . I also added an example to the docstring. Please let me know if further fixes are required.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:54,usability,error,errors,54,"Thanks for getting back to me. I fixed the formatting errors and moved trimap to scanpy external. . trimap is no longer imported by default, so the overall import time is unaffected. . I also added an example to the docstring. Please let me know if further fixes are required.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:18,interoperability,conflict,conflicts,18,I fixed the merge conflicts. Hope it is all good now! Please let me know if you need more changes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:95,availability,error,error,95,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:122,deployability,modul,modules,122,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:122,modifiability,modul,modules,122,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:95,performance,error,error,95,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:95,safety,error,error,95,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:122,safety,modul,modules,122,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:187,safety,test,test,187,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:187,testability,test,test,187,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:38,usability,feedback,feedback,38,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:95,usability,error,error,95,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:146,deployability,updat,updated,146,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:154,deployability,version,version,154,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:190,deployability,fail,fail,190,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:263,energy efficiency,profil,profiler,263,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:154,integrability,version,version,154,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:154,modifiability,version,version,154,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:263,performance,profil,profiler,263,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:176,reliability,Doe,Does,176,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:190,reliability,fail,fail,190,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:146,safety,updat,updated,146,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:185,safety,test,test,185,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:146,security,updat,updated,146,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:185,testability,test,test,185,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:252,safety,test,tests,252,Thanks @flying-sheep! I haven't added any new imports other those for type hints and copying anndata: [https://github.com/eamid/scanpy/compare/9ae6c19...b7ed705](https://github.com/eamid/scanpy/compare/9ae6c19...b7ed705). My previous commit passed the tests. Could this be because of recent commits on your side?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:252,testability,test,tests,252,Thanks @flying-sheep! I haven't added any new imports other those for type hints and copying anndata: [https://github.com/eamid/scanpy/compare/9ae6c19...b7ed705](https://github.com/eamid/scanpy/compare/9ae6c19...b7ed705). My previous commit passed the tests. Could this be because of recent commits on your side?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:75,usability,hint,hints,75,Thanks @flying-sheep! I haven't added any new imports other those for type hints and copying anndata: [https://github.com/eamid/scanpy/compare/9ae6c19...b7ed705](https://github.com/eamid/scanpy/compare/9ae6c19...b7ed705). My previous commit passed the tests. Could this be because of recent commits on your side?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:84,interoperability,specif,specific,84,"I have the same issue here https://github.com/theislab/scanpy/pull/945, so it's not specific to this PR,fyi.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:160,deployability,depend,dependency,160,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:194,deployability,modul,module,194,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:160,integrability,depend,dependency,160,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:160,modifiability,depend,dependency,160,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:194,modifiability,modul,module,194,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:105,reliability,doe,does,105,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:160,safety,depend,dependency,160,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:194,safety,modul,module,194,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:160,testability,depend,dependency,160,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:3,usability,help,help,3,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/issues/863:164,deployability,api,api,164,and may be also this?? . abs(fold_change_matrix) > min_fold_change. there is also no need to change the help in https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.tl.filter_rank_genes_groups.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:164,integrability,api,api,164,and may be also this?? . abs(fold_change_matrix) > min_fold_change. there is also no need to change the help in https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.tl.filter_rank_genes_groups.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:164,interoperability,api,api,164,and may be also this?? . abs(fold_change_matrix) > min_fold_change. there is also no need to change the help in https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.tl.filter_rank_genes_groups.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:104,usability,help,help,104,and may be also this?? . abs(fold_change_matrix) > min_fold_change. there is also no need to change the help in https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.tl.filter_rank_genes_groups.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:998,availability,down,down,998,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1088,availability,cluster,cluster,1088," out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but han",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1779,availability,consist,consistent,1779,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1896,availability,down,downregulated,1896,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:2097,availability,down,downregulated,2097,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:469,deployability,log,logfoldchanges,469,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:968,deployability,log,log,968,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:972,deployability,scale,scale,972,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1088,deployability,cluster,cluster,1088," out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but han",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1947,deployability,updat,update,1947,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:972,energy efficiency,scale,scale,972,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:132,integrability,filter,filtering,132,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:225,integrability,discover,discover,225,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:911,integrability,filter,filtering,911,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1481,integrability,filter,filtered,1481,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:225,interoperability,discover,discover,225,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:972,modifiability,scal,scale,972,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:972,performance,scale,scale,972,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1428,reliability,doe,does,1428,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:469,safety,log,logfoldchanges,469,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:968,safety,log,log,968,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1258,safety,compl,completely,1258,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1947,safety,updat,update,1947,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:469,security,log,logfoldchanges,469,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:968,security,log,log,968,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1258,security,compl,completely,1258,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1947,security,updat,update,1947,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:469,testability,log,logfoldchanges,469,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:968,testability,log,log,968,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:225,usability,discov,discover,225,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1250,usability,user,user,1250,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:1779,usability,consist,consistent,1779,"changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167. - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained. - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?! - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:117,availability,consist,consistent,117,"@Koncopd, since you're already looking at the DE code, would you mind taking a look at this? We should definitely be consistent about how we calculate this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:117,usability,consist,consistent,117,"@Koncopd, since you're already looking at the DE code, would you mind taking a look at this? We should definitely be consistent about how we calculate this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:56,integrability,filter,filtering,56,Has this been solved? By the way I see some cases where filtering doesn't work. I still see a lot of genes not passing the fold threshold surviving. . And depleted genes are also there,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:66,reliability,doe,doesn,66,Has this been solved? By the way I see some cases where filtering doesn't work. I still see a lot of genes not passing the fold threshold surviving. . And depleted genes are also there,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:318,availability,consist,consistently,318,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:43,deployability,updat,updated,43,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:156,deployability,updat,updated,156,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:178,deployability,version,version,178,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:695,deployability,automat,automatically,695,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:178,integrability,version,version,178,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:353,integrability,filter,filter,353,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:557,integrability,filter,filter,557,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:709,integrability,filter,filtered,709,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:178,modifiability,version,version,178,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:43,safety,updat,updated,43,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:156,safety,updat,updated,156,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:43,security,updat,updated,43,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:156,security,updat,updated,156,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:695,testability,automat,automatically,695,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:318,usability,consist,consistently,318,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:606,usability,user,user,606,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. . - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to. - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/864:30,deployability,log,logFC,30,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:188,deployability,log,loged,188,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:437,deployability,log,logFC,437,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:285,performance,time,times,285,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:492,reliability,doe,doesn,492,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:30,safety,log,logFC,30,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:188,safety,log,loged,188,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:437,safety,log,logFC,437,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:30,security,log,logFC,30,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:188,security,log,loged,188,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:437,security,log,logFC,437,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:30,testability,log,logFC,30,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:188,testability,log,loged,188,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:437,testability,log,logFC,437,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:252,usability,intuit,intuitive,252,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/pull/865:141,availability,sli,slight,141,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:259,availability,down,downstream,259,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:426,modifiability,maintain,maintaining,426,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:148,performance,perform,performance,148,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:365,performance,time,times,365,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:141,reliability,sli,slight,141,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:297,safety,test,tested,297,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:426,safety,maintain,maintaining,426,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:442,safety,input,input,442,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:297,testability,test,tested,297,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:148,usability,perform,performance,148,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:442,usability,input,input,442,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:56,availability,down,downstream,56,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:223,modifiability,maintain,maintaining,223,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:162,performance,time,times,162,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:94,safety,test,tested,94,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:223,safety,maintain,maintaining,223,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:239,safety,input,input,239,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:94,testability,test,tested,94,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:239,usability,input,input,239,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:787,deployability,pipelin,pipelines,787,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:214,energy efficiency,estimat,estimate,214,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:787,integrability,pipelin,pipelines,787,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:292,modifiability,layer,layers,292,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:366,modifiability,layer,layers,366,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:410,modifiability,paramet,parameter,410,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:421,modifiability,layer,layers,421,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:648,modifiability,layer,layers,648,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:141,availability,down,downcast,141,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:155,interoperability,incompatib,incompatible,155,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:21,modifiability,maintain,maintain,21,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:42,modifiability,layer,layers,42,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:65,modifiability,maintain,maintaining,65,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:21,safety,maintain,maintain,21,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:65,safety,maintain,maintaining,65,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:310,availability,down,downcast,310,"Hah, I've gotten much better at numba since I wrote this function. I figured out I can just get the core part to work on floats and don't have to worry about casting between types. Makes this a much easier decision. Now floats aren't converted to integers in the first place. > We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64. I think we can be a little flexible on this, and just generally follow numpy promotion rules (except for when they're bad).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:100,energy efficiency,core,core,100,"Hah, I've gotten much better at numba since I wrote this function. I figured out I can just get the core part to work on floats and don't have to worry about casting between types. Makes this a much easier decision. Now floats aren't converted to integers in the first place. > We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64. I think we can be a little flexible on this, and just generally follow numpy promotion rules (except for when they're bad).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:324,interoperability,incompatib,incompatible,324,"Hah, I've gotten much better at numba since I wrote this function. I figured out I can just get the core part to work on floats and don't have to worry about casting between types. Makes this a much easier decision. Now floats aren't converted to integers in the first place. > We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64. I think we can be a little flexible on this, and just generally follow numpy promotion rules (except for when they're bad).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:518,safety,except,except,518,"Hah, I've gotten much better at numba since I wrote this function. I figured out I can just get the core part to work on floats and don't have to worry about casting between types. Makes this a much easier decision. Now floats aren't converted to integers in the first place. > We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64. I think we can be a little flexible on this, and just generally follow numpy promotion rules (except for when they're bad).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:408,interoperability,convers,conversion,408,"> But you still convert ints to floats? `downsample_counts` now doesn't convert anything, or even copy if you don't want it to. It just modifies the passed matrix (or copy, if `copy=True`) whether it has integer or floating point values. > And when are they bad? This one is pretty weird:. ```python. np.result_type(np.int64, np.uint64). # dtype('float64'). ```. But also numpy does promotion (or some other conversion, not quite sure for this one) when it probably shouldn't:. ```python. type(np.uint8(255) << 1). # numpy.int64. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:64,reliability,doe,doesn,64,"> But you still convert ints to floats? `downsample_counts` now doesn't convert anything, or even copy if you don't want it to. It just modifies the passed matrix (or copy, if `copy=True`) whether it has integer or floating point values. > And when are they bad? This one is pretty weird:. ```python. np.result_type(np.int64, np.uint64). # dtype('float64'). ```. But also numpy does promotion (or some other conversion, not quite sure for this one) when it probably shouldn't:. ```python. type(np.uint8(255) << 1). # numpy.int64. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:378,reliability,doe,does,378,"> But you still convert ints to floats? `downsample_counts` now doesn't convert anything, or even copy if you don't want it to. It just modifies the passed matrix (or copy, if `copy=True`) whether it has integer or floating point values. > And when are they bad? This one is pretty weird:. ```python. np.result_type(np.int64, np.uint64). # dtype('float64'). ```. But also numpy does promotion (or some other conversion, not quite sure for this one) when it probably shouldn't:. ```python. type(np.uint8(255) << 1). # numpy.int64. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:136,security,modif,modifies,136,"> But you still convert ints to floats? `downsample_counts` now doesn't convert anything, or even copy if you don't want it to. It just modifies the passed matrix (or copy, if `copy=True`) whether it has integer or floating point values. > And when are they bad? This one is pretty weird:. ```python. np.result_type(np.int64, np.uint64). # dtype('float64'). ```. But also numpy does promotion (or some other conversion, not quite sure for this one) when it probably shouldn't:. ```python. type(np.uint8(255) << 1). # numpy.int64. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:184,safety,compl,completely,184,"> This one is pretty weird:. > . > ```py. > np.result_type(np.int64, np.uint64). > # dtype('float64'). > ```. That’s also a bug, the following code changes the first integer (+=1) and completely destroys the second one, without need. ```py. >>> infos = [np.iinfo(dt).max for dt in [np.int64, np.uint64]]. >>> infos. [9223372036854775807, 18446744073709551615]. >>> np.array(infos). array([9.22337204e+18, 1.84467441e+19]. >>> np.array(infos).astype(np.uint64). array([9223372036854775808, 0], dtype=uint64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:184,security,compl,completely,184,"> This one is pretty weird:. > . > ```py. > np.result_type(np.int64, np.uint64). > # dtype('float64'). > ```. That’s also a bug, the following code changes the first integer (+=1) and completely destroys the second one, without need. ```py. >>> infos = [np.iinfo(dt).max for dt in [np.int64, np.uint64]]. >>> infos. [9223372036854775807, 18446744073709551615]. >>> np.array(infos). array([9.22337204e+18, 1.84467441e+19]. >>> np.array(infos).astype(np.uint64). array([9223372036854775808, 0], dtype=uint64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/issues/866:26,integrability,filter,filtering,26,Are you talking about the filtering function that remove cells? If you want to contribute a PR for this please do it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:15,usability,help,help,15,"I'm asking for help, :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:617,integrability,Batch,Batch,617,"I've wrote some code in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:664,integrability,batch,batch,664,"I've wrote some code in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:922,integrability,Batch,Batch,922,"I've wrote some code in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:969,integrability,batch,batch,969,"I've wrote some code in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1031,integrability,filter,filtering,1031,"(show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1213,integrability,sub,subset,1213,"ode:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1527,integrability,Filter,Filtered,1527,"or = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1715,integrability,batch,batch,1715,"x, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1849,integrability,batch,batch,1849,"""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 70",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1976,integrability,sub,subset,1976," dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:2300,integrability,Filter,Filtered,2300,"in(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:2631,integrability,sub,subset,2631,"). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:2936,integrability,Filter,Filtered,2936,"]==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > sink(). >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:3120,integrability,batch,batch,3120,"]==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > sink(). >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:3264,integrability,batch,batch,3264,"]==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > sink(). >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:3391,integrability,sub,subset,3391,"]==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > sink(). >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:3706,integrability,Filter,Filtered,3706,"]==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > sink(). >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1014,interoperability,distribut,distribution,1014," in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] +",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:617,performance,Batch,Batch,617,"I've wrote some code in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:664,performance,batch,batch,664,"I've wrote some code in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:922,performance,Batch,Batch,922,"I've wrote some code in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:969,performance,batch,batch,969,"I've wrote some code in R to remove (show the min and max of n_genes and percent_mito) by boxplot. It would read the metadata of the ""obs.csv"" and give the txt file which includes the result. The following is the R code:. > . > setwd(""Z:/Scanpy/unofficial fig""). > library(stringr). > library(ggplot2). > . > Total_Matrix <- read.csv(""Z:/Scanpy/unofficial fig/Pre-Metadata/obs.csv"", header = T). > . > Plot <- ggplot(Total_Matrix, aes(x = n_genes)). > png(filename = ""Pre-Metadata - n_genes - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - n_genes - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > Plot <- ggplot(Total_Matrix, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1715,performance,batch,batch,1715,"x, aes(x = percent_mito)). > png(filename = ""Pre-Metadata - percent_mito - Total.png""). > Plot + geom_density(color = ""black"", fill = ""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:1849,performance,batch,batch,1849,"""gray""). > dev.off(). > png(filename = ""Pre-Metadata - percent_mito - Batch.png""). > Plot + geom_density(aes(color = batch)). > dev.off(). > . > . > sink(""Normal distribution for filtering.txt""). > print(""object: Total n_genes""). > ob = Total_Matrix$n_genes. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: n_genes"", batch_name[i])). > ob = Total_Matrix$n_genes[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 70",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:3120,performance,batch,batch,3120,"]==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > sink(). >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:3264,performance,batch,batch,3264,"]==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The min and max:""). > print(c(min(nb),max(nb))). > print(' '). > figurename <- paste(""n_genes"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""n_genes"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > . > print(' '). > print(""object: Total percent_mito""). > ob = Total_Matrix$percent_mito. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", ""Total.png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > . > batch_name <- row.names(as.matrix(summary(Total_Matrix$batch))). > for (i in 1:length(batch_name)){. > print(c(""object: percent_mito"", batch_name[i])). > ob = Total_Matrix$percent_mito[Total_Matrix['batch']==batch_name[i]]. > qb <- quantile(ob, probs = c(.25, .75), na.rm = TRUE). > rb <- 1.5 * IQR(ob, na.rm = TRUE). > nb <- subset(ob, ob > (qb[1] - rb) & ob < (qb[2] + rb)). > print(""The max:""). > print(max(nb)). > print(' '). > figurename <- paste(""percent_mito"", ""Original"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 700, width = 200). > boxplot(ob). > dev.off(). > figurename <- paste(""percent_mito"", ""Filtered"", batch_name[i], "".png"", sep = "" - ""). > png(filename = figurename, height = 300, width = 200). > boxplot(nb). > dev.off(). > }. > sink(). >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/868:841,availability,slo,slower,841,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1692,availability,avail,available,1692,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1807,availability,avail,available,1807,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:276,deployability,releas,release,276,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:567,energy efficiency,model,model,567,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:968,energy efficiency,load,load,968,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1178,energy efficiency,model,model,1178,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1277,energy efficiency,model,model,1277,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1102,integrability,transform,transforming,1102,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1578,integrability,filter,filtering,1578,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1102,interoperability,transform,transforming,1102,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1198,interoperability,standard,standard,1198,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:86,modifiability,pac,package,86,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:109,modifiability,pac,package,109,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:244,modifiability,pac,package,244,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1635,modifiability,variab,variable,1635,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1710,modifiability,pac,package,1710,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:968,performance,load,load,968,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:347,reliability,doe,does,347,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:395,reliability,doe,does,395,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:841,reliability,slo,slower,841,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1692,reliability,availab,available,1692,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1807,reliability,availab,available,1807,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1692,safety,avail,available,1692,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1807,safety,avail,available,1807,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:567,security,model,model,567,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1178,security,model,model,1178,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1277,security,model,model,1277,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1692,security,availab,available,1692,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1807,security,availab,available,1807,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1518,testability,simpl,simpler,1518,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1302,usability,close,closed-form,1302,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1518,usability,simpl,simpler,1518,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1783,usability,help,help,1783,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:. * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood. * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting). * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster. * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:490,availability,slo,slow,490,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:517,deployability,releas,release,517,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:480,energy efficiency,current,currently,480,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:425,modifiability,pac,package,425,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:230,performance,bottleneck,bottleneck,230,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:490,reliability,slo,slow,490,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:192,safety,compl,complex,192,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:353,safety,except,except,353,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:192,security,compl,complex,192,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then? A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:50,availability,avail,available,50,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:145,deployability,instal,installable,145,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:157,modifiability,pac,package,157,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:50,reliability,availab,available,50,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:50,safety,avail,available,50,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:187,safety,test,tests,187,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:50,security,availab,available,50,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:182,testability,unit,unit,182,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:187,testability,test,tests,187,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:19,availability,avail,available,19,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:96,deployability,automat,automated,96,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:4,modifiability,pac,package,4,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:19,reliability,availab,available,19,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:19,safety,avail,available,19,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:106,safety,test,test,106,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:170,safety,test,tests,170,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:19,security,availab,available,19,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:96,testability,automat,automated,96,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:106,testability,test,test,106,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:170,testability,test,tests,170,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:126,deployability,log,log-transforming,126,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:254,energy efficiency,model,modeled,254,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:287,energy efficiency,model,model,287,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:367,energy efficiency,model,model,367,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:617,energy efficiency,model,model,617,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:691,energy efficiency,model,model,691,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:106,integrability,protocol,protocols,106,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:130,integrability,transform,transforming,130,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:106,interoperability,protocol,protocols,106,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:130,interoperability,transform,transforming,130,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:477,performance,perform,performance,477,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:80,reliability,doe,does,80,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:359,reliability,doe,doesn,359,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:126,safety,log,log-transforming,126,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:126,security,log,log-transforming,126,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:254,security,model,modeled,254,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:287,security,model,model,287,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:367,security,model,model,367,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:617,security,model,model,617,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:653,security,control,control,653,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:691,security,model,model,691,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:126,testability,log,log-transforming,126,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:653,testability,control,control,653,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:477,usability,perform,performance,477,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:44,energy efficiency,model,model,44,I agree that there is definitely no need to model zero inflation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:44,security,model,model,44,I agree that there is definitely no need to model zero inflation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:89,deployability,log,log-transform,89,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:334,deployability,log,log-transforming,334,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:361,deployability,log,log,361,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1014,deployability,log,log,1014,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:167,energy efficiency,current,current,167,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:318,energy efficiency,current,currently,318,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:765,energy efficiency,model,model,765,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:93,integrability,transform,transform,93,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:338,integrability,transform,transforming,338,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1018,integrability,transform,transform,1018,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:93,interoperability,transform,transform,93,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:202,interoperability,distribut,distributed,202,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:338,interoperability,transform,transforming,338,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1018,interoperability,transform,transform,1018,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1121,reliability,doe,doesn,1121,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:89,safety,log,log-transform,89,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:334,safety,log,log-transforming,334,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:361,safety,log,log,361,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1014,safety,log,log,1014,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:89,security,log,log-transform,89,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:334,security,log,log-transforming,334,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:361,security,log,log,361,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:765,security,model,model,765,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1014,security,log,log,1014,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:89,testability,log,log-transform,89,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:334,testability,log,log-transforming,334,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:361,testability,log,log,361,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1014,testability,log,log,1014,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:534,usability,efficien,efficiency,534,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions. One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:125,availability,slo,slower,125,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1346,availability,down,down,1346,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:645,deployability,log,log-transforming,645,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:649,integrability,transform,transforming,649,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1254,integrability,batch,batch,1254,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1315,integrability,event,eventually,1315,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:649,interoperability,transform,transforming,649,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:453,modifiability,pac,package,453,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:917,modifiability,pac,package,917,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1110,modifiability,variab,variable,1110,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1254,performance,batch,batch,1254,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:125,reliability,slo,slower,125,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:191,reliability,stabil,stability,191,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:537,reliability,poisson,poisson,537,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:728,reliability,doe,doesn,728,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:645,safety,log,log-transforming,645,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:776,safety,compl,complicated,776,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:849,safety,valid,validity,849,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:645,security,log,log-transforming,645,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:776,security,compl,complicated,776,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:645,testability,log,log-transforming,645,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1397,testability,simpl,simple,1397,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:594,usability,close,closed,594,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:743,usability,close,closed,743,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1141,usability,close,closed,1141,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:1397,usability,simpl,simple,1397,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:207,availability,avail,available,207,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:74,deployability,log,log,74,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:162,deployability,automat,automatically,162,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:94,integrability,batch,batchglm,94,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:94,performance,batch,batchglm,94,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:64,reliability,Poisson,Poisson,64,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:207,reliability,availab,available,207,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:74,safety,log,log,74,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:207,safety,avail,available,207,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:74,security,log,log,74,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:207,security,availab,available,207,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:74,testability,log,log,74,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:162,testability,automat,automatically,162,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:192,usability,close,closed,192,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/pull/869:383,availability,replic,replicated,383,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:562,availability,error,error,562,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:402,modifiability,exten,extent,402,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:562,performance,error,error,562,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:562,safety,error,error,562,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:106,security,modif,modifications,106,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:197,security,modif,modifications,197,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:512,testability,simpl,simplified,512,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:512,usability,simpl,simplified,512,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:562,usability,error,error,562,"Here is the summary:. * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications. * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`. * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`. * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/issues/870:88,deployability,Depend,Depending,88,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:146,deployability,contain,contain,146,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:88,integrability,Depend,Depending,88,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:88,modifiability,Depend,Depending,88,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:88,safety,Depend,Depending,88,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:88,testability,Depend,Depending,88,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:122,deployability,stack,stackoverflow,122,Hi! I hope you’ll enjoy using scanpy. Beginner questions like this are more suited for https://scanpy.discourse.group/ or stackoverflow so we still have some overview over this bug tracker.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/871:42,deployability,version,version,42,Can you check that you are using a recent version of scanpy? Older versions of scanpy may not have this parameter.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:67,deployability,version,versions,67,Can you check that you are using a recent version of scanpy? Older versions of scanpy may not have this parameter.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:42,integrability,version,version,42,Can you check that you are using a recent version of scanpy? Older versions of scanpy may not have this parameter.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:67,integrability,version,versions,67,Can you check that you are using a recent version of scanpy? Older versions of scanpy may not have this parameter.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:42,modifiability,version,version,42,Can you check that you are using a recent version of scanpy? Older versions of scanpy may not have this parameter.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:67,modifiability,version,versions,67,Can you check that you are using a recent version of scanpy? Older versions of scanpy may not have this parameter.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:104,modifiability,paramet,parameter,104,Can you check that you are using a recent version of scanpy? Older versions of scanpy may not have this parameter.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:29,deployability,upgrad,upgrade,29,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much. scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:85,deployability,depend,dependencies,85,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much. scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:85,integrability,depend,dependencies,85,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much. scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:29,modifiability,upgrad,upgrade,29,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much. scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:85,modifiability,depend,dependencies,85,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much. scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:85,safety,depend,dependencies,85,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much. scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:85,testability,depend,dependencies,85,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much. scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:62,deployability,version,version,62,"Well, 1.4.3 doesn’t have that parameter, as seen in the 1.4.3 version of that file: https://github.com/theislab/scanpy/blob/1.4.3/scanpy/preprocessing/_normalization.py#L118-L119",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:62,integrability,version,version,62,"Well, 1.4.3 doesn’t have that parameter, as seen in the 1.4.3 version of that file: https://github.com/theislab/scanpy/blob/1.4.3/scanpy/preprocessing/_normalization.py#L118-L119",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:30,modifiability,paramet,parameter,30,"Well, 1.4.3 doesn’t have that parameter, as seen in the 1.4.3 version of that file: https://github.com/theislab/scanpy/blob/1.4.3/scanpy/preprocessing/_normalization.py#L118-L119",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:62,modifiability,version,version,62,"Well, 1.4.3 doesn’t have that parameter, as seen in the 1.4.3 version of that file: https://github.com/theislab/scanpy/blob/1.4.3/scanpy/preprocessing/_normalization.py#L118-L119",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:12,reliability,doe,doesn,12,"Well, 1.4.3 doesn’t have that parameter, as seen in the 1.4.3 version of that file: https://github.com/theislab/scanpy/blob/1.4.3/scanpy/preprocessing/_normalization.py#L118-L119",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:5,deployability,depend,dependency,5,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:45,deployability,instal,installed,45,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:108,deployability,updat,update,108,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:5,integrability,depend,dependency,5,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:5,modifiability,depend,dependency,5,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:5,safety,depend,dependency,5,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:108,safety,updat,update,108,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:108,security,updat,update,108,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/871:5,testability,depend,dependency,5,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/872:89,deployability,automat,automated,89,It's a more central preprocessing method for selection PCs for further analysis. It's an automated alternative to just using the elbow of the PC variance explained plot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:89,testability,automat,automated,89,It's a more central preprocessing method for selection PCs for further analysis. It's an automated alternative to just using the elbow of the PC variance explained plot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:81,availability,cluster,clusters,81,"There is another related paper which focus on how to chose the number of PCs and clusters, I have tried it yet, but failed to tell whether it makes sense. https://academic.oup.com/gigascience/article/8/10/giz121/5579995",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:81,deployability,cluster,clusters,81,"There is another related paper which focus on how to chose the number of PCs and clusters, I have tried it yet, but failed to tell whether it makes sense. https://academic.oup.com/gigascience/article/8/10/giz121/5579995",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:116,deployability,fail,failed,116,"There is another related paper which focus on how to chose the number of PCs and clusters, I have tried it yet, but failed to tell whether it makes sense. https://academic.oup.com/gigascience/article/8/10/giz121/5579995",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:116,reliability,fail,failed,116,"There is another related paper which focus on how to chose the number of PCs and clusters, I have tried it yet, but failed to tell whether it makes sense. https://academic.oup.com/gigascience/article/8/10/giz121/5579995",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:32,performance,perform,performance,32,"This sounds interesting. If the performance is acceptable, it might make a good addition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:32,usability,perform,performance,32,"This sounds interesting. If the performance is acceptable, it might make a good addition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:119,safety,valid,validation,119,"It's more along the line of selecting number of PCs for denoising but MCV (https://github.com/czbiohub/molecular-cross-validation) is also interesting here. It helps with hyperparameter selection based on reconstruction loss on the hold out ""molecules"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:119,security,validat,validation,119,"It's more along the line of selecting number of PCs for denoising but MCV (https://github.com/czbiohub/molecular-cross-validation) is also interesting here. It helps with hyperparameter selection based on reconstruction loss on the hold out ""molecules"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:220,security,loss,loss,220,"It's more along the line of selecting number of PCs for denoising but MCV (https://github.com/czbiohub/molecular-cross-validation) is also interesting here. It helps with hyperparameter selection based on reconstruction loss on the hold out ""molecules"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:160,usability,help,helps,160,"It's more along the line of selecting number of PCs for denoising but MCV (https://github.com/czbiohub/molecular-cross-validation) is also interesting here. It helps with hyperparameter selection based on reconstruction loss on the hold out ""molecules"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:67,modifiability,variab,variable,67,"Yeah, I assumed it might be useful for other algorithms that use a variable number of PCs as input as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:93,safety,input,input,93,"Yeah, I assumed it might be useful for other algorithms that use a variable number of PCs as input as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:93,usability,input,input,93,"Yeah, I assumed it might be useful for other algorithms that use a variable number of PCs as input as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:221,availability,cluster,clustering,221,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:221,deployability,cluster,clustering,221,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:510,deployability,scale,scale,510,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:569,deployability,log,log,569,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:290,energy efficiency,load,loadings,290,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:510,energy efficiency,scale,scale,510,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:502,interoperability,share,share,502,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:281,modifiability,variab,variable,281,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:417,modifiability,variab,variable,417,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:510,modifiability,scal,scale,510,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:290,performance,load,loadings,290,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:510,performance,scale,scale,510,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:569,safety,log,log,569,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:569,security,log,log,569,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:569,testability,log,log,569,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:236,usability,visual,visualization,236,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:324,usability,help,helpful,324,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:132,availability,robust,robustly,132,I recently had a discussion with @LisaSikkema about how much you can overshoot here. The suggestion was made to use 100 PCs. Can we robustly compute that many or do the numerical methods break down when too little variance is represented by a PC? I recall @falexwolf mentioning this at some point.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:193,availability,down,down,193,I recently had a discussion with @LisaSikkema about how much you can overshoot here. The suggestion was made to use 100 PCs. Can we robustly compute that many or do the numerical methods break down when too little variance is represented by a PC? I recall @falexwolf mentioning this at some point.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:132,reliability,robust,robustly,132,I recently had a discussion with @LisaSikkema about how much you can overshoot here. The suggestion was made to use 100 PCs. Can we robustly compute that many or do the numerical methods break down when too little variance is represented by a PC? I recall @falexwolf mentioning this at some point.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:132,safety,robust,robustly,132,I recently had a discussion with @LisaSikkema about how much you can overshoot here. The suggestion was made to use 100 PCs. Can we robustly compute that many or do the numerical methods break down when too little variance is represented by a PC? I recall @falexwolf mentioning this at some point.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:54,availability,cluster,clustering,54,Another solution is to use Seurat to do jackstraw and clustering and then import into Scanpy for other analysis,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:54,deployability,cluster,clustering,54,Another solution is to use Seurat to do jackstraw and clustering and then import into Scanpy for other analysis,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:210,reliability,doe,doesen,210,"This is one way where Scanpy and Seurat diverge strikingly. Even the 'basic' Seurat tutorial uses JackStraw and has a whole section on ""Determine the ‘dimensionality’ of the dataset"", while the Scanpy tutorial doesen't mention it. Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:292,availability,cluster,clusters,292,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:292,deployability,cluster,clusters,292,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:461,energy efficiency,optim,optimal,461,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:205,integrability,batch,batch,205,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:568,interoperability,heterogen,heterogeneity,568,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:186,modifiability,variab,variability,186,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:205,performance,batch,batch,205,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:486,safety,avoid,avoid,486,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:274,security,ident,identification,274,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:448,security,ident,identify,448,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:111,availability,down,downstream,111,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:352,availability,down,downstream,352,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:703,availability,down,downstream,703,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:197,deployability,observ,observe,197,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:962,deployability,contain,contain,962,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1019,integrability,compon,component,1019,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1186,integrability,batch,batches,1186,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1285,integrability,batch,batch,1285,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1391,integrability,batch,batch,1391,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1435,integrability,compon,components,1435,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1019,interoperability,compon,component,1019,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1435,interoperability,compon,components,1435,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:317,modifiability,paramet,parameter,317,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:985,modifiability,variab,variability,985,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1019,modifiability,compon,component,1019,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1049,modifiability,variab,variability,1049,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1115,modifiability,variab,variability,1115,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1309,modifiability,variab,variability,1309,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1435,modifiability,compon,components,1435,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:341,performance,perform,performing,341,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1186,performance,batch,batches,1186,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1285,performance,batch,batch,1285,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1391,performance,batch,batch,1391,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:388,reliability,doe,does,388,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:832,reliability,doe,doesn,832,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:393,security,sign,significantly,393,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:197,testability,observ,observe,197,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:95,usability,user,users,95,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:266,usability,user,users,266,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:341,usability,perform,performing,341,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically. > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well? I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type? Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:75,deployability,contain,containing,75,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:209,deployability,SCALE,SCALE,209,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:581,deployability,build,building,581,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:931,deployability,integr,integration,931,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:209,energy efficiency,SCALE,SCALE,209,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:637,energy efficiency,load,loadings,637,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:86,integrability,batch,batch,86,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:925,integrability,batch,batch,925,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:931,integrability,integr,integration,931,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:1105,integrability,filter,filtering,1105,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:931,interoperability,integr,integration,931,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:209,modifiability,SCAL,SCALE,209,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:931,modifiability,integr,integration,931,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:86,performance,batch,batch,86,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:144,performance,time,times,144,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:209,performance,SCALE,SCALE,209,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:637,performance,load,loadings,637,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:925,performance,batch,batch,925,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:931,reliability,integr,integration,931,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:538,safety,input,input,538,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:931,security,integr,integration,931,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:931,testability,integr,integration,931,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:538,usability,input,input,538,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph? I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/873:652,deployability,Integr,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:130,integrability,batch,batch,130,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:198,integrability,batch,batch,198,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:306,integrability,batch,batch,306,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:652,integrability,Integr,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:980,integrability,batch,batch,980,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:652,interoperability,Integr,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:652,modifiability,Integr,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:36,performance,time,time,36,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:130,performance,batch,batch,130,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:198,performance,batch,batch,198,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:306,performance,batch,batch,306,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:980,performance,batch,batch,980,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:652,reliability,Integr,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:652,security,Integr,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:652,testability,Integr,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:863,usability,user,user-images,863,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:1041,usability,user,user-images,1041,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```. sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle. sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None). #OR. import mnnpy. mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata). ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. . ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch). ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,deployability,integr,integration,23,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,integrability,integr,integration,23,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:167,integrability,batch,batches,167,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:375,integrability,batch,batch,375,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,interoperability,integr,integration,23,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:356,interoperability,specif,specify,356,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,modifiability,integr,integration,23,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:167,performance,batch,batches,167,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:375,performance,batch,batch,375,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,reliability,integr,integration,23,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,security,integr,integration,23,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,testability,integr,integration,23,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:92,usability,document,documentation,92,"Hey! You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:. `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:122,integrability,batch,batch,122,"@LuckyMD Thanks for you suggestion. But I definitely used `sc.pp.pca()` first and then used `bbknn`. However, the UMAP of batch have no significant change. And now I am running the `mnnpy.mnn_correct()` as you suggested. Unfortunately, this code cost 12 hours to show ""step 2 of 3"" (step 1 took more than 2 hours). I don't know if it's normal, because I re-did `sc.pp.pca()` and got the UMAP, the wholw program of which only cost 30 minutes (including regress out cell cycle). ![微信截图_20191018084700](https://user-images.githubusercontent.com/49429496/67068198-e7131580-f1ab-11e9-962f-ddf2ebebac2c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:122,performance,batch,batch,122,"@LuckyMD Thanks for you suggestion. But I definitely used `sc.pp.pca()` first and then used `bbknn`. However, the UMAP of batch have no significant change. And now I am running the `mnnpy.mnn_correct()` as you suggested. Unfortunately, this code cost 12 hours to show ""step 2 of 3"" (step 1 took more than 2 hours). I don't know if it's normal, because I re-did `sc.pp.pca()` and got the UMAP, the wholw program of which only cost 30 minutes (including regress out cell cycle). ![微信截图_20191018084700](https://user-images.githubusercontent.com/49429496/67068198-e7131580-f1ab-11e9-962f-ddf2ebebac2c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:136,security,sign,significant,136,"@LuckyMD Thanks for you suggestion. But I definitely used `sc.pp.pca()` first and then used `bbknn`. However, the UMAP of batch have no significant change. And now I am running the `mnnpy.mnn_correct()` as you suggested. Unfortunately, this code cost 12 hours to show ""step 2 of 3"" (step 1 took more than 2 hours). I don't know if it's normal, because I re-did `sc.pp.pca()` and got the UMAP, the wholw program of which only cost 30 minutes (including regress out cell cycle). ![微信截图_20191018084700](https://user-images.githubusercontent.com/49429496/67068198-e7131580-f1ab-11e9-962f-ddf2ebebac2c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:452,testability,regress,regress,452,"@LuckyMD Thanks for you suggestion. But I definitely used `sc.pp.pca()` first and then used `bbknn`. However, the UMAP of batch have no significant change. And now I am running the `mnnpy.mnn_correct()` as you suggested. Unfortunately, this code cost 12 hours to show ""step 2 of 3"" (step 1 took more than 2 hours). I don't know if it's normal, because I re-did `sc.pp.pca()` and got the UMAP, the wholw program of which only cost 30 minutes (including regress out cell cycle). ![微信截图_20191018084700](https://user-images.githubusercontent.com/49429496/67068198-e7131580-f1ab-11e9-962f-ddf2ebebac2c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:508,usability,user,user-images,508,"@LuckyMD Thanks for you suggestion. But I definitely used `sc.pp.pca()` first and then used `bbknn`. However, the UMAP of batch have no significant change. And now I am running the `mnnpy.mnn_correct()` as you suggested. Unfortunately, this code cost 12 hours to show ""step 2 of 3"" (step 1 took more than 2 hours). I don't know if it's normal, because I re-did `sc.pp.pca()` and got the UMAP, the wholw program of which only cost 30 minutes (including regress out cell cycle). ![微信截图_20191018084700](https://user-images.githubusercontent.com/49429496/67068198-e7131580-f1ab-11e9-962f-ddf2ebebac2c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:80,integrability,batch,batch,80,"And for `combat`, the description claims that ""key: `str`, optional (default: `""batch""`)"", hence I didn't put the ""key"". I used `sc.pp.combat(adata, key=""batch"")`, and the result seems the same as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:154,integrability,batch,batch,154,"And for `combat`, the description claims that ""key: `str`, optional (default: `""batch""`)"", hence I didn't put the ""key"". I used `sc.pp.combat(adata, key=""batch"")`, and the result seems the same as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:80,performance,batch,batch,80,"And for `combat`, the description claims that ""key: `str`, optional (default: `""batch""`)"", hence I didn't put the ""key"". I used `sc.pp.combat(adata, key=""batch"")`, and the result seems the same as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:154,performance,batch,batch,154,"And for `combat`, the description claims that ""key: `str`, optional (default: `""batch""`)"", hence I didn't put the ""key"". I used `sc.pp.combat(adata, key=""batch"")`, and the result seems the same as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:45,deployability,version,version,45,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:145,deployability,integr,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:45,integrability,version,version,45,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:145,integrability,integr,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:206,integrability,batch,batch,206,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:145,interoperability,integr,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:45,modifiability,version,version,45,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:145,modifiability,integr,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:206,performance,batch,batch,206,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:4,reliability,doe,does,4,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:145,reliability,integr,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:145,security,integr,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:145,testability,integr,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,deployability,fail,fail,23,@LuckyMD I think I may fail in all possible methods~~. ![1111](https://user-images.githubusercontent.com/49429496/67175536-1f5b6380-f3f9-11e9-90db-897bd465c746.png). ![222](https://user-images.githubusercontent.com/49429496/67175551-2b472580-f3f9-11e9-823b-31f4f3dd4b4e.png). ![33333](https://user-images.githubusercontent.com/49429496/67175540-24b8ae00-f3f9-11e9-97e5-c90364a79946.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:23,reliability,fail,fail,23,@LuckyMD I think I may fail in all possible methods~~. ![1111](https://user-images.githubusercontent.com/49429496/67175536-1f5b6380-f3f9-11e9-90db-897bd465c746.png). ![222](https://user-images.githubusercontent.com/49429496/67175551-2b472580-f3f9-11e9-823b-31f4f3dd4b4e.png). ![33333](https://user-images.githubusercontent.com/49429496/67175540-24b8ae00-f3f9-11e9-97e5-c90364a79946.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:71,usability,user,user-images,71,@LuckyMD I think I may fail in all possible methods~~. ![1111](https://user-images.githubusercontent.com/49429496/67175536-1f5b6380-f3f9-11e9-90db-897bd465c746.png). ![222](https://user-images.githubusercontent.com/49429496/67175551-2b472580-f3f9-11e9-823b-31f4f3dd4b4e.png). ![33333](https://user-images.githubusercontent.com/49429496/67175540-24b8ae00-f3f9-11e9-97e5-c90364a79946.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:181,usability,user,user-images,181,@LuckyMD I think I may fail in all possible methods~~. ![1111](https://user-images.githubusercontent.com/49429496/67175536-1f5b6380-f3f9-11e9-90db-897bd465c746.png). ![222](https://user-images.githubusercontent.com/49429496/67175551-2b472580-f3f9-11e9-823b-31f4f3dd4b4e.png). ![33333](https://user-images.githubusercontent.com/49429496/67175540-24b8ae00-f3f9-11e9-97e5-c90364a79946.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:293,usability,user,user-images,293,@LuckyMD I think I may fail in all possible methods~~. ![1111](https://user-images.githubusercontent.com/49429496/67175536-1f5b6380-f3f9-11e9-90db-897bd465c746.png). ![222](https://user-images.githubusercontent.com/49429496/67175551-2b472580-f3f9-11e9-823b-31f4f3dd4b4e.png). ![33333](https://user-images.githubusercontent.com/49429496/67175540-24b8ae00-f3f9-11e9-97e5-c90364a79946.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:119,availability,error,error,119,"You just saw in your output line [7], that you get back a tuple from `mnn_correct()`. This is also what it says in the error you get. Thus, `adata[0]` is your anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:119,performance,error,error,119,"You just saw in your output line [7], that you get back a tuple from `mnn_correct()`. This is also what it says in the error you get. Thus, `adata[0]` is your anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:119,safety,error,error,119,"You just saw in your output line [7], that you get back a tuple from `mnn_correct()`. This is also what it says in the error you get. Thus, `adata[0]` is your anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:119,usability,error,error,119,"You just saw in your output line [7], that you get back a tuple from `mnn_correct()`. This is also what it says in the error you get. Thus, `adata[0]` is your anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:112,usability,close,close,112,I think this is a case for https://scanpy.discourse.group/ if there’s more problems. Also please respond to and close issues that are resolved for you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:54,usability,user,user-images,54,> @LuckyMD我想我可能在所有可能的方法上都失败了~~. > . > ![1111](https://user-images.githubusercontent.com/49429496/67175536-1f5b6380-f3f9-11e9-90db-897bd465c746.png). > ![222](https://user-images.githubusercontent.com/49429496/67175551-2b472580-f3f9-11e9-823b-31f4f3dd4b4e.png). > ![33333](https://user-images.githubusercontent.com/49429496/67175540-24b8ae00-f3f9-11e9-97e5-c90364a79946.png). Have you finally solved the problem? I have the same problem as you. Can we discuss about it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:166,usability,user,user-images,166,> @LuckyMD我想我可能在所有可能的方法上都失败了~~. > . > ![1111](https://user-images.githubusercontent.com/49429496/67175536-1f5b6380-f3f9-11e9-90db-897bd465c746.png). > ![222](https://user-images.githubusercontent.com/49429496/67175551-2b472580-f3f9-11e9-823b-31f4f3dd4b4e.png). > ![33333](https://user-images.githubusercontent.com/49429496/67175540-24b8ae00-f3f9-11e9-97e5-c90364a79946.png). Have you finally solved the problem? I have the same problem as you. Can we discuss about it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:280,usability,user,user-images,280,> @LuckyMD我想我可能在所有可能的方法上都失败了~~. > . > ![1111](https://user-images.githubusercontent.com/49429496/67175536-1f5b6380-f3f9-11e9-90db-897bd465c746.png). > ![222](https://user-images.githubusercontent.com/49429496/67175551-2b472580-f3f9-11e9-823b-31f4f3dd4b4e.png). > ![33333](https://user-images.githubusercontent.com/49429496/67175540-24b8ae00-f3f9-11e9-97e5-c90364a79946.png). Have you finally solved the problem? I have the same problem as you. Can we discuss about it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:863,integrability,batch,batch,863,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:889,integrability,batch,batches,889,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:955,integrability,batch,batch,955,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:964,integrability,batch,batches,964,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:982,integrability,batch,batch,982,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:1022,integrability,batch,batch,1022,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:863,performance,batch,batch,863,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:889,performance,batch,batches,889,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:955,performance,batch,batch,955,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:964,performance,batch,batches,964,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:982,performance,batch,batch,982,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:1022,performance,batch,batch,1022,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:206,usability,help,help,206,"> @Zifeng1995 Please read the comment above:. > . > > Thus, `adata[0]` is your anndata object. > . > you have do use `adata = adata[0]` to get your `AnnData` object from the `mnnpy` output. Thanks for your help! My problem was that no visible impact was found after `mnnpy` compared with `CCA` in seurat. I did `computeSumFactors` and `sc.pp.log1p` to normalize the data before mnnpy. After that I did pca and `sc.pp.neighbors`, the result was not OK. So I want to find out if I did something wrong. My codes are as follows. > adata.obs['size_factors'] = size_factors. > adata.X /= adata.obs['size_factors'].values[:,None]. > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample'). > var_select = adata.var.highly_variable_nbatches > 1. > var_genes = var_select.index[var_select]. > ## split per batch into new objects. > batches = ['TB','G10', 'G11', 'G16','G17']. > alldata = {}. > for batch in batches:. alldata[batch] = adata3[adata3.obs['sample'] == batch,]. > cdata =sc.external.pp.mnn_correct(alldata['TB'], alldata['G10'], alldata['G11'], alldata['G16'], alldata['G17'], svd_dim = 50,batch_key = 'sample', batch_categories = ['TB','G10','G11','G16','G17'],save_raw = True, var_subset = var_genes). > corr_data = cdata[0][:,var_genes]. > sc.tl.pca(corr_data, svd_solver = 'arpack', use_highly_variable = False). > sc.pp.neighbors(corr_data, n_pcs = 50, n_neighbors = 20). > sc.tl.umap(corr_data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:132,usability,document,documentation,132,This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:402,deployability,scale,scale,402,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:402,energy efficiency,scale,scale,402,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:402,modifiability,scal,scale,402,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:402,performance,scale,scale,402,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:134,usability,document,documentation,134,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:203,usability,document,documentation,203,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:264,usability,user,user-images,264,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:486,usability,help,help,486,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:509,usability,user,user-images,509,> This looks fine to me... I'm just not sure if you use `save_raw = True` what the output of the function is then. Check the function documentation `sc.external.pp.mnn_correct()`. I checked the function documentation. It didn't affect the output. ![image](https://user-images.githubusercontent.com/53402047/80896889-b9478580-8d25-11ea-8080-8c5ee851d184.png). Then I check the tutorial of mnnpy. It did scale after the mnn_correct. Did that affect the outcome? Thanks again for you kind help. ![image](https://user-images.githubusercontent.com/53402047/80896975-a5505380-8d26-11ea-9839-30cc408b7bcf.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:155,security,auth,authors,155,I'm afraid I can't really answer detailed questions on the functionality of `mnnpy` as I didn't write it. Maybe these questions would be best asked to the authors of the method? Why do you say it didn't work? Do you have an example output?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/874:16,deployability,instal,install,16,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help? This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:55,reliability,Doe,Does,55,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help? This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:101,usability,learn,learn,101,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help? This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:107,usability,help,help,107,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help? This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/875:230,deployability,api,api,230,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:230,integrability,api,api,230,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:230,interoperability,api,api,230,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:111,modifiability,paramet,parameters,111,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:314,modifiability,paramet,parameter,314,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:22,reliability,pra,praise,22,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:274,reliability,doe,doesn,274,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:335,testability,simpl,simplify,335,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:51,usability,document,documentation,51,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:100,usability,document,documented,100,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:335,usability,simpl,simplify,335,"Hi! thank you for the praise! I’m afraid this is a documentation bug! We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:147,reliability,doe,doesn,147,"> Soonish! awesome! > There's probably also a way […]. Yeah, the function should return an `Axes` object (at least if `show=False`, but idk why it doesn’t just always returns that)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/876:212,deployability,Instal,Installing,212,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:308,deployability,instal,installs,308,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:374,deployability,instal,installations,374,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:391,deployability,contain,container,391,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:449,deployability,resourc,resources,449,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:487,deployability,instal,installation,487,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:449,energy efficiency,resourc,resources,449,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:548,modifiability,pac,packaging,548,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:325,performance,parallel,parallel,325,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:449,performance,resourc,resources,449,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:19,reliability,doe,doesn,19,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:449,safety,resourc,resources,449,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:449,testability,resourc,resources,449,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:569,usability,guid,guides,569,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:109,modifiability,pac,package,109,"Thanks for the tip @flying-sheep. ;) I will consider this. . And regarding Bioconda, I was not aware how the package actually comes into bioconda. So the @BiocondaBot is not chekcing the requeiremts. Really strange.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:15,usability,tip,tip,15,"Thanks for the tip @flying-sheep. ;) I will consider this. . And regarding Bioconda, I was not aware how the package actually comes into bioconda. So the @BiocondaBot is not chekcing the requeiremts. Really strange.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:51,deployability,releas,release,51,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:114,deployability,version,version,114,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:136,deployability,build,build,136,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:243,deployability,depend,dependencies,243,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:415,deployability,updat,updates,415,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:423,deployability,depend,dependencies,423,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:440,deployability,build,build,440,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:292,energy efficiency,drain,draining,292,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:114,integrability,version,version,114,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:243,integrability,depend,dependencies,243,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:423,integrability,depend,dependencies,423,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:24,modifiability,pac,package,24,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:114,modifiability,version,version,114,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:243,modifiability,depend,dependencies,243,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:423,modifiability,depend,dependencies,423,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:363,performance,time,times,363,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:243,safety,depend,dependencies,243,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:415,safety,updat,updates,415,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:423,safety,depend,dependencies,423,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:415,security,updat,updates,415,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:243,testability,depend,dependencies,243,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:271,testability,understand,understandable,271,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:389,testability,simpl,simply,389,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:423,testability,depend,dependencies,423,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:389,usability,simpl,simply,389,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/882:148,deployability,pipelin,pipelines,148,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:241,energy efficiency,predict,predict,241,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:270,energy efficiency,adapt,adapt,270,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:148,integrability,pipelin,pipelines,148,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:270,integrability,adapt,adapt,270,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:270,interoperability,adapt,adapt,270,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:270,modifiability,adapt,adapt,270,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:548,performance,cach,cache,548,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:554,performance,cach,cache,554,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:241,safety,predict,predict,241,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:87,usability,support,support,87,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py. samples = []. for sample in range(1, 10):. s = read(. path / f'{sample}.matrix.mtx',. cache=cache,. cache_compression=cache_compression,. ).T. genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). s.var_names = genes[0]. s.var['gene_symbols'] = genes[1].values. s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. samples.append(s). adata = AnnData.concatenate(samples). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:88,energy efficiency,cool,cool,88,"Hi @flying-sheep , . Thanks, this is what I did myself, too. I just thought it would be cool to have the function to more general in case someone gets non standard data (especially for the people who are new to scanpy or python). . Anyways, love the package, great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:155,interoperability,standard,standard,155,"Hi @flying-sheep , . Thanks, this is what I did myself, too. I just thought it would be cool to have the function to more general in case someone gets non standard data (especially for the people who are new to scanpy or python). . Anyways, love the package, great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:250,modifiability,pac,package,250,"Hi @flying-sheep , . Thanks, this is what I did myself, too. I just thought it would be cool to have the function to more general in case someone gets non standard data (especially for the people who are new to scanpy or python). . Anyways, love the package, great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:139,interoperability,specif,specify,139,"Thanks for the praise! If there was a way to generalize this function, we could do it. As is, I don’t see any, other than letting the user specify the all three file names. Is that what you want?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:15,reliability,pra,praise,15,"Thanks for the praise! If there was a way to generalize this function, we could do it. As is, I don’t see any, other than letting the user specify the all three file names. Is that what you want?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:134,usability,user,user,134,"Thanks for the praise! If there was a way to generalize this function, we could do it. As is, I don’t see any, other than letting the user specify the all three file names. Is that what you want?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:284,energy efficiency,Load,Load,284,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````. def read(folder,mtx_file=None,features_file=None,...):. if mtx_file is not None:. # Load mtx file. else: . # Fall back to load from folder. ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:322,energy efficiency,load,load,322,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````. def read(folder,mtx_file=None,features_file=None,...):. if mtx_file is not None:. # Load mtx file. else: . # Fall back to load from folder. ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:163,interoperability,specif,specify,163,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````. def read(folder,mtx_file=None,features_file=None,...):. if mtx_file is not None:. # Load mtx file. else: . # Fall back to load from folder. ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:284,performance,Load,Load,284,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````. def read(folder,mtx_file=None,features_file=None,...):. if mtx_file is not None:. # Load mtx file. else: . # Fall back to load from folder. ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:322,performance,load,load,322,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````. def read(folder,mtx_file=None,features_file=None,...):. if mtx_file is not None:. # Load mtx file. else: . # Fall back to load from folder. ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:70,interoperability,specif,specify,70,Are there plans to incorporate this feature? Would be very helpful to specify the filenames to read in -- a lot of GEO data doesn't follow the expected format.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:152,interoperability,format,format,152,Are there plans to incorporate this feature? Would be very helpful to specify the filenames to read in -- a lot of GEO data doesn't follow the expected format.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:124,reliability,doe,doesn,124,Are there plans to incorporate this feature? Would be very helpful to specify the filenames to read in -- a lot of GEO data doesn't follow the expected format.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:10,testability,plan,plans,10,Are there plans to incorporate this feature? Would be very helpful to specify the filenames to read in -- a lot of GEO data doesn't follow the expected format.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:59,usability,help,helpful,59,Are there plans to incorporate this feature? Would be very helpful to specify the filenames to read in -- a lot of GEO data doesn't follow the expected format.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:21,availability,slo,slow,21,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:426,availability,reliab,reliably,426,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:270,deployability,pipelin,pipeline,270,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:673,deployability,pipelin,pipelines,673,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:766,energy efficiency,predict,predict,766,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:795,energy efficiency,adapt,adapt,795,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:270,integrability,pipelin,pipeline,270,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:673,integrability,pipelin,pipelines,673,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:795,integrability,adapt,adapt,795,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:795,interoperability,adapt,adapt,795,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:795,modifiability,adapt,adapt,795,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:1105,performance,cach,cache,1105,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:1111,performance,cach,cache,1111,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:21,reliability,slo,slow,21,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:285,reliability,doe,does,285,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:426,reliability,reliab,reliably,426,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:766,safety,predict,predict,766,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:386,usability,custom,custom,386,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:393,usability,workflow,workflows,393,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:463,usability,custom,custom,463,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:612,usability,support,support,612,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming? If yes, scanpy should indeed be able to deal with this;. If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. > . > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. > . > Took me 3 minutes:. > . > ```python. > samples = []. > for sample in range(1, 10):. > s = read(. > path / f'{sample}.matrix.mtx',. > cache=cache,. > cache_compression=cache_compression,. > ).T. > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'). > s.var_names = genes[0]. > s.var['gene_symbols'] = genes[1].values. > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]. > samples.append(s). > adata = AnnData.concatenate(samples). > ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:36,deployability,pipelin,pipeline,36,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:692,deployability,version,version,692,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:36,integrability,pipelin,pipeline,36,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:692,integrability,version,version,692,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:119,interoperability,compatib,compatible,119,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:578,interoperability,format,format,578,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:647,interoperability,share,share,647,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:682,modifiability,paramet,parameter,682,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:692,modifiability,version,version,692,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:51,reliability,doe,does,51,"> Is there a widely used processing pipeline which does not adhere to this file naming? STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```. barcodes.tsv.gz. features.tsv.gz. matrix.mtx.gz. UniqueAndMult-EM.mtx.gz. UniqueAndMult-PropUnique.mtx.gz. UniqueAndMult-Rescue.mtx.gz. UniqueAndMult-Uniform.mtx.gz. ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/883:426,availability,ERROR,ERROR,426,"Not sure if still required (the code in the anndata repo is close to this), but here is a minimal example:. ```py. import scanpy as sc. adata = sc.datasets.pbmc3k(). h5adfile = 'pbmc3k.h5ad'. adata.write(h5adfile). a1 = sc.read_h5ad(h5adfile). a2 = sc.read_h5ad(h5adfile, backed='r+'). sc.tl.score_genes(a1, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # OK. sc.tl.score_genes(a2, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # ERROR . ```. thanks..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:426,performance,ERROR,ERROR,426,"Not sure if still required (the code in the anndata repo is close to this), but here is a minimal example:. ```py. import scanpy as sc. adata = sc.datasets.pbmc3k(). h5adfile = 'pbmc3k.h5ad'. adata.write(h5adfile). a1 = sc.read_h5ad(h5adfile). a2 = sc.read_h5ad(h5adfile, backed='r+'). sc.tl.score_genes(a1, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # OK. sc.tl.score_genes(a2, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # ERROR . ```. thanks..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:426,safety,ERROR,ERROR,426,"Not sure if still required (the code in the anndata repo is close to this), but here is a minimal example:. ```py. import scanpy as sc. adata = sc.datasets.pbmc3k(). h5adfile = 'pbmc3k.h5ad'. adata.write(h5adfile). a1 = sc.read_h5ad(h5adfile). a2 = sc.read_h5ad(h5adfile, backed='r+'). sc.tl.score_genes(a1, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # OK. sc.tl.score_genes(a2, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # ERROR . ```. thanks..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:60,usability,close,close,60,"Not sure if still required (the code in the anndata repo is close to this), but here is a minimal example:. ```py. import scanpy as sc. adata = sc.datasets.pbmc3k(). h5adfile = 'pbmc3k.h5ad'. adata.write(h5adfile). a1 = sc.read_h5ad(h5adfile). a2 = sc.read_h5ad(h5adfile, backed='r+'). sc.tl.score_genes(a1, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # OK. sc.tl.score_genes(a2, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # ERROR . ```. thanks..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:90,usability,minim,minimal,90,"Not sure if still required (the code in the anndata repo is close to this), but here is a minimal example:. ```py. import scanpy as sc. adata = sc.datasets.pbmc3k(). h5adfile = 'pbmc3k.h5ad'. adata.write(h5adfile). a1 = sc.read_h5ad(h5adfile). a2 = sc.read_h5ad(h5adfile, backed='r+'). sc.tl.score_genes(a1, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # OK. sc.tl.score_genes(a2, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # ERROR . ```. thanks..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:426,usability,ERROR,ERROR,426,"Not sure if still required (the code in the anndata repo is close to this), but here is a minimal example:. ```py. import scanpy as sc. adata = sc.datasets.pbmc3k(). h5adfile = 'pbmc3k.h5ad'. adata.write(h5adfile). a1 = sc.read_h5ad(h5adfile). a2 = sc.read_h5ad(h5adfile, backed='r+'). sc.tl.score_genes(a1, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # OK. sc.tl.score_genes(a2, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']). # ERROR . ```. thanks..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/884:161,testability,trace,tracebacks,161,"Hi @a-munoz-rojas, please use [fenced code blocks](https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown) for code. The “language” for tracebacks is “pytb”: ```` ```pytb````. @ivirshup do you know what’s happening here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:59,usability,guid,guides,59,"Hi @a-munoz-rojas, please use [fenced code blocks](https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown) for code. The “language” for tracebacks is “pytb”: ```` ```pytb````. @ivirshup do you know what’s happening here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:10,safety,reme,remember,10,"I think I remember seeing this before, but I'm not sure if it had an issue opened. The problem was `varm` for `.raw` not being read as an `AxisArray` but just as a `dict`. This can be worked around with just not having `adata.raw.varm`, but it's fixed on master here: https://github.com/theislab/anndata/commit/a9b8b035980e505e43d09ba0edb6c47e8c37163d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:222,availability,restor,restore,222,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:397,integrability,transform,transformed,397,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:397,interoperability,transform,transformed,397,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:409,modifiability,variab,variables,409,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:222,reliability,restor,restore,222,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:72,testability,verif,verify,72,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:28,usability,tip,tips,28,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/885:124,availability,operat,operations,124,"please ask questions like this in https://scanpy.discourse.group/ or stackoverflow. answer: `in`, `not in`, `and`, `or` are operations that can only return a single boolean. You want the “bitwise or” operator instead of `or` and `ndarray.in1d` instead of `in`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/885
https://github.com/scverse/scanpy/issues/885:200,availability,operat,operator,200,"please ask questions like this in https://scanpy.discourse.group/ or stackoverflow. answer: `in`, `not in`, `and`, `or` are operations that can only return a single boolean. You want the “bitwise or” operator instead of `or` and `ndarray.in1d` instead of `in`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/885
https://github.com/scverse/scanpy/issues/885:69,deployability,stack,stackoverflow,69,"please ask questions like this in https://scanpy.discourse.group/ or stackoverflow. answer: `in`, `not in`, `and`, `or` are operations that can only return a single boolean. You want the “bitwise or” operator instead of `or` and `ndarray.in1d` instead of `in`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/885
https://github.com/scverse/scanpy/issues/887:99,safety,test,testing,99,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:159,safety,test,test,159,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:223,safety,test,test,223,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:253,safety,test,testing,253,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:364,safety,test,testing,364,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:99,testability,test,testing,99,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:159,testability,test,test,159,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:223,testability,test,test,223,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:253,testability,test,testing,253,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:364,testability,test,testing,364,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:0,usability,minim,minimal,0,"minimal reproducible example:. ```. import scanpy as sc . adata = sc.datasets.blobs() . adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} . adata.write('test.h5ad') . ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') . a2.uns . Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns . Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:8,deployability,version,versions,8,"oh, and versions:. ```. scanpy==1.4.3+116.g0075c62 . anndata==0.6.22.post2.dev80+g72c2bde . umap==0.3.9 . numpy==1.17.2 . scipy==1.3.0 . pandas==0.24.1 . scikit-learn==0.21.3 . statsmodels==0.10.0rc2 . ython-igraph==0.7.1 . louvain==0.6.1. ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:254,deployability,version,version,254,"oh, and versions:. ```. scanpy==1.4.3+116.g0075c62 . anndata==0.6.22.post2.dev80+g72c2bde . umap==0.3.9 . numpy==1.17.2 . scipy==1.3.0 . pandas==0.24.1 . scikit-learn==0.21.3 . statsmodels==0.10.0rc2 . ython-igraph==0.7.1 . louvain==0.6.1. ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:8,integrability,version,versions,8,"oh, and versions:. ```. scanpy==1.4.3+116.g0075c62 . anndata==0.6.22.post2.dev80+g72c2bde . umap==0.3.9 . numpy==1.17.2 . scipy==1.3.0 . pandas==0.24.1 . scikit-learn==0.21.3 . statsmodels==0.10.0rc2 . ython-igraph==0.7.1 . louvain==0.6.1. ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:254,integrability,version,version,254,"oh, and versions:. ```. scanpy==1.4.3+116.g0075c62 . anndata==0.6.22.post2.dev80+g72c2bde . umap==0.3.9 . numpy==1.17.2 . scipy==1.3.0 . pandas==0.24.1 . scikit-learn==0.21.3 . statsmodels==0.10.0rc2 . ython-igraph==0.7.1 . louvain==0.6.1. ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:8,modifiability,version,versions,8,"oh, and versions:. ```. scanpy==1.4.3+116.g0075c62 . anndata==0.6.22.post2.dev80+g72c2bde . umap==0.3.9 . numpy==1.17.2 . scipy==1.3.0 . pandas==0.24.1 . scikit-learn==0.21.3 . statsmodels==0.10.0rc2 . ython-igraph==0.7.1 . louvain==0.6.1. ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:254,modifiability,version,version,254,"oh, and versions:. ```. scanpy==1.4.3+116.g0075c62 . anndata==0.6.22.post2.dev80+g72c2bde . umap==0.3.9 . numpy==1.17.2 . scipy==1.3.0 . pandas==0.24.1 . scikit-learn==0.21.3 . statsmodels==0.10.0rc2 . ython-igraph==0.7.1 . louvain==0.6.1. ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:161,usability,learn,learn,161,"oh, and versions:. ```. scanpy==1.4.3+116.g0075c62 . anndata==0.6.22.post2.dev80+g72c2bde . umap==0.3.9 . numpy==1.17.2 . scipy==1.3.0 . pandas==0.24.1 . scikit-learn==0.21.3 . statsmodels==0.10.0rc2 . ython-igraph==0.7.1 . louvain==0.6.1. ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:5,reliability,doe,doesn,5,"h5py doesn't allow storing lists as i know, so they are converted to numpy arrays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:28,performance,time,time,28,Strangely this is the first time I've come across this. Is this new?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:66,modifiability,exten,extend,66,Okay... scvelo uses `adata.uns['velocity_settings']['embeddings'].extend()` which only works on lists. @VolkerBergen shall i report this again in the `scvelo` repo or is this sufficient for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:20,reliability,doe,doesn,20,"Thanks, Malte. h5py doesn't store as lists. Fixed now in scvelo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/888:123,interoperability,compatib,compatible,123,We also tried to address this in #572 and #624 but neither of those PRs have been merged since they're not fully backwards compatible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888
https://github.com/scverse/scanpy/issues/888:35,deployability,releas,release,35,"Well, we *really* need to get that release out of the door. But on the other hand, as @gokceneraslan said the current behavior is a bug, so we can change it now! Gökçen, do you still plan on consolidating those PRs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888
https://github.com/scverse/scanpy/issues/888:110,energy efficiency,current,current,110,"Well, we *really* need to get that release out of the door. But on the other hand, as @gokceneraslan said the current behavior is a bug, so we can change it now! Gökçen, do you still plan on consolidating those PRs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888
https://github.com/scverse/scanpy/issues/888:183,testability,plan,plan,183,"Well, we *really* need to get that release out of the door. But on the other hand, as @gokceneraslan said the current behavior is a bug, so we can change it now! Gökçen, do you still plan on consolidating those PRs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888
https://github.com/scverse/scanpy/issues/888:118,usability,behavi,behavior,118,"Well, we *really* need to get that release out of the door. But on the other hand, as @gokceneraslan said the current behavior is a bug, so we can change it now! Gökçen, do you still plan on consolidating those PRs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888
https://github.com/scverse/scanpy/issues/889:421,availability,Cluster,ClusterID,421,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:438,availability,Cluster,ClusterName,438,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1070,availability,error,errors,1070,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:45,deployability,fail,fails,45,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:214,deployability,continu,continuous,214,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:318,deployability,contain,contains,318,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:421,deployability,Cluster,ClusterID,421,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:438,deployability,Cluster,ClusterName,438,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:35,energy efficiency,current,currently,35,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:370,energy efficiency,current,currently,370,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:133,modifiability,variab,variable,133,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:225,modifiability,variab,variable,225,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:351,modifiability,variab,variables,351,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1070,performance,error,errors,1070,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:45,reliability,fail,fails,45,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1042,safety,avoid,avoid,1042,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1070,safety,error,errors,1070,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1070,usability,error,errors,1070,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects. ```pytb. data.obs.dtypes. ClusterID int32. ClusterName object. RNA_snn_res_0_5 object. nCount_RNA float32. nFeature_RNA int32. orig_ident object. percent_mt float32. seurat_clusters object. louvain category. dtype: object. ```. As a quick fix, I think you can do something like this:. ```python. adata = data.copy(). obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]. adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') . sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). ```. Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:0,reliability,doe,doesn,0,doesn't `sanitize_anndata()` do exactly that @flying-sheep? Could just add a call to that before exporting to SPRING?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/890:705,deployability,api,api,705,"I just tried and worked for me:. Default order:. ![image](https://user-images.githubusercontent.com/4964309/67553457-7d47be00-f70d-11e9-9381-b323cc25a739.png). Order changed: . ![image](https://user-images.githubusercontent.com/4964309/67553505-a0726d80-f70d-11e9-8a35-fe3ded5780ea.png). If this is not working for you, please check that the old order and the new order has the same elements as this may be the problem. FYI: The order of the cell type is given by the categorical order in `adata.obs['<column name>'] `. Thus, another way to change the order is to change the pandas categorical order was you want it. . See this link on how to do it: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.CategoricalIndex.reorder_categories.html.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:705,integrability,api,api,705,"I just tried and worked for me:. Default order:. ![image](https://user-images.githubusercontent.com/4964309/67553457-7d47be00-f70d-11e9-9381-b323cc25a739.png). Order changed: . ![image](https://user-images.githubusercontent.com/4964309/67553505-a0726d80-f70d-11e9-8a35-fe3ded5780ea.png). If this is not working for you, please check that the old order and the new order has the same elements as this may be the problem. FYI: The order of the cell type is given by the categorical order in `adata.obs['<column name>'] `. Thus, another way to change the order is to change the pandas categorical order was you want it. . See this link on how to do it: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.CategoricalIndex.reorder_categories.html.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:705,interoperability,api,api,705,"I just tried and worked for me:. Default order:. ![image](https://user-images.githubusercontent.com/4964309/67553457-7d47be00-f70d-11e9-9381-b323cc25a739.png). Order changed: . ![image](https://user-images.githubusercontent.com/4964309/67553505-a0726d80-f70d-11e9-8a35-fe3ded5780ea.png). If this is not working for you, please check that the old order and the new order has the same elements as this may be the problem. FYI: The order of the cell type is given by the categorical order in `adata.obs['<column name>'] `. Thus, another way to change the order is to change the pandas categorical order was you want it. . See this link on how to do it: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.CategoricalIndex.reorder_categories.html.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:66,usability,user,user-images,66,"I just tried and worked for me:. Default order:. ![image](https://user-images.githubusercontent.com/4964309/67553457-7d47be00-f70d-11e9-9381-b323cc25a739.png). Order changed: . ![image](https://user-images.githubusercontent.com/4964309/67553505-a0726d80-f70d-11e9-8a35-fe3ded5780ea.png). If this is not working for you, please check that the old order and the new order has the same elements as this may be the problem. FYI: The order of the cell type is given by the categorical order in `adata.obs['<column name>'] `. Thus, another way to change the order is to change the pandas categorical order was you want it. . See this link on how to do it: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.CategoricalIndex.reorder_categories.html.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:194,usability,user,user-images,194,"I just tried and worked for me:. Default order:. ![image](https://user-images.githubusercontent.com/4964309/67553457-7d47be00-f70d-11e9-9381-b323cc25a739.png). Order changed: . ![image](https://user-images.githubusercontent.com/4964309/67553505-a0726d80-f70d-11e9-8a35-fe3ded5780ea.png). If this is not working for you, please check that the old order and the new order has the same elements as this may be the problem. FYI: The order of the cell type is given by the categorical order in `adata.obs['<column name>'] `. Thus, another way to change the order is to change the pandas categorical order was you want it. . See this link on how to do it: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.CategoricalIndex.reorder_categories.html.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:469,interoperability,Prox,Proximal,469,"Thank you @fidelram! I guess for me it wasn't working because I wasn't using the 'swap_axes=True' command so maybe the function didn't know what I wanted it to reorder. However after using 'swap_axes' it started working. Now I still wanted the cell types to be along the y-axis so I tried your second suggestion for reordering the pandas categorical order like so:. `g0_g1_ctrl_ifn.obs['cluster_id'].cat.reorder_categories(['Glomerular Epithelial Cells', 'Early GEC', 'Proximal and Distal Tubule',. 'Endothelial Cells', 'Cycling Cells', 'Mesenchyme', 'Neuron'], inplace=True)`. And it worked! Thank you again for your very helpful suggestions! - Behram",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:98,usability,command,command,98,"Thank you @fidelram! I guess for me it wasn't working because I wasn't using the 'swap_axes=True' command so maybe the function didn't know what I wanted it to reorder. However after using 'swap_axes' it started working. Now I still wanted the cell types to be along the y-axis so I tried your second suggestion for reordering the pandas categorical order like so:. `g0_g1_ctrl_ifn.obs['cluster_id'].cat.reorder_categories(['Glomerular Epithelial Cells', 'Early GEC', 'Proximal and Distal Tubule',. 'Endothelial Cells', 'Cycling Cells', 'Mesenchyme', 'Neuron'], inplace=True)`. And it worked! Thank you again for your very helpful suggestions! - Behram",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:623,usability,help,helpful,623,"Thank you @fidelram! I guess for me it wasn't working because I wasn't using the 'swap_axes=True' command so maybe the function didn't know what I wanted it to reorder. However after using 'swap_axes' it started working. Now I still wanted the cell types to be along the y-axis so I tried your second suggestion for reordering the pandas categorical order like so:. `g0_g1_ctrl_ifn.obs['cluster_id'].cat.reorder_categories(['Glomerular Epithelial Cells', 'Early GEC', 'Proximal and Distal Tubule',. 'Endothelial Cells', 'Cycling Cells', 'Mesenchyme', 'Neuron'], inplace=True)`. And it worked! Thank you again for your very helpful suggestions! - Behram",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:23,deployability,updat,updating,23,"@MediciPrime I am also updating the underlying plotting code to fix the problem with `swap_axes=True`. After this is merged to master, you would not need to do the reordering of the categories.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:23,safety,updat,updating,23,"@MediciPrime I am also updating the underlying plotting code to fix the problem with `swap_axes=True`. After this is merged to master, you would not need to do the reordering of the categories.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:23,security,updat,updating,23,"@MediciPrime I am also updating the underlying plotting code to fix the problem with `swap_axes=True`. After this is merged to master, you would not need to do the reordering of the categories.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/891:65,reliability,doe,does,65,Wasn’t there recently a PR to introduce ordered categoricals? Or does that only affect legend order and not order of plotting?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:79,interoperability,specif,specific,79,"On similar issue: is there a way to plot the cells in random order? So that no specific group would be on top, but randomly mixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:25,usability,help,helpful,25,"Thank you, that could be helpful. Then I'd have to shuffle the order of the cells in the AnnData object first. Is there any easy way to do this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:17,integrability,coupl,couple,17,"I can think of a couple ways:. ```python. view = adata[np.random.choice(adata.n_obs, adata.n_obs, replace=False)]. new = sc.pp.subsample(adata, fraction=1., copy=True). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:127,integrability,sub,subsample,127,"I can think of a couple ways:. ```python. view = adata[np.random.choice(adata.n_obs, adata.n_obs, replace=False)]. new = sc.pp.subsample(adata, fraction=1., copy=True). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:17,modifiability,coupl,couple,17,"I can think of a couple ways:. ```python. view = adata[np.random.choice(adata.n_obs, adata.n_obs, replace=False)]. new = sc.pp.subsample(adata, fraction=1., copy=True). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:17,testability,coupl,couple,17,"I can think of a couple ways:. ```python. view = adata[np.random.choice(adata.n_obs, adata.n_obs, replace=False)]. new = sc.pp.subsample(adata, fraction=1., copy=True). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:40,integrability,sub,subsample,40,"Thank you! This works. ```. new = sc.pp.subsample(adata, fraction=1., copy=True). sc.pl.umap(new, color=['scsn'], legend_loc='on data', size=10, sort_order=False, legend_fontsize=15). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/pull/893:446,deployability,automat,automatically,446,"Great! Can’t say I understand the #890-related fix though: What went wrong before? Is there a test for it? Please use 4 space indentation, not visual indentation. Basically, running `black` on the any newly changed code should yield minimal changes. Feel free to remove a file where you changed a lot from here:. https://github.com/theislab/scanpy/blob/b3933ac185f9af3908261e939fc5df2336f1932e/pyproject.toml#L9-L13. Then everything will be done automatically. You’ll just have to go through the changes and fix ugly ones like black making `some = code[:] # comment` into `some = (\n code\n) # comment` instead of `#comment\nsome = code`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:94,safety,test,test,94,"Great! Can’t say I understand the #890-related fix though: What went wrong before? Is there a test for it? Please use 4 space indentation, not visual indentation. Basically, running `black` on the any newly changed code should yield minimal changes. Feel free to remove a file where you changed a lot from here:. https://github.com/theislab/scanpy/blob/b3933ac185f9af3908261e939fc5df2336f1932e/pyproject.toml#L9-L13. Then everything will be done automatically. You’ll just have to go through the changes and fix ugly ones like black making `some = code[:] # comment` into `some = (\n code\n) # comment` instead of `#comment\nsome = code`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:19,testability,understand,understand,19,"Great! Can’t say I understand the #890-related fix though: What went wrong before? Is there a test for it? Please use 4 space indentation, not visual indentation. Basically, running `black` on the any newly changed code should yield minimal changes. Feel free to remove a file where you changed a lot from here:. https://github.com/theislab/scanpy/blob/b3933ac185f9af3908261e939fc5df2336f1932e/pyproject.toml#L9-L13. Then everything will be done automatically. You’ll just have to go through the changes and fix ugly ones like black making `some = code[:] # comment` into `some = (\n code\n) # comment` instead of `#comment\nsome = code`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:94,testability,test,test,94,"Great! Can’t say I understand the #890-related fix though: What went wrong before? Is there a test for it? Please use 4 space indentation, not visual indentation. Basically, running `black` on the any newly changed code should yield minimal changes. Feel free to remove a file where you changed a lot from here:. https://github.com/theislab/scanpy/blob/b3933ac185f9af3908261e939fc5df2336f1932e/pyproject.toml#L9-L13. Then everything will be done automatically. You’ll just have to go through the changes and fix ugly ones like black making `some = code[:] # comment` into `some = (\n code\n) # comment` instead of `#comment\nsome = code`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:446,testability,automat,automatically,446,"Great! Can’t say I understand the #890-related fix though: What went wrong before? Is there a test for it? Please use 4 space indentation, not visual indentation. Basically, running `black` on the any newly changed code should yield minimal changes. Feel free to remove a file where you changed a lot from here:. https://github.com/theislab/scanpy/blob/b3933ac185f9af3908261e939fc5df2336f1932e/pyproject.toml#L9-L13. Then everything will be done automatically. You’ll just have to go through the changes and fix ugly ones like black making `some = code[:] # comment` into `some = (\n code\n) # comment` instead of `#comment\nsome = code`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:143,usability,visual,visual,143,"Great! Can’t say I understand the #890-related fix though: What went wrong before? Is there a test for it? Please use 4 space indentation, not visual indentation. Basically, running `black` on the any newly changed code should yield minimal changes. Feel free to remove a file where you changed a lot from here:. https://github.com/theislab/scanpy/blob/b3933ac185f9af3908261e939fc5df2336f1932e/pyproject.toml#L9-L13. Then everything will be done automatically. You’ll just have to go through the changes and fix ugly ones like black making `some = code[:] # comment` into `some = (\n code\n) # comment` instead of `#comment\nsome = code`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:233,usability,minim,minimal,233,"Great! Can’t say I understand the #890-related fix though: What went wrong before? Is there a test for it? Please use 4 space indentation, not visual indentation. Basically, running `black` on the any newly changed code should yield minimal changes. Feel free to remove a file where you changed a lot from here:. https://github.com/theislab/scanpy/blob/b3933ac185f9af3908261e939fc5df2336f1932e/pyproject.toml#L9-L13. Then everything will be done automatically. You’ll just have to go through the changes and fix ugly ones like black making `some = code[:] # comment` into `some = (\n code\n) # comment` instead of `#comment\nsome = code`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:72,modifiability,paramet,parameters,72,"@flying-sheep For #890 probably we had never tested that combination of parameters because the output was a broking image. . If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:45,safety,test,tested,45,"@flying-sheep For #890 probably we had never tested that combination of parameters because the output was a broking image. . If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:45,testability,test,tested,45,"@flying-sheep For #890 probably we had never tested that combination of parameters because the output was a broking image. . If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:130,testability,understand,understand,130,"@flying-sheep For #890 probably we had never tested that combination of parameters because the output was a broking image. . If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:256,usability,experien,experience,256,"@flying-sheep For #890 probably we had never tested that combination of parameters because the output was a broking image. . If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:62,deployability,continu,continuous,62,"One question related to #891, is there any plotting order for continuous values like higher expression plotted on top? That's more controversial but sometimes dropout et al might obscure cells expressing a gene. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:255,availability,cluster,cluster,255,"@gokceneraslan by default, higher values of the gene/obs column you are plotting are put on top. You can change this behaviour by setting `sort_order=False`. . Note: The default behaviour can be misleading and give you the impression that for example one cluster is expressing certain gene, when in reality only a small fraction of cells do express the gene, but because they are all plot on top, they seem to be more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:255,deployability,cluster,cluster,255,"@gokceneraslan by default, higher values of the gene/obs column you are plotting are put on top. You can change this behaviour by setting `sort_order=False`. . Note: The default behaviour can be misleading and give you the impression that for example one cluster is expressing certain gene, when in reality only a small fraction of cells do express the gene, but because they are all plot on top, they seem to be more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:117,usability,behavi,behaviour,117,"@gokceneraslan by default, higher values of the gene/obs column you are plotting are put on top. You can change this behaviour by setting `sort_order=False`. . Note: The default behaviour can be misleading and give you the impression that for example one cluster is expressing certain gene, when in reality only a small fraction of cells do express the gene, but because they are all plot on top, they seem to be more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:178,usability,behavi,behaviour,178,"@gokceneraslan by default, higher values of the gene/obs column you are plotting are put on top. You can change this behaviour by setting `sort_order=False`. . Note: The default behaviour can be misleading and give you the impression that for example one cluster is expressing certain gene, when in reality only a small fraction of cells do express the gene, but because they are all plot on top, they seem to be more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:51,modifiability,paramet,parameters,51,"> probably we had never tested that combination of parameters because the output was a broking image. got it! > If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience? AFAIK black can’t be applied to some lines only. Thus my suggestion to just run it on files where you changed a lot. [about pycharm](https://black.readthedocs.io/en/stable/editor_integration.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:24,safety,test,tested,24,"> probably we had never tested that combination of parameters because the output was a broking image. got it! > If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience? AFAIK black can’t be applied to some lines only. Thus my suggestion to just run it on files where you changed a lot. [about pycharm](https://black.readthedocs.io/en/stable/editor_integration.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:24,testability,test,tested,24,"> probably we had never tested that combination of parameters because the output was a broking image. got it! > If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience? AFAIK black can’t be applied to some lines only. Thus my suggestion to just run it on files where you changed a lot. [about pycharm](https://black.readthedocs.io/en/stable/editor_integration.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:117,testability,understand,understand,117,"> probably we had never tested that combination of parameters because the output was a broking image. got it! > If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience? AFAIK black can’t be applied to some lines only. Thus my suggestion to just run it on files where you changed a lot. [about pycharm](https://black.readthedocs.io/en/stable/editor_integration.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:243,usability,experien,experience,243,"> probably we had never tested that combination of parameters because the output was a broking image. got it! > If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience? AFAIK black can’t be applied to some lines only. Thus my suggestion to just run it on files where you changed a lot. [about pycharm](https://black.readthedocs.io/en/stable/editor_integration.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:125,energy efficiency,current,currently,125,"That’s a lot of files reformatted! I didn’t blacken ` scanpy/plotting/_anndata.py` yet because people were working on it. If currently nobody is working on it, we can go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/issues/895:242,deployability,api,api,242,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:246,deployability,log,logging,246,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:242,integrability,api,api,242,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:242,interoperability,api,api,242,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:116,modifiability,pac,packages,116,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:246,safety,log,logging,246,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:246,security,log,logging,246,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:246,testability,log,logging,246,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/pull/896:95,deployability,api,api-wrap,95,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:138,deployability,api,api-wrap,138,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:220,deployability,API,API,220,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:95,integrability,api,api-wrap,95,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:138,integrability,api,api-wrap,138,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:220,integrability,API,API,220,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:49,interoperability,compatib,compatibility,49,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:95,interoperability,api,api-wrap,95,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:138,interoperability,api,api-wrap,138,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:220,interoperability,API,API,220,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:242,modifiability,paramet,parameters,242,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:341,modifiability,paramet,parameter,341,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt. 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right). 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:7,usability,learn,learned,7,"Wow, I learned something new! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:134,deployability,automat,automatically,134,Thanks @flying-sheep ! I've also started using `black` in my own code now. You might be interested in this - I've added a git hook to automatically format any committed `.py` files with `black`. https://github.com/KrishnaswamyLab/scprep/blob/master/autoblack.sh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:148,interoperability,format,format,148,Thanks @flying-sheep ! I've also started using `black` in my own code now. You might be interested in this - I've added a git hook to automatically format any committed `.py` files with `black`. https://github.com/KrishnaswamyLab/scprep/blob/master/autoblack.sh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:126,testability,hook,hook,126,Thanks @flying-sheep ! I've also started using `black` in my own code now. You might be interested in this - I've added a git hook to automatically format any committed `.py` files with `black`. https://github.com/KrishnaswamyLab/scprep/blob/master/autoblack.sh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:134,testability,automat,automatically,134,Thanks @flying-sheep ! I've also started using `black` in my own code now. You might be interested in this - I've added a git hook to automatically format any committed `.py` files with `black`. https://github.com/KrishnaswamyLab/scprep/blob/master/autoblack.sh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/issues/897:61,integrability,pub,publications,61,"I think Arial was set as default as it's required for Nature publications. I set the font to arial for a manuscript manually, and then noticed that it's default already.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:140,deployability,stack,stack,140,> required for Nature publications. OK Nature is garbage now :laughing: (but seriously what are they thinking). Can we cheat and use a font stack that uses metrically compatible but actually palatable fonts before trying Arial?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:22,integrability,pub,publications,22,> required for Nature publications. OK Nature is garbage now :laughing: (but seriously what are they thinking). Can we cheat and use a font stack that uses metrically compatible but actually palatable fonts before trying Arial?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:167,interoperability,compatib,compatible,167,> required for Nature publications. OK Nature is garbage now :laughing: (but seriously what are they thinking). Can we cheat and use a font stack that uses metrically compatible but actually palatable fonts before trying Arial?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:13,integrability,topic,topic,13,"A little off topic, but if we want to put effort into setting a preferred font, do we also want to make it the default one people see in their plots?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:64,usability,prefer,preferred,64,"A little off topic, but if we want to put effort into setting a preferred font, do we also want to make it the default one people see in their plots?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:73,security,modif,modifies,73,"I don't think so, not unless you call `sc.set_figure_params()`. But this modifies the global config.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/pull/899:28,safety,test,test,28,"Thank you, judging from the test pics, those look like very useful changes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/899
https://github.com/scverse/scanpy/pull/899:28,testability,test,test,28,"Thank you, judging from the test pics, those look like very useful changes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/899
https://github.com/scverse/scanpy/issues/900:43,deployability,build,build,43,This seems to be a problem related to h5py build. You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:58,deployability,instal,install,58,This seems to be a problem related to h5py build. You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:164,deployability,manag,manager,164,This seems to be a problem related to h5py build. You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:164,energy efficiency,manag,manager,164,This seems to be a problem related to h5py build. You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:156,modifiability,pac,package,156,This seems to be a problem related to h5py build. You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:164,safety,manag,manager,164,This seems to be a problem related to h5py build. You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/pull/902:102,modifiability,paramet,parameter,102,"Thank you for your contribution! The function is already called `mitochondrial_genes`, so I think the parameter is a bit much to type. Why not just `chromosome_name` or even `chromosome`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/902
https://github.com/scverse/scanpy/pull/903:128,deployability,modul,module,128,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:435,deployability,depend,depending,435,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:435,integrability,depend,depending,435,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:128,modifiability,modul,module,128,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:142,modifiability,paramet,parameter,142,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:366,modifiability,layer,layers,366,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:435,modifiability,depend,depending,435,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:475,modifiability,paramet,parameter,475,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:128,safety,modul,module,128,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:435,safety,depend,depending,435,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:435,testability,depend,depending,435,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names. - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers. - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:27,interoperability,format,format,27,"Whoops, I had forgotten to format with BLACK so the checks didn't pass. Should be good now, @flying-sheep",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:77,interoperability,format,format,77,"Hi, sorry for the silence, I’m doing my PhD thesis right now. Can you please format it so it passes CI? `black .` should be sufficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:24,usability,help,help,24,Thanks so much for your help!! Good luck with your thesis :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/issues/904:82,usability,behavi,behaviour,82,"Just came across this, thanks for letting us know - hope you obtained the desired behaviour in the end :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/905:50,deployability,scale,scales,50,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:190,deployability,api,api,190,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:201,deployability,api,api,201,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:50,energy efficiency,scale,scales,50,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:190,integrability,api,api,190,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:201,integrability,api,api,201,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:190,interoperability,api,api,190,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:201,interoperability,api,api,201,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:50,modifiability,scal,scales,50,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:50,performance,scale,scales,50,"The recipe is correct, `sc.pp.normalize_per_cell` scales the UMI counts such that the sum per cell is the median of the total UMI counts (https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.api.pp.normalize_per_cell.html). The implementation could proceed differently than what is written on the paper but the goal is the same. . Or, did you spot a bug in the implementation? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:10,usability,document,documentation,10,"Maybe the documentation is not clear. ```. counts_per_cell_after : float or None, optional (default: None). If None, after normalization, each cell has a total count equal to the median of the counts_per_cell before normalization. ```. I thought that only ""counts_per_cell_after"" would multiply by the median, but the argument is:. ```. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). ```. not ""counts_per_cell_after"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:31,usability,clear,clear,31,"Maybe the documentation is not clear. ```. counts_per_cell_after : float or None, optional (default: None). If None, after normalization, each cell has a total count equal to the median of the counts_per_cell before normalization. ```. I thought that only ""counts_per_cell_after"" would multiply by the median, but the argument is:. ```. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). ```. not ""counts_per_cell_after"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:11,usability,document,documentation,11,"For me the documentation seems clear, but the example may be confusing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:31,usability,clear,clear,31,"For me the documentation seems clear, but the example may be confusing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/906:7,deployability,instal,installed,7,"Yes, I installed the development version of scanpy, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:33,deployability,version,version,33,"Yes, I installed the development version of scanpy, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:33,integrability,version,version,33,"Yes, I installed the development version of scanpy, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:33,modifiability,version,version,33,"Yes, I installed the development version of scanpy, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:240,integrability,compon,components,240,"> This is a duplicate of #891, I think, which is already fixed in master branch. It is fixed in sc.pl.umap(), but still exist in sc.pl.diffmap(), if not all of the groups are used, for example, sc.pl.diffmap(adata, color=['leiden'],size=20,components=['4,5'],groups=['0','1','2','3']), the rest groups 4, 5, 6 ....... will be gray and cover the points of group 0, 1, 2, 3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:240,interoperability,compon,components,240,"> This is a duplicate of #891, I think, which is already fixed in master branch. It is fixed in sc.pl.umap(), but still exist in sc.pl.diffmap(), if not all of the groups are used, for example, sc.pl.diffmap(adata, color=['leiden'],size=20,components=['4,5'],groups=['0','1','2','3']), the rest groups 4, 5, 6 ....... will be gray and cover the points of group 0, 1, 2, 3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:240,modifiability,compon,components,240,"> This is a duplicate of #891, I think, which is already fixed in master branch. It is fixed in sc.pl.umap(), but still exist in sc.pl.diffmap(), if not all of the groups are used, for example, sc.pl.diffmap(adata, color=['leiden'],size=20,components=['4,5'],groups=['0','1','2','3']), the rest groups 4, 5, 6 ....... will be gray and cover the points of group 0, 1, 2, 3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/911:86,deployability,api,api,86,See **Returns** section of the [`sc.tl.umap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.umap.html)’s docs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/911:86,integrability,api,api,86,See **Returns** section of the [`sc.tl.umap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.umap.html)’s docs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/911:86,interoperability,api,api,86,See **Returns** section of the [`sc.tl.umap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.umap.html)’s docs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/912:476,deployability,scale,scale,476,"Hi,. You could just set an `adata.obs` field with the exact value that you want to plot and then use the `sc.pl.tsne()` call with that field. For example:. ```. adata.obs['gene_AB'] = adata[:,'geneA'].X + adata[:,'geneB'].X. sc.pl.tsne(adata, color='gene_AB'). ```. However you should consider what you are actually trying to plot first. Maybe you don't just want to add the two values. Other alternatives are first scaling, then adding (to add z-scores). You may not want to scale your whole data, so that would require making a copy of `adata` to do this. Alternatively you can use `sc.tl.score_genes()` as well. That takes the sum of the expression values and subtracts the sum of 2 random genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:476,energy efficiency,scale,scale,476,"Hi,. You could just set an `adata.obs` field with the exact value that you want to plot and then use the `sc.pl.tsne()` call with that field. For example:. ```. adata.obs['gene_AB'] = adata[:,'geneA'].X + adata[:,'geneB'].X. sc.pl.tsne(adata, color='gene_AB'). ```. However you should consider what you are actually trying to plot first. Maybe you don't just want to add the two values. Other alternatives are first scaling, then adding (to add z-scores). You may not want to scale your whole data, so that would require making a copy of `adata` to do this. Alternatively you can use `sc.tl.score_genes()` as well. That takes the sum of the expression values and subtracts the sum of 2 random genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:663,integrability,sub,subtracts,663,"Hi,. You could just set an `adata.obs` field with the exact value that you want to plot and then use the `sc.pl.tsne()` call with that field. For example:. ```. adata.obs['gene_AB'] = adata[:,'geneA'].X + adata[:,'geneB'].X. sc.pl.tsne(adata, color='gene_AB'). ```. However you should consider what you are actually trying to plot first. Maybe you don't just want to add the two values. Other alternatives are first scaling, then adding (to add z-scores). You may not want to scale your whole data, so that would require making a copy of `adata` to do this. Alternatively you can use `sc.tl.score_genes()` as well. That takes the sum of the expression values and subtracts the sum of 2 random genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:416,modifiability,scal,scaling,416,"Hi,. You could just set an `adata.obs` field with the exact value that you want to plot and then use the `sc.pl.tsne()` call with that field. For example:. ```. adata.obs['gene_AB'] = adata[:,'geneA'].X + adata[:,'geneB'].X. sc.pl.tsne(adata, color='gene_AB'). ```. However you should consider what you are actually trying to plot first. Maybe you don't just want to add the two values. Other alternatives are first scaling, then adding (to add z-scores). You may not want to scale your whole data, so that would require making a copy of `adata` to do this. Alternatively you can use `sc.tl.score_genes()` as well. That takes the sum of the expression values and subtracts the sum of 2 random genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:476,modifiability,scal,scale,476,"Hi,. You could just set an `adata.obs` field with the exact value that you want to plot and then use the `sc.pl.tsne()` call with that field. For example:. ```. adata.obs['gene_AB'] = adata[:,'geneA'].X + adata[:,'geneB'].X. sc.pl.tsne(adata, color='gene_AB'). ```. However you should consider what you are actually trying to plot first. Maybe you don't just want to add the two values. Other alternatives are first scaling, then adding (to add z-scores). You may not want to scale your whole data, so that would require making a copy of `adata` to do this. Alternatively you can use `sc.tl.score_genes()` as well. That takes the sum of the expression values and subtracts the sum of 2 random genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:476,performance,scale,scale,476,"Hi,. You could just set an `adata.obs` field with the exact value that you want to plot and then use the `sc.pl.tsne()` call with that field. For example:. ```. adata.obs['gene_AB'] = adata[:,'geneA'].X + adata[:,'geneB'].X. sc.pl.tsne(adata, color='gene_AB'). ```. However you should consider what you are actually trying to plot first. Maybe you don't just want to add the two values. Other alternatives are first scaling, then adding (to add z-scores). You may not want to scale your whole data, so that would require making a copy of `adata` to do this. Alternatively you can use `sc.tl.score_genes()` as well. That takes the sum of the expression values and subtracts the sum of 2 random genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:80,usability,help,help,80,Thanks for the quick response. Sounds lie the sc.tl.score_genes() function will help me too figure things out too.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:80,deployability,contain,contains,80,"Hi there! I already have the average gene expression. I have two sets, each one contains 2 genes (example: gene_AB and gene_CD). I am trying to plot them together. However, it gave me two separate plots when I plotted. Is there a way to plot both in the same plot using sc.pl.velocity_embedding_stream or another function? ```. sc.pl.velocity_embedding_stream(adata, basis=""tsne"",color=['gene_AB','gene_CD'], linewidth=2, scale=2). ```. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:422,deployability,scale,scale,422,"Hi there! I already have the average gene expression. I have two sets, each one contains 2 genes (example: gene_AB and gene_CD). I am trying to plot them together. However, it gave me two separate plots when I plotted. Is there a way to plot both in the same plot using sc.pl.velocity_embedding_stream or another function? ```. sc.pl.velocity_embedding_stream(adata, basis=""tsne"",color=['gene_AB','gene_CD'], linewidth=2, scale=2). ```. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:422,energy efficiency,scale,scale,422,"Hi there! I already have the average gene expression. I have two sets, each one contains 2 genes (example: gene_AB and gene_CD). I am trying to plot them together. However, it gave me two separate plots when I plotted. Is there a way to plot both in the same plot using sc.pl.velocity_embedding_stream or another function? ```. sc.pl.velocity_embedding_stream(adata, basis=""tsne"",color=['gene_AB','gene_CD'], linewidth=2, scale=2). ```. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:422,modifiability,scal,scale,422,"Hi there! I already have the average gene expression. I have two sets, each one contains 2 genes (example: gene_AB and gene_CD). I am trying to plot them together. However, it gave me two separate plots when I plotted. Is there a way to plot both in the same plot using sc.pl.velocity_embedding_stream or another function? ```. sc.pl.velocity_embedding_stream(adata, basis=""tsne"",color=['gene_AB','gene_CD'], linewidth=2, scale=2). ```. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:422,performance,scale,scale,422,"Hi there! I already have the average gene expression. I have two sets, each one contains 2 genes (example: gene_AB and gene_CD). I am trying to plot them together. However, it gave me two separate plots when I plotted. Is there a way to plot both in the same plot using sc.pl.velocity_embedding_stream or another function? ```. sc.pl.velocity_embedding_stream(adata, basis=""tsne"",color=['gene_AB','gene_CD'], linewidth=2, scale=2). ```. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/913:92,deployability,api,api,92,What did you set [`sc.settings.n_jobs`](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy._settings.ScanpyConfig.n_jobs.html) to?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:92,integrability,api,api,92,What did you set [`sc.settings.n_jobs`](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy._settings.ScanpyConfig.n_jobs.html) to?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:92,interoperability,api,api,92,What did you set [`sc.settings.n_jobs`](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy._settings.ScanpyConfig.n_jobs.html) to?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:329,energy efficiency,cpu,cpu,329,"I actually had not set that attribute. But I just tested it now. ```. sc.settings.n_jobs = 15. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. OR. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. and in either case it only uses one cpu and takes the same amount of time as above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:329,performance,cpu,cpu,329,"I actually had not set that attribute. But I just tested it now. ```. sc.settings.n_jobs = 15. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. OR. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. and in either case it only uses one cpu and takes the same amount of time as above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:362,performance,time,time,362,"I actually had not set that attribute. But I just tested it now. ```. sc.settings.n_jobs = 15. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. OR. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. and in either case it only uses one cpu and takes the same amount of time as above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:50,safety,test,tested,50,"I actually had not set that attribute. But I just tested it now. ```. sc.settings.n_jobs = 15. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. OR. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. and in either case it only uses one cpu and takes the same amount of time as above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:50,testability,test,tested,50,"I actually had not set that attribute. But I just tested it now. ```. sc.settings.n_jobs = 15. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. OR. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. and in either case it only uses one cpu and takes the same amount of time as above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:56,performance,parallel,parallelization,56,Aren't umap 0.4 and latest pynndescent required for the parallelization support? 🤔,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:72,usability,support,support,72,Aren't umap 0.4 and latest pynndescent required for the parallelization support? 🤔,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:16,deployability,version,version,16,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:66,deployability,version,version,66,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:94,deployability,version,version,94,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:147,deployability,instal,install,147,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:240,deployability,instal,installed,240,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:16,integrability,version,version,16,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:66,integrability,version,version,66,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:94,integrability,version,version,94,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:16,modifiability,version,version,16,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:66,modifiability,version,version,66,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:94,modifiability,version,version,94,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:319,performance,parallel,parallelization,319,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:40,usability,learn,learn,40,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:377,usability,command,commands,377,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something? I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:155,availability,error,error,155,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:5,deployability,instal,install,5,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:33,deployability,instal,install,33,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:308,deployability,modul,module,308,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:308,modifiability,modul,module,308,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:364,modifiability,pac,packages,364,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:754,modifiability,pac,packages,754,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1106,modifiability,pac,packages,1106,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:155,performance,error,error,155,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:292,performance,time,timed,292,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:101,reliability,doe,doesn,101,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:155,safety,error,error,155,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:308,safety,modul,module,308,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:256,testability,Trace,Traceback,256,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:155,usability,error,error,155,"OK I install umap 0.4 . ```. pip install git+git://github.com/lmcinnes/umap@0.4dev. ```. However, it doesn't seem to run any faster and actually throws an error now. ```. sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11). ```. gives. ```. AttributeError Traceback (most recent call last). <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:21,deployability,instal,installed,21,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:548,integrability,transform,transformation,548,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:190,interoperability,specif,specifying,190,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:531,interoperability,specif,specified,531,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:548,interoperability,transform,transformation,548,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:431,modifiability,pac,packages,431,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:807,modifiability,pac,packages,807,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:512,performance,parallel,parallel,512,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:567,performance,parallel,parallel,567,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:632,performance,parallel,parallel,632,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:704,performance,parallel,parallel,704,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:858,performance,parallel,parallel,858,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:641,reliability,diagno,diagnostics,641,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:718,reliability,diagno,diagnostics,718,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:641,testability,diagno,diagnostics,641,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:718,testability,diagno,diagnostics,718,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:87,usability,learn,learn,87,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:153,usability,command,commands,153,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:699,usability,user,user,699,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:734,usability,help,help,734,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8. umap-learn 0.4.0. pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```. sc.settings.n_jobs = 15. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. gives the warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:190,deployability,instal,installed,190,"Ah I see. Sorry about the red herring with `sc.settings.n_jobs`, all those warnings seem to come directly from umap code. So according to @tomwhite, only umap 0.4 and pynndescent need to be installed and it should automatically be parallelized. Scanpy doesn’t effect this in any way. You should probably report this to the umap repo. Please write here once you openend an issue there. The only thing we can to is to call umap in a way that respects our settings once this is fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:214,deployability,automat,automatically,214,"Ah I see. Sorry about the red herring with `sc.settings.n_jobs`, all those warnings seem to come directly from umap code. So according to @tomwhite, only umap 0.4 and pynndescent need to be installed and it should automatically be parallelized. Scanpy doesn’t effect this in any way. You should probably report this to the umap repo. Please write here once you openend an issue there. The only thing we can to is to call umap in a way that respects our settings once this is fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:231,performance,parallel,parallelized,231,"Ah I see. Sorry about the red herring with `sc.settings.n_jobs`, all those warnings seem to come directly from umap code. So according to @tomwhite, only umap 0.4 and pynndescent need to be installed and it should automatically be parallelized. Scanpy doesn’t effect this in any way. You should probably report this to the umap repo. Please write here once you openend an issue there. The only thing we can to is to call umap in a way that respects our settings once this is fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:252,reliability,doe,doesn,252,"Ah I see. Sorry about the red herring with `sc.settings.n_jobs`, all those warnings seem to come directly from umap code. So according to @tomwhite, only umap 0.4 and pynndescent need to be installed and it should automatically be parallelized. Scanpy doesn’t effect this in any way. You should probably report this to the umap repo. Please write here once you openend an issue there. The only thing we can to is to call umap in a way that respects our settings once this is fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:214,testability,automat,automatically,214,"Ah I see. Sorry about the red herring with `sc.settings.n_jobs`, all those warnings seem to come directly from umap code. So according to @tomwhite, only umap 0.4 and pynndescent need to be installed and it should automatically be parallelized. Scanpy doesn’t effect this in any way. You should probably report this to the umap repo. Please write here once you openend an issue there. The only thing we can to is to call umap in a way that respects our settings once this is fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:342,availability,sli,slightly,342,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:161,deployability,instal,installation,161,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:212,energy efficiency,core,cores,212,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:24,performance,parallel,parallelization,24,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:351,performance,memor,memory,351,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:342,reliability,sli,slightly,342,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:351,usability,memor,memory,351,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:101,deployability,continu,continue,101,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:151,deployability,version,version,151,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:241,deployability,API,API,241,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:151,integrability,version,version,151,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:241,integrability,API,API,241,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:241,interoperability,API,API,241,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:151,modifiability,version,version,151,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:142,security,modif,modified,142,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:174,security,modif,modify,174,OK thank you! Well there’s nothing for us to do here as this would need editing umap’s code. We will continue to import umap and won’t ship a modified version so you need to modify your own copy of umap or wait for @lmcinnes to implement an API for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:140,deployability,instal,install,140,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:226,deployability,manag,manager,226,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:226,energy efficiency,manag,manager,226,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:37,performance,parallel,parallel,37,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:209,performance,parallel,parallel,209,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:226,safety,manag,manager,226,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:218,testability,context,context,218,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:31,reliability,doe,doesn,31,"@dylkot demonstrated that this doesn’t have much effect though, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:69,deployability,instal,installed,69,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:292,deployability,manag,manager,292,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:135,energy efficiency,CPU,CPUs,135,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:153,energy efficiency,core,core,153,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:292,energy efficiency,manag,manager,292,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:135,performance,CPU,CPUs,135,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:275,performance,parallel,parallel,275,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:292,safety,manag,manager,292,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:284,testability,context,context,284,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:36,deployability,manag,manager,36,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:36,energy efficiency,manag,manager,36,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:208,energy efficiency,optim,optimized,208,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:222,energy efficiency,core,cores,222,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:208,performance,optimiz,optimized,208,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:36,safety,manag,manager,36,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:324,security,sign,significant,324,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:28,testability,context,context,28,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:140,energy efficiency,core,cores,140,"@tomwhite, thanks for posting this script. The setup steps ultimately seem to work for me and to produce code that uses all of the allotted cores but it also increases the memory overhead dramatically such that I need >250 GB of RAM which doesn't seem worth it in this case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:172,performance,memor,memory,172,"@tomwhite, thanks for posting this script. The setup steps ultimately seem to work for me and to produce code that uses all of the allotted cores but it also increases the memory overhead dramatically such that I need >250 GB of RAM which doesn't seem worth it in this case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:179,performance,overhead,overhead,179,"@tomwhite, thanks for posting this script. The setup steps ultimately seem to work for me and to produce code that uses all of the allotted cores but it also increases the memory overhead dramatically such that I need >250 GB of RAM which doesn't seem worth it in this case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:239,reliability,doe,doesn,239,"@tomwhite, thanks for posting this script. The setup steps ultimately seem to work for me and to produce code that uses all of the allotted cores but it also increases the memory overhead dramatically such that I need >250 GB of RAM which doesn't seem worth it in this case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:172,usability,memor,memory,172,"@tomwhite, thanks for posting this script. The setup steps ultimately seem to work for me and to produce code that uses all of the allotted cores but it also increases the memory overhead dramatically such that I need >250 GB of RAM which doesn't seem worth it in this case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/914:192,reliability,doe,doesn,192,"The `adata.var_names` that I have are the unique barcodes that the DropSeq gives each cell. I originally assumed that was right, should I have the gene names under there instead? Even so that doesn't explain why I would get numbers as the barcodes are nucleotide and read as such. . ```. >>> adata.var_names. Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GTGATTCATATC', 'GCCGTCGTCACG', 'AGGAGGGTTACT', 'AGAACGATTAAT',. 'ACGACTATCGTC', 'TGCCAGCCTGAA', 'TTAAACTCGGCT', 'GCAGACACCGGT',. 'GGGGCTCTTGGG', 'GCGACGACCTTG'],. dtype='object', length=3971). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:232,deployability,version,version,232,"So, in general `var_names` should be the gene ids and `obs_names` should be the cell ids. I'm interested in how you could have gone from any sort of string label to integers before. If you wouldn't mind, could you run this modified version of your previous example and showing the output? ```python. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. print(day00a.var_names). print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. print(adata.var_names, adata). sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:232,integrability,version,version,232,"So, in general `var_names` should be the gene ids and `obs_names` should be the cell ids. I'm interested in how you could have gone from any sort of string label to integers before. If you wouldn't mind, could you run this modified version of your previous example and showing the output? ```python. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. print(day00a.var_names). print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. print(adata.var_names, adata). sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:232,modifiability,version,version,232,"So, in general `var_names` should be the gene ids and `obs_names` should be the cell ids. I'm interested in how you could have gone from any sort of string label to integers before. If you wouldn't mind, could you run this modified version of your previous example and showing the output? ```python. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. print(day00a.var_names). print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. print(adata.var_names, adata). sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:223,security,modif,modified,223,"So, in general `var_names` should be the gene ids and `obs_names` should be the cell ids. I'm interested in how you could have gone from any sort of string label to integers before. If you wouldn't mind, could you run this modified version of your previous example and showing the output? ```python. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. print(day00a.var_names). print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. print(adata.var_names, adata). sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1252,availability,error,errors,1252,"'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1423,availability,error,errors,1423,"TTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',. 'CTGCATCTGCTG', 'GGGACTAATAGC', 'AAACATATTGAA', 'AAGGCCTAACTG',. 'CGACAACGTCCA', 'ATCCTGACTGAC'],. dtype='object', length=500). >>> print(day04.var_names). Index(['ACGCA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:654,deployability,modul,module,654,"@ivirshup . I ran the code as you requested, the following output is what I got for the print calls. ```. >>> print(day00a.var_names). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1205,integrability,batch,batch,1205,"CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:654,modifiability,modul,module,654,"@ivirshup . I ran the code as you requested, the following output is what I got for the print calls. ```. >>> print(day00a.var_names). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1205,performance,batch,batch,1205,"CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1252,performance,error,errors,1252,"'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1423,performance,error,errors,1423,"TTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',. 'CTGCATCTGCTG', 'GGGACTAATAGC', 'AAACATATTGAA', 'AAGGCCTAACTG',. 'CGACAACGTCCA', 'ATCCTGACTGAC'],. dtype='object', length=500). >>> print(day04.var_names). Index(['ACGCA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:654,safety,modul,module,654,"@ivirshup . I ran the code as you requested, the following output is what I got for the print calls. ```. >>> print(day00a.var_names). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1252,safety,error,errors,1252,"'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1423,safety,error,errors,1423,"TTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',. 'CTGCATCTGCTG', 'GGGACTAATAGC', 'AAACATATTGAA', 'AAGGCCTAACTG',. 'CGACAACGTCCA', 'ATCCTGACTGAC'],. dtype='object', length=500). >>> print(day04.var_names). Index(['ACGCA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:590,testability,Trace,Traceback,590,"@ivirshup . I ran the code as you requested, the following output is what I got for the print calls. ```. >>> print(day00a.var_names). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1252,usability,error,errors,1252,"'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',. 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1423,usability,error,errors,1423,"TTCTGGTTT',. 'ACTATGGCTGGT', 'CGTGAACCCTGT'],. dtype='object', length=1500). >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""<stdin>"", line 1, in <genexpr>. NameError: name 'adata00a' is not defined. >>> print(adata.var_names, adata). Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',. 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',. 'CGCGCCTTGTCA', 'AACCTTTGATGG',. ... 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',. 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',. 'GACATAAATCAG', 'AGGGGTGACGAC'],. dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 . obs: 'batch', 'tech'. ```. The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```. >>> print(day01.var_names). Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',. 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',. 'AGCCCGCCCAGN', 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',. 'CTGCATCTGCTG', 'GGGACTAATAGC', 'AAACATATTGAA', 'AAGGCCTAACTG',. 'CGACAACGTCCA', 'ATCCTGACTGAC'],. dtype='object', length=500). >>> print(day04.var_names). Index(['ACGCA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:3680,usability,user,user-images,3680,", 'GAAAATCGATCN',. ... 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',. 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',. 'CTAAACGGCTGT', 'GAAATGAGGATG'],. dtype='object', length=500). >>> print(day02.var_names). Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',. 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',. 'GCATTATGTCCC', 'TTCGGTGTCATG',. ... 'AGCAGCGTTATA', 'ATAGTGGGCGAG', 'TTTCCCCCCGTC', 'TTCGGTCTCATG',. 'CTGCATCTGCTG', 'GGGACTAATAGC', 'AAACATATTGAA', 'AAGGCCTAACTG',. 'CGACAACGTCCA', 'ATCCTGACTGAC'],. dtype='object', length=500). >>> print(day04.var_names). Index(['ACGCAAGTACAA', 'TCTCCCACACAG', 'TCGGAATGCGCC', 'CTTGGCCAACTT',. 'ACTTTTAATTCC', 'CGACCGATCCCC', 'GCCTGAGACGAN', 'CACCCCGGTCAG',. 'AACTGGCTTGAN', 'TCGTATAACTAN',. ... 'CAACTTCCTATT', 'AAAGTAGTCCGC', 'TTGTGACGAGAA', 'GCCATGACCTGC',. 'GCCGAAATCTCC', 'GGTGCCACTCTT', 'CTAGTCTCAGCC', 'CACACCATGCAC',. 'CCCACTTCTCCT', 'AATTAGATCTCA'],. dtype='object', length=500). >>> print(day09.var_names). Index(['ATCCCGCCGCGA', 'CTGGCATCCTCN', 'CATACAAATGTG', 'AATACCAGTATC',. 'TGGAACTATCGC', 'TTGGAGGATAGN', 'CGCCTCCCTTAN', 'TCAATCCTGCGN',. 'ACGAGATAATTC', 'ATTATGCAACGN',. ... 'CAGGGCTTCTGG', 'ATCTTTTACTTC', 'TACATCCGACAA', 'CGCTACTTAGTA',. 'TGGAGGATAGTG', 'TCACCCTATGCG', 'CTTGTGGGCATT', 'GCCTCTACTAAG',. 'TGGAGGATAGGG', 'AATCCGCCATCC'],. dtype='object', length=494). >>> print(day11.var_names). Index(['GTGAAATACGTN', 'TCAATTGACCTN', 'ACAACCATAACT', 'TTTCAGCTGTAG',. 'CTCTGAATGAAA', 'AGCCAAGTTTCT', 'ATGACTTACGGN', 'CACGCGTGCGGN',. 'CCACCCTTCCAT', 'CGGCTCTTTCCN',. ... 'CCCGATCTCCAT', 'CATAGAATTGAC', 'CCCGCTCATTGG', 'ATCGATCGGAAT',. 'GGGGATGCTATC', 'TCAATCGCCCCA', 'GTCGACAATCAA', 'ATTCATCGACCC',. 'GAGGTGTCTCAT', 'GGGTATGCTATA'],. dtype='object', length=500). ```. As for the graph it produced:. <img width=""979"" alt=""Screen Shot 2019-11-14 at 11 07 07 AM"" src=""https://user-images.githubusercontent.com/9083834/68879435-03cd3b00-06cf-11ea-903e-c59f7da461b8.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:569,availability,state,statements,569,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:275,deployability,observ,observation,275,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:322,deployability,observ,observations,322,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:569,integrability,state,statements,569,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:233,interoperability,share,shared,233,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:202,modifiability,variab,variables,202,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:536,modifiability,variab,variables,536,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:525,security,modif,modify,525,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:275,testability,observ,observation,275,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:322,testability,observ,observations,322,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes? I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:507,availability,error,errors,507,"> @ivirshup @LuckyMD. > I fixed the problem - the issue was in the original h5ad file converted from a Seurat object using SeuratDisk::Convert(). It seems the var data wasn't ported over properly for the assay I was using. I rebuilt the h5ad file using reticulate instead and that solved the problem. I have encountered the same issue as @mosquitoCat . Although adata.var_names still returns correct gene symbols, all my name IDs become numbers:. for example, sc.pl.umap(adata,color='GeneName') will return errors. but sc.pl.umap(adata,color='123') can be recognized. SeuratDisk::Convert() seems to cause some trouble here. Is there a way to fix it? @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:507,performance,error,errors,507,"> @ivirshup @LuckyMD. > I fixed the problem - the issue was in the original h5ad file converted from a Seurat object using SeuratDisk::Convert(). It seems the var data wasn't ported over properly for the assay I was using. I rebuilt the h5ad file using reticulate instead and that solved the problem. I have encountered the same issue as @mosquitoCat . Although adata.var_names still returns correct gene symbols, all my name IDs become numbers:. for example, sc.pl.umap(adata,color='GeneName') will return errors. but sc.pl.umap(adata,color='123') can be recognized. SeuratDisk::Convert() seems to cause some trouble here. Is there a way to fix it? @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:507,safety,error,errors,507,"> @ivirshup @LuckyMD. > I fixed the problem - the issue was in the original h5ad file converted from a Seurat object using SeuratDisk::Convert(). It seems the var data wasn't ported over properly for the assay I was using. I rebuilt the h5ad file using reticulate instead and that solved the problem. I have encountered the same issue as @mosquitoCat . Although adata.var_names still returns correct gene symbols, all my name IDs become numbers:. for example, sc.pl.umap(adata,color='GeneName') will return errors. but sc.pl.umap(adata,color='123') can be recognized. SeuratDisk::Convert() seems to cause some trouble here. Is there a way to fix it? @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:507,usability,error,errors,507,"> @ivirshup @LuckyMD. > I fixed the problem - the issue was in the original h5ad file converted from a Seurat object using SeuratDisk::Convert(). It seems the var data wasn't ported over properly for the assay I was using. I rebuilt the h5ad file using reticulate instead and that solved the problem. I have encountered the same issue as @mosquitoCat . Although adata.var_names still returns correct gene symbols, all my name IDs become numbers:. for example, sc.pl.umap(adata,color='GeneName') will return errors. but sc.pl.umap(adata,color='123') can be recognized. SeuratDisk::Convert() seems to cause some trouble here. Is there a way to fix it? @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:64,interoperability,convers,conversion,64,"@Imevernoteva I had the issue 3 years ago and have checked that conversion from Seurat object to h5ad works well with the latest SeuratDisk package, with var data ported correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:140,modifiability,pac,package,140,"@Imevernoteva I had the issue 3 years ago and have checked that conversion from Seurat object to h5ad works well with the latest SeuratDisk package, with var data ported correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:66,interoperability,convers,conversion,66,"> @Imevernoteva I had the issue 3 years ago and have checked that conversion from Seurat object to h5ad works well with the latest SeuratDisk package, with var data ported correctly. Thanks for the info. I find that it works well with scRNA-seq data, but causes issues with Spatial (Visium) data. Would appreciate any tips! Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:142,modifiability,pac,package,142,"> @Imevernoteva I had the issue 3 years ago and have checked that conversion from Seurat object to h5ad works well with the latest SeuratDisk package, with var data ported correctly. Thanks for the info. I find that it works well with scRNA-seq data, but causes issues with Spatial (Visium) data. Would appreciate any tips! Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:318,usability,tip,tips,318,"> @Imevernoteva I had the issue 3 years ago and have checked that conversion from Seurat object to h5ad works well with the latest SeuratDisk package, with var data ported correctly. Thanks for the info. I find that it works well with scRNA-seq data, but causes issues with Spatial (Visium) data. Would appreciate any tips! Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/pull/915:83,reliability,doe,does,83,The confusion matrix metric looks very similar to what `sc.marker_genes_overlap()` does. It might be worth moving that function over as well and potentially merging the functions. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:159,deployability,log,logcounts,159,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:309,deployability,API,API,309,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:344,energy efficiency,current,current,344,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:309,integrability,API,API,309,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:309,interoperability,API,API,309,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:152,modifiability,layer,layer,152,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:159,safety,log,logcounts,159,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:159,security,log,logcounts,159,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:159,testability,log,logcounts,159,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:80,usability,tool,tools,80,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:361,usability,tool,tools,361,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object? ```py. sc.tl.gearys_c(pbmc, layer=""logcounts""). to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]. ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:959,energy efficiency,measur,measure,959,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:401,interoperability,specif,specific,401,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:466,interoperability,specif,specific,466,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1444,interoperability,specif,specify,1444,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:556,reliability,doe,doesn,556,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1669,safety,compl,complexity,1669,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:682,security,modif,modify,682,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:765,security,modif,modifies,765,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1032,security,modif,modifying,1032,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1111,security,modif,modify,1111,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1669,security,compl,complexity,1669,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:93,testability,plan,planning,93,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:112,usability,support,support,112,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1134,usability,help,helpful,1134,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1432,usability,user,user,1432,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1497,usability,user,user,1497,"@LuckyMD. There's definitely some similarities, especially for the `overlap_count` case. I'm planning on adding support for categories encoded as boolean sparse matrices, which would make the calculation even more similar. I don't think they have enough of a common use case to be merged however, especially since calculation is such a small part of both functions. `sc.tl.marker_genes_overlap` seems specific to differential expression, while `confusion_matrix` is specific to comparing sets of labels. @flying-sheep. Why `sc.metrics` and not `sc.tl`? It doesn't feel like it fits. My expectation for most things in `sc.tl` and `sc.pp` is that they will take an anndata object and modify it. One of these functions takes an anndata object (optionally) and neither modifies it. I would agree with @LuckyMD that `sc.tl.marker_gene_overlap` could also fit in `sc.metrics`. There are possibly more philosophical points to be made about the difference between a ""measure"" and a ""representation"" but I'd have to think on that. As to not modifying the AnnData object, It's not obvious to me how these functions would modify the object in a helpful way. I don't expect the outputs of these functions to have a similar shape to the AnnData object particularly often. Admittedly, the example I included has that, but I'm not sure if that'll be a common use case. You could whack it in `.uns`, but then there's either some default key or the user has to specify one. It'd be pretty straight forward for the user to do `adata.uns[""some key""] = sc.metrics.some_func`. I guess I'd summarize this as: I don't see added value from adding the results to the object – but I do see more complexity in implementing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:112,modifiability,extens,extensions,112,> get multipledispatch support into scanpydoc. Would you mind handling that? I have zero experience with sphinx extensions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:23,usability,support,support,23,> get multipledispatch support into scanpydoc. Would you mind handling that? I have zero experience with sphinx extensions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:89,usability,experien,experience,89,> get multipledispatch support into scanpydoc. Would you mind handling that? I have zero experience with sphinx extensions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:9,deployability,api,api,9,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:334,deployability,depend,depends,334,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:9,integrability,api,api,9,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:334,integrability,depend,depends,334,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:9,interoperability,api,api,9,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:127,modifiability,variab,variable,127,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:258,modifiability,variab,variable,258,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:334,modifiability,depend,depends,334,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1018,modifiability,pac,packages,1018,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:334,safety,depend,depends,334,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:374,safety,input,input,374,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:414,safety,input,input,414,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:173,security,sign,signature,173,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:243,security,access,access,243,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:334,testability,depend,depends,334,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:374,usability,input,input,374,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:389,usability,intuit,intuitive,389,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:414,usability,input,input,414,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed? Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python. sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""). sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]). ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:108,availability,redund,redundancy,108,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:108,deployability,redundan,redundancy,108,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:96,energy efficiency,reduc,reduce,96,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:108,reliability,redundan,redundancy,108,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:108,safety,redund,redundancy,108,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:4,deployability,updat,updates,4,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package? Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:205,deployability,modul,module,205,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package? Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:152,modifiability,pac,package,152,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package? Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:205,modifiability,modul,module,205,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package? Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:4,safety,updat,updates,4,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package? Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:205,safety,modul,module,205,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package? Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:4,security,updat,updates,4,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package? Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:199,deployability,modul,module,199,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:245,deployability,releas,release,245,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:401,deployability,version,version,401,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:401,integrability,version,version,401,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:199,modifiability,modul,module,199,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:401,modifiability,version,version,401,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:199,safety,modul,module,199,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:366,safety,test,testing,366,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:366,testability,test,testing,366,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:75,deployability,integr,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:75,integrability,integr,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:75,interoperability,integr,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:75,modifiability,integr,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:75,reliability,integr,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:75,security,integr,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:129,security,access,access,129,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:75,testability,integr,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:456,availability,mainten,maintenance,456,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:569,deployability,integr,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:171,energy efficiency,current,current,171,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:569,integrability,integr,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:569,interoperability,integr,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:214,modifiability,pac,package,214,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:351,modifiability,pac,package,351,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:569,modifiability,integr,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:456,reliability,mainten,maintenance,456,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:569,reliability,integr,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:369,security,assess,assesses,369,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:569,security,integr,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:179,testability,plan,plan,179,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:569,testability,integr,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:202,usability,usab,usable,202,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:329,usability,usab,usability,329,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:378,usability,usab,usability,378,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:407,usability,prefer,prefer,407,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:205,interoperability,distribut,distribute,205,"@LuckyMD, I've actually been meaning to ask you about this. I'd like to include at least some of those metrics here. In some cases it mean making numba implementations (which would ultimately be easier to distribute). There would definitely be a prioritization for general usefulness of the metric. What do you think about that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:281,availability,slo,slowest,281,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:260,deployability,depend,dependencies,260,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:343,deployability,updat,update,343,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:260,integrability,depend,dependencies,260,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:260,modifiability,depend,dependencies,260,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:148,performance,parallel,parallelization,148,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:281,reliability,slo,slowest,281,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:260,safety,depend,dependencies,260,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:343,safety,updat,update,343,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:343,security,updat,update,343,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:260,testability,depend,dependencies,260,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:385,usability,help,help,385,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:35,deployability,releas,release,35,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:254,deployability,modul,module,254,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:254,modifiability,modul,module,254,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:335,performance,time,time,335,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:254,safety,modul,module,254,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:463,security,access,accessed,463,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:266,testability,understand,understand,266,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:293,testability,understand,understand,293,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:95,usability,help,help,95,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:415,deployability,modul,module,415,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:464,deployability,Integr,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:571,deployability,integr,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:155,integrability,wrap,wrapping,155,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:448,integrability,sub,subsection,448,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:464,integrability,Integr,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:571,integrability,integr,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:464,interoperability,Integr,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:571,interoperability,integr,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:258,modifiability,exten,extent,258,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:415,modifiability,modul,module,415,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:464,modifiability,Integr,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:548,modifiability,pac,package,548,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:571,modifiability,integr,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:224,performance,content,contents,224,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:597,performance,time,time,597,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:464,reliability,Integr,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:571,reliability,integr,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:415,safety,modul,module,415,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:464,security,Integr,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:571,security,integr,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:280,testability,understand,understand,280,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:464,testability,Integr,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:571,testability,integr,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:20,usability,user,user,20,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:171,usability,learn,learn,171,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:371,safety,compl,complicated,371,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:371,security,compl,complicated,371,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:548,security,access,access,548,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:523,testability,simpl,simpler,523,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:686,testability,understand,understand,686,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:523,usability,simpl,simpler,523,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:187,availability,avail,available,187,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:510,integrability,topic,topic,510,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:784,performance,time,time,784,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:832,performance,time,time,832,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:187,reliability,availab,available,187,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:187,safety,avail,available,187,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:50,security,access,access,50,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:187,security,availab,available,187,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:462,security,access,accessible,462,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:25,testability,simpl,simpler,25,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:25,usability,simpl,simpler,25,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:366,usability,document,documentation,366,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:609,usability,help,help,609,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:792,usability,learn,learn,792,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:840,usability,learn,learn,840,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:193,availability,error,errors,193,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:371,deployability,releas,release,371,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:16,energy efficiency,current,currently,16,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:193,performance,error,errors,193,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:423,performance,time,timely,423,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:114,reliability,doe,doesn,114,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:152,reliability,pra,prange,152,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:233,reliability,doe,does,233,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:193,safety,error,errors,193,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:265,safety,except,except,265,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:193,usability,error,errors,193,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/917:163,modifiability,paramet,parameter,163,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:64,performance,parallel,parallelization,64,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:179,performance,parallel,parallelization,179,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:201,performance,parallel,parallel,201,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:11,testability,understand,understand,11,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:88,usability,user,user,88,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:93,usability,experien,experience,93,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:85,usability,close,close,85,I agree with @gokceneraslan . Since this PR hasn't seen any movement in 3 years I'll close it for now. Feel free to reopen if you want to discuss this further.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/issues/918:92,usability,behavi,behavior,92,Could you provide some more details? It'd be useful to see a script that can reproduce this behavior from scratch. There are some guides on how to write this up in the [contributing section](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md#before-filing-an-issue). Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:130,usability,guid,guides,130,Could you provide some more details? It'd be useful to see a script that can reproduce this behavior from scratch. There are some guides on how to write this up in the [contributing section](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md#before-filing-an-issue). Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:94,usability,close,close,94,"Yeah, you deleted the issue template that prompts you to do this. In the future we might just close issues when people do that, it’s rude.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:667,modifiability,paramet,parameters,667,"> Yeah, you deleted the issue template that prompts you to do this. In the future we might just close issues when people do that, it’s rude. Sorry for deleting the issue template. Here are details:. I just followed the example of PAGA for hematopoiesis in mouse (Paul et al., 2015) to cope with my data.I want to recomputing the umap embedding using PAGA-initialization. ![image](https://user-images.githubusercontent.com/20806068/69139814-41c9c500-0afc-11ea-9991-e21638db61c3.png). ![image](https://user-images.githubusercontent.com/20806068/69139884-64f47480-0afc-11ea-98dd-9e7430e94cfd.png). I got strange umap embedding even though i have tried several different parameters. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:96,usability,close,close,96,"> Yeah, you deleted the issue template that prompts you to do this. In the future we might just close issues when people do that, it’s rude. Sorry for deleting the issue template. Here are details:. I just followed the example of PAGA for hematopoiesis in mouse (Paul et al., 2015) to cope with my data.I want to recomputing the umap embedding using PAGA-initialization. ![image](https://user-images.githubusercontent.com/20806068/69139814-41c9c500-0afc-11ea-9991-e21638db61c3.png). ![image](https://user-images.githubusercontent.com/20806068/69139884-64f47480-0afc-11ea-98dd-9e7430e94cfd.png). I got strange umap embedding even though i have tried several different parameters. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:256,usability,mous,mouse,256,"> Yeah, you deleted the issue template that prompts you to do this. In the future we might just close issues when people do that, it’s rude. Sorry for deleting the issue template. Here are details:. I just followed the example of PAGA for hematopoiesis in mouse (Paul et al., 2015) to cope with my data.I want to recomputing the umap embedding using PAGA-initialization. ![image](https://user-images.githubusercontent.com/20806068/69139814-41c9c500-0afc-11ea-9991-e21638db61c3.png). ![image](https://user-images.githubusercontent.com/20806068/69139884-64f47480-0afc-11ea-98dd-9e7430e94cfd.png). I got strange umap embedding even though i have tried several different parameters. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:388,usability,user,user-images,388,"> Yeah, you deleted the issue template that prompts you to do this. In the future we might just close issues when people do that, it’s rude. Sorry for deleting the issue template. Here are details:. I just followed the example of PAGA for hematopoiesis in mouse (Paul et al., 2015) to cope with my data.I want to recomputing the umap embedding using PAGA-initialization. ![image](https://user-images.githubusercontent.com/20806068/69139814-41c9c500-0afc-11ea-9991-e21638db61c3.png). ![image](https://user-images.githubusercontent.com/20806068/69139884-64f47480-0afc-11ea-98dd-9e7430e94cfd.png). I got strange umap embedding even though i have tried several different parameters. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:500,usability,user,user-images,500,"> Yeah, you deleted the issue template that prompts you to do this. In the future we might just close issues when people do that, it’s rude. Sorry for deleting the issue template. Here are details:. I just followed the example of PAGA for hematopoiesis in mouse (Paul et al., 2015) to cope with my data.I want to recomputing the umap embedding using PAGA-initialization. ![image](https://user-images.githubusercontent.com/20806068/69139814-41c9c500-0afc-11ea-9991-e21638db61c3.png). ![image](https://user-images.githubusercontent.com/20806068/69139884-64f47480-0afc-11ea-98dd-9e7430e94cfd.png). I got strange umap embedding even though i have tried several different parameters. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:388,deployability,contain,container,388,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:482,deployability,log,logging,482,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:885,deployability,scale,scale,885,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:975,deployability,log,log,975,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:331,energy efficiency,core,core,331,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:885,energy efficiency,scale,scale,885,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:48,integrability,pub,published,48,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:848,integrability,batch,batch,848,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1171,integrability,sub,subplots,1171,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:885,modifiability,scal,scale,885,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:848,performance,batch,batch,848,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:885,performance,scale,scale,885,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:482,safety,log,logging,482,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:975,safety,log,log,975,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:482,security,log,logging,482,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:975,security,log,log,975,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:482,testability,log,logging,482,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:975,testability,log,log,975,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1343,usability,tool,tools,1343,"@flying-sheep . Because the paper have not been published, so i can't offer the data to reproduce this problem. Here are the codes:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. from IPython.core.display import display, HTML. display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###. sc.settings.verbosity = 1. sc.logging.print_versions(). results_file = './write/sp.h5ad'. sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.combat(sp,key='batch',covariates=['sample']). sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:121,availability,error,error,121,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:189,availability,error,error,189,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1304,availability,error,errors,1304,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1495,availability,error,error,1495,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:241,deployability,Fail,Failed,241,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:265,deployability,pipelin,pipeline,265,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1099,deployability,releas,release,1099,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:321,energy efficiency,CPU,CPUDispatcher,321,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:573,energy efficiency,CPU,CPUDispatcher,573,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:885,energy efficiency,current,current,885,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:265,integrability,pipelin,pipeline,265,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1501,integrability,messag,message,1501,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1501,interoperability,messag,message,1501,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:377,modifiability,paramet,parameters,377,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:518,modifiability,paramet,parameterized,518,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:705,modifiability,pac,packages,705,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:789,modifiability,pac,packages,789,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:121,performance,error,error,121,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:189,performance,error,error,189,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:321,performance,CPU,CPUDispatcher,321,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:573,performance,CPU,CPUDispatcher,573,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1304,performance,error,errors,1304,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1495,performance,error,error,1495,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:241,reliability,Fail,Failed,241,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1413,reliability,doe,doesn-t-compile,1413,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:121,safety,error,error,121,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:189,safety,error,error,189,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1304,safety,error,errors,1304,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1495,safety,error,error,1495,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:442,security,sign,signatures,442,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1514,testability,trace,traceback,1514,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:121,usability,error,error,121,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:189,usability,error,error,189,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1075,usability,support,supported,1075,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1304,usability,error,errors,1304,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1382,usability,user,user,1382,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1495,usability,error,error,1495,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1538,usability,minim,minimal,1538,"@ivirshup . Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks! ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)). [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:2086,availability,down,down,2086,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:426,deployability,log,logging,426,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:883,deployability,scale,scale,883,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:973,deployability,log,log,973,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1891,deployability,version,version,1891,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1930,deployability,releas,release,1930,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1949,deployability,updat,update,1949,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:883,energy efficiency,scale,scale,883,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1922,energy efficiency,current,current,1922,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1169,integrability,sub,subplots,1169,"lts (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This wil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1891,integrability,version,version,1891,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:198,modifiability,variab,variables,198,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:883,modifiability,scal,scale,883,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1766,modifiability,variab,variables,1766,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1891,modifiability,version,version,1891,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:2195,modifiability,variab,variables,2195,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:883,performance,scale,scale,883,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:2011,reliability,doe,does,2011,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:426,safety,log,logging,426,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:973,safety,log,log,973,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1949,safety,updat,update,1949,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:426,security,log,logging,426,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:973,security,log,log,973,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1252,security,Modif,Modified,1252,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1949,security,updat,update,1949,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:426,testability,log,logging,426,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:973,testability,log,log,973,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:40,usability,help,helpful,40,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:562,usability,learn,learn,562,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1465,usability,tool,tools,1465,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:1642,usability,user,user-images,1642,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:2105,usability,command,commands,2105,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:2231,usability,behavi,behavior,2231,"`python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. import seaborn as sns. import anndata. import matplotlib as mpl. import scipy. sc.logging.print_versions(). # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 . # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(). sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'). sc.pp.log1p(sp). sp.raw=sp. sc.pp.highly_variable_genes(sp, n_top_genes=2000). sc.pl.highly_variable_genes(sp). sp = sp[:, sp.var['highly_variable']]. sc.pp.scale(sp, max_value=10). sc.tl.pca(sp, svd_solver='arpack'). sc.pl.pca_variance_ratio(sp, log=True). sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30). sc.tl.diffmap(sp). sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'). sc.tl.louvain(sp,resolution=1). sc.tl.paga(sp). _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}). # Modified this call because pos_coord wasn't defined:. # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) . sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs). from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(sp,init_pos=init(sp)). sc.pl.umap(sp,color='louvain'). ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue. * I noticed your scanpy version wasn't the same as the current release, could you update that? * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts? * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:35,safety,input,inputs,35,"As we haven't heard back after the inputs above we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:35,usability,input,inputs,35,"As we haven't heard back after the inputs above we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:56,usability,close,close,56,"As we haven't heard back after the inputs above we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:117,usability,behavi,behaviour,117,"As we haven't heard back after the inputs above we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/919:145,availability,cluster,cluster,145,"Hi @arutik,. The reason the two DE gene sets are not the same is that `sc.tl.rank_genes_groups()` only reports genes that are upregulated in one cluster (the one in specified in the `group` parameter) compared to the other. The test itself should be symmetric if I'm not mistaken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:145,deployability,cluster,cluster,145,"Hi @arutik,. The reason the two DE gene sets are not the same is that `sc.tl.rank_genes_groups()` only reports genes that are upregulated in one cluster (the one in specified in the `group` parameter) compared to the other. The test itself should be symmetric if I'm not mistaken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:165,interoperability,specif,specified,165,"Hi @arutik,. The reason the two DE gene sets are not the same is that `sc.tl.rank_genes_groups()` only reports genes that are upregulated in one cluster (the one in specified in the `group` parameter) compared to the other. The test itself should be symmetric if I'm not mistaken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:190,modifiability,paramet,parameter,190,"Hi @arutik,. The reason the two DE gene sets are not the same is that `sc.tl.rank_genes_groups()` only reports genes that are upregulated in one cluster (the one in specified in the `group` parameter) compared to the other. The test itself should be symmetric if I'm not mistaken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:228,safety,test,test,228,"Hi @arutik,. The reason the two DE gene sets are not the same is that `sc.tl.rank_genes_groups()` only reports genes that are upregulated in one cluster (the one in specified in the `group` parameter) compared to the other. The test itself should be symmetric if I'm not mistaken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:228,testability,test,test,228,"Hi @arutik,. The reason the two DE gene sets are not the same is that `sc.tl.rank_genes_groups()` only reports genes that are upregulated in one cluster (the one in specified in the `group` parameter) compared to the other. The test itself should be symmetric if I'm not mistaken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:208,availability,down,downregulated,208,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:146,deployability,log,logFC,146,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:390,integrability,filter,filter,390,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:96,safety,test,test,96,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:146,safety,log,logFC,146,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:174,safety,test,test,174,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:296,safety,test,test,296,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:537,safety,test,tests,537,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:146,security,log,logFC,146,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:96,testability,test,test,96,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:146,testability,log,logFC,146,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:174,testability,test,test,174,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:296,testability,test,test,296,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:537,testability,test,tests,537,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:36,usability,experien,experienced,36,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:47,usability,behavi,behaviour,47,Do you have some example code to reproduce the behaviour you are seeing?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:467,availability,down,downregulated,467,"Hi, sorry for the delay, sure,. I have my adata object and let's say a key 'group' in the annotation that can either be A or B for each cell, and then I do:. sc.tl.rank_genes_groups(adata, groupby='group', n_genes=1000). As a result, from the object attributes I can get the tables of DE genes, so I get a table for group A vs rest (which is only group B in this case) and for group B vs rest (which is only group A). Both of these lists contain both upregulated and downregulated DE genes, but are not symmetrical. Let me know if this is unclear. . Thank you. . Sincerely, . Anna Arutyunyan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:438,deployability,contain,contain,438,"Hi, sorry for the delay, sure,. I have my adata object and let's say a key 'group' in the annotation that can either be A or B for each cell, and then I do:. sc.tl.rank_genes_groups(adata, groupby='group', n_genes=1000). As a result, from the object attributes I can get the tables of DE genes, so I get a table for group A vs rest (which is only group B in this case) and for group B vs rest (which is only group A). Both of these lists contain both upregulated and downregulated DE genes, but are not symmetrical. Let me know if this is unclear. . Thank you. . Sincerely, . Anna Arutyunyan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/920:55,deployability,instal,installation,55,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920
https://github.com/scverse/scanpy/issues/920:122,deployability,instal,installation,122,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920
https://github.com/scverse/scanpy/issues/920:256,deployability,instal,install,256,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920
https://github.com/scverse/scanpy/issues/920:155,modifiability,pac,package,155,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920
https://github.com/scverse/scanpy/issues/921:446,deployability,pipelin,pipeline,446,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:359,energy efficiency,profil,profiling,359,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:505,energy efficiency,GPU,GPU,505,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:859,energy efficiency,GPU,GPU,859,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:446,integrability,pipelin,pipeline,446,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:946,integrability,wrap,wrapping,946,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:971,integrability,wrap,wrapper,971,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1039,integrability,protocol,protocol,1039,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:971,interoperability,wrapper,wrapper,971,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1039,interoperability,protocol,protocol,1039,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:162,performance,overhead,overhead,162,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:359,performance,profil,profiling,359,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:416,performance,perform,performance,416,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:505,performance,GPU,GPU,505,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:859,performance,GPU,GPU,859,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:416,usability,perform,performance,416,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:788,usability,help,help,788,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases. * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help. * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1205,availability,operat,operations,1205,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:557,deployability,version,version,557,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:976,deployability,pipelin,pipeline,976,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:547,energy efficiency,optim,optimized,547,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:889,energy efficiency,profil,profiling,889,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1296,energy efficiency,GPU,GPU,1296,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:455,integrability,sub,subset,455,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:506,integrability,sub,subset,506,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:557,integrability,version,version,557,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:976,integrability,pipelin,pipeline,976,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:557,modifiability,version,version,557,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:95,performance,overhead,overhead,95,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:547,performance,optimiz,optimized,547,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:889,performance,profil,profiling,889,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:946,performance,perform,performance,946,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1220,performance,bottleneck,bottlenecks,1220,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1296,performance,GPU,GPU,1296,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1272,safety,compl,complete,1272,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1272,security,compl,complete,1272,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:946,usability,perform,performance,946,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:990,usability,close,closest,990,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1035,usability,UI,UI,1035,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:83,integrability,wrap,wrapper,83,"Hi @mrocklin, you might be interested in this work, especially the Dask-compatible wrapper around `scipy.sparse`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:72,interoperability,compatib,compatible,72,"Hi @mrocklin, you might be interested in this work, especially the Dask-compatible wrapper around `scipy.sparse`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:83,interoperability,wrapper,wrapper,83,"Hi @mrocklin, you might be interested in this work, especially the Dask-compatible wrapper around `scipy.sparse`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:62,availability,ping,pinging,62,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:227,availability,slo,slower,227,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:316,availability,sli,slimmed,316,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:324,availability,down,down,324,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:344,availability,operat,operations,344,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:753,availability,operat,operations,753,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1199,availability,operat,operation,1199,"Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation sl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1739,availability,operat,operations,1739,"th basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2202,availability,slo,slowed,2202,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2209,availability,down,down,2209,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2681,availability,operat,operations,2681,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:329,deployability,version,version,329,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2450,deployability,pipelin,pipeline,2450,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2874,deployability,modul,module,2874,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:804,energy efficiency,core,cores,804,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1561,energy efficiency,current,currently,1561,"I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the bench",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2363,energy efficiency,profil,profiling,2363,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2771,energy efficiency,profil,profiling,2771,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2827,energy efficiency,schedul,scheduler,2827,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2941,energy efficiency,profil,profiling,2941,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:329,integrability,version,version,329,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:398,integrability,sub,submit,398,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:576,integrability,wrap,wrapper,576,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:644,integrability,protocol,protocol,644,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:924,integrability,wrap,wrapper,924,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2450,integrability,pipelin,pipeline,2450,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:576,interoperability,wrapper,wrapper,576,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:644,interoperability,protocol,protocol,644,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:924,interoperability,wrapper,wrapper,924,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1718,interoperability,semant,semantically,1718,"seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:329,modifiability,version,version,329,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1017,modifiability,interm,intermediate,1017,"many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2874,modifiability,modul,module,2874,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1664,performance,time,time,1664,"sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is usef",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2363,performance,profil,profiling,2363,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2420,performance,perform,performance,2420,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2696,performance,bottleneck,bottlenecks,2696,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2771,performance,profil,profiling,2771,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2827,performance,schedul,scheduler,2827,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2902,performance,parallel,parallelism,2902,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2941,performance,profil,profiling,2941,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3064,performance,time,time,3064,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:227,reliability,slo,slower,227,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:316,reliability,sli,slimmed,316,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2202,reliability,slo,slowed,2202,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:914,safety,avoid,avoid,914,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2748,safety,compl,complete,2748,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2874,safety,modul,module,2874,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2892,safety,avoid,avoid,2892,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1877,security,sign,significant,1877,"y.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2214,security,sign,significantly,2214,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2748,security,compl,complete,2748,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:471,usability,responsiv,responsive,471,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:514,usability,learn,learning,514,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2108,usability,help,helpful,2108,"ause the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with y",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2420,usability,perform,performance,2420,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2466,usability,close,closest,2466,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2511,usability,UI,UI,2511,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2995,usability,person,personally,2995,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:194,availability,operat,operations,194,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:245,energy efficiency,core,cores,245,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:17,integrability,wrap,wrapper,17,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:85,integrability,protocol,protocol,85,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:371,integrability,wrap,wrapper,371,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:17,interoperability,wrapper,wrapper,17,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:85,interoperability,protocol,protocol,85,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:371,interoperability,wrapper,wrapper,371,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:550,interoperability,semant,semantics,550,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:570,interoperability,compatib,compatible,570,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:361,safety,avoid,avoid,361,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:587,safety,input,input,587,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:587,usability,input,input,587,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. > . > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:19,integrability,wrap,wrapper,19,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:19,interoperability,wrapper,wrapper,19,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:138,modifiability,pac,package,138,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:226,testability,context,context,226,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:151,usability,user,users,151,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:263,usability,user,users,263,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:76,availability,sli,slimmed,76,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:84,availability,down,down,84,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:104,availability,operat,operations,104,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:89,deployability,version,version,89,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:89,integrability,version,version,89,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:158,integrability,sub,submit,158,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:89,modifiability,version,version,89,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:76,reliability,sli,slimmed,76,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:223,safety,test,test,223,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:223,testability,test,test,223,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:438,usability,support,support,438,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:770,usability,help,helpful,770,"Thank you for the detailed response @mrocklin! > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support. Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:325,deployability,API,API,325,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:282,energy efficiency,current,currently,282,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:21,integrability,wrap,wrapper,21,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:259,integrability,wrap,wrapper,259,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:325,integrability,API,API,325,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:463,integrability,wrap,wrapper,463,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:21,interoperability,wrapper,wrapper,21,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:259,interoperability,wrapper,wrapper,259,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:325,interoperability,API,API,325,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:463,interoperability,wrapper,wrapper,463,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:140,modifiability,pac,package,140,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:292,security,expos,exposes,292,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:153,usability,user,users,153,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:8,availability,operat,operations,8,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:857,integrability,queue,queue,857,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:857,performance,queue,queue,857,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:697,reliability,doe,does,697,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:743,reliability,doe,does,743,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:773,reliability,doe,does,773,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:146,security,sign,significant,146,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:382,usability,help,helpful,382,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:594,usability,close,close,594,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. >. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:255,energy efficiency,current,currently,255,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:684,integrability,wrap,wrapper,684,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:412,interoperability,semant,semantically,412,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:598,interoperability,compatib,compatible,598,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:641,interoperability,format,formats,641,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:684,interoperability,wrapper,wrapper,684,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:358,performance,time,time,358,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:811,testability,simpl,simple,811,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:811,usability,simpl,simple,811,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:926,usability,help,help,926,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>. <summary><code>__array_function__</code> implementation</summary>. ```python. def __array_function__(self, func, types, args, kwargs):. result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs). if issparse(result):. result = SparseArray(result). elif isinstance(result, np.matrix):. result = np.asarray(result). return result. ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:388,usability,support,supporting,388,"> @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library. That's a question for @hameerabbasi , who leads pydata/sparse development. I think that he may also have other thoughts on supporting CSR/CSC classes. I think that he and others have been working on a general solution to this for a little while now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:46,interoperability,format,format,46,"That general solution for CSR/CSC is the GXCS format, see https://github.com/pydata/sparse/issues/125. Now that SciPy 1.4.0 added support for using `scipy.sparse.linalg` with pydata/sparse arrays, and we have that GXCS format getting closer to being ready for general use, I think that's the main way forward.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:219,interoperability,format,format,219,"That general solution for CSR/CSC is the GXCS format, see https://github.com/pydata/sparse/issues/125. Now that SciPy 1.4.0 added support for using `scipy.sparse.linalg` with pydata/sparse arrays, and we have that GXCS format getting closer to being ready for general use, I think that's the main way forward.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:130,usability,support,support,130,"That general solution for CSR/CSC is the GXCS format, see https://github.com/pydata/sparse/issues/125. Now that SciPy 1.4.0 added support for using `scipy.sparse.linalg` with pydata/sparse arrays, and we have that GXCS format getting closer to being ready for general use, I think that's the main way forward.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:234,usability,close,closer,234,"That general solution for CSR/CSC is the GXCS format, see https://github.com/pydata/sparse/issues/125. Now that SciPy 1.4.0 added support for using `scipy.sparse.linalg` with pydata/sparse arrays, and we have that GXCS format getting closer to being ready for general use, I think that's the main way forward.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/922:27,reliability,pra,practices,27,could you do this for best practices as well? That would be a nice link between the two. Even if best practices is more than only scanpy. Still get a lot of questions in scanpy issues and discourse that best practices can help answer.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/issues/922:102,reliability,pra,practices,102,could you do this for best practices as well? That would be a nice link between the two. Even if best practices is more than only scanpy. Still get a lot of questions in scanpy issues and discourse that best practices can help answer.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/issues/922:208,reliability,pra,practices,208,could you do this for best practices as well? That would be a nice link between the two. Even if best practices is more than only scanpy. Still get a lot of questions in scanpy issues and discourse that best practices can help answer.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/issues/922:222,usability,help,help,222,could you do this for best practices as well? That would be a nice link between the two. Even if best practices is more than only scanpy. Still get a lot of questions in scanpy issues and discourse that best practices can help answer.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/issues/922:3,energy efficiency,current,currently,3,"We currently have scanpy_tutorials, scanpy_usage and single-cell-tutorial, right? We shouldn’t include too many in the sidebar, but less than about 5 should be fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/pull/923:15,deployability,continu,continue,15,"Closing, let’s continue this in https://github.com/scverse/scanpy-tutorials/issues/64",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/923
https://github.com/scverse/scanpy/issues/924:69,deployability,updat,updating,69,"Hi guys,. I figure it out by creating a python 3.7.1 environment and updating the following libraries: numpy - 1.16.3 and scanpy - 1.4.1 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:69,safety,updat,updating,69,"Hi guys,. I figure it out by creating a python 3.7.1 environment and updating the following libraries: numpy - 1.16.3 and scanpy - 1.4.1 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:69,security,updat,updating,69,"Hi guys,. I figure it out by creating a python 3.7.1 environment and updating the following libraries: numpy - 1.16.3 and scanpy - 1.4.1 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:15,deployability,updat,update,15,Thanks for the update!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:15,safety,updat,update,15,Thanks for the update!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:15,security,updat,update,15,Thanks for the update!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/925:4,availability,cluster,clusters,4,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:98,availability,cluster,clusters,98,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:158,availability,cluster,clusters,158,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:4,deployability,cluster,clusters,4,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:98,deployability,cluster,clusters,98,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:135,deployability,contain,containing,135,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:158,deployability,cluster,clusters,158,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:17,testability,simpl,simply,17,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:17,usability,simpl,simply,17,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON. old_to_new = dict(. old_cluster1='new_cluster1',. old_cluster2='new_cluster1',. old_cluster3='new_cluster2',. ). adata.obs['new_clusters'] = (. adata.obs['old_clusters']. .map(old_to_new). .astype('category'). ). ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:12,usability,help,help,12,"For general help like this, please go to https://scanpy.discourse.group/. This is also what the issue template says. How could we have made the text more clear so that you’d have found your way there? ![grafik](https://user-images.githubusercontent.com/291575/69068901-e0cfbd80-0a25-11ea-8095-52e567b86574.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:154,usability,clear,clear,154,"For general help like this, please go to https://scanpy.discourse.group/. This is also what the issue template says. How could we have made the text more clear so that you’d have found your way there? ![grafik](https://user-images.githubusercontent.com/291575/69068901-e0cfbd80-0a25-11ea-8095-52e567b86574.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:219,usability,user,user-images,219,"For general help like this, please go to https://scanpy.discourse.group/. This is also what the issue template says. How could we have made the text more clear so that you’d have found your way there? ![grafik](https://user-images.githubusercontent.com/291575/69068901-e0cfbd80-0a25-11ea-8095-52e567b86574.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:84,availability,cluster,clusters,84,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:175,availability,cluster,clusters,175,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:200,availability,cluster,clustering,200,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:218,availability,cluster,clusters,218,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:240,availability,cluster,clustering,240,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:84,deployability,cluster,clusters,84,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:175,deployability,cluster,clusters,175,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:200,deployability,cluster,clustering,200,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:218,deployability,cluster,clusters,218,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:240,deployability,cluster,clustering,240,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:74,integrability,sub,subset,74,"Here's a related question, if I want to make a labelling which includes a subset of clusters from a few different solutions, is there a concise way to write that? I.e. I want clusters 1,2, and 3 from clustering A, and clusters 4 and 5 from clustering B.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:44,availability,cluster,clusters,44,"I think you will need two steps, one to get clusters 1,2, and 3 from clustering A and other for the rest",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:69,availability,cluster,clustering,69,"I think you will need two steps, one to get clusters 1,2, and 3 from clustering A and other for the rest",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:44,deployability,cluster,clusters,44,"I think you will need two steps, one to get clusters 1,2, and 3 from clustering A and other for the rest",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:69,deployability,cluster,clustering,69,"I think you will need two steps, one to get clusters 1,2, and 3 from clustering A and other for the rest",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:32,availability,cluster,clustering,32,"Can this method apply to Leiden clustering as well? I recapitulated the above code in my program, and my new cluster column returned only NaNs. What should the ""old_cluster1"" side of the structure look like when I am trying to make that dictionary? Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:109,availability,cluster,cluster,109,"Can this method apply to Leiden clustering as well? I recapitulated the above code in my program, and my new cluster column returned only NaNs. What should the ""old_cluster1"" side of the structure look like when I am trying to make that dictionary? Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:32,deployability,cluster,clustering,32,"Can this method apply to Leiden clustering as well? I recapitulated the above code in my program, and my new cluster column returned only NaNs. What should the ""old_cluster1"" side of the structure look like when I am trying to make that dictionary? Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:109,deployability,cluster,cluster,109,"Can this method apply to Leiden clustering as well? I recapitulated the above code in my program, and my new cluster column returned only NaNs. What should the ""old_cluster1"" side of the structure look like when I am trying to make that dictionary? Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:34,availability,cluster,clustering,34,"> Can this method apply to Leiden clustering as well? I recapitulated the above code in my program, and my new cluster column returned only NaNs. What should the ""old_cluster1"" side of the structure look like when I am trying to make that dictionary? > . > Thanks. I have the same issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:111,availability,cluster,cluster,111,"> Can this method apply to Leiden clustering as well? I recapitulated the above code in my program, and my new cluster column returned only NaNs. What should the ""old_cluster1"" side of the structure look like when I am trying to make that dictionary? > . > Thanks. I have the same issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:34,deployability,cluster,clustering,34,"> Can this method apply to Leiden clustering as well? I recapitulated the above code in my program, and my new cluster column returned only NaNs. What should the ""old_cluster1"" side of the structure look like when I am trying to make that dictionary? > . > Thanks. I have the same issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:111,deployability,cluster,cluster,111,"> Can this method apply to Leiden clustering as well? I recapitulated the above code in my program, and my new cluster column returned only NaNs. What should the ""old_cluster1"" side of the structure look like when I am trying to make that dictionary? > . > Thanks. I have the same issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:731,interoperability,specif,specific,731,"Just to answer those that, like me, are beginners in python, the solution provided by @ivirshup works perfectly (of course for `louvain` and `leiden,` and any other `adata.obs` that you want to remap):. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b""}.get(x, x)). .astype(""category""). ). ```. Where ""a"" is the name of the category you want to change, and ""b"" is the new name of the category that you want to change. If you have more categories you want to change simply add more entries to the dictionary like:. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b"", ""c"": ""d""}.get(x, x)). .astype(""category""). ). ```. @fidelram answer does not work in this specific case because the `adata.obs` from the louvain (or leiden) algorithm are categories named 0, 1, 2, 3, 4 and you cannot construct a dictionary using '0':'X' because `SyntaxError: keyword can't be an expression`. Hope this helps,. Best,. A.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:709,reliability,doe,does,709,"Just to answer those that, like me, are beginners in python, the solution provided by @ivirshup works perfectly (of course for `louvain` and `leiden,` and any other `adata.obs` that you want to remap):. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b""}.get(x, x)). .astype(""category""). ). ```. Where ""a"" is the name of the category you want to change, and ""b"" is the new name of the category that you want to change. If you have more categories you want to change simply add more entries to the dictionary like:. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b"", ""c"": ""d""}.get(x, x)). .astype(""category""). ). ```. @fidelram answer does not work in this specific case because the `adata.obs` from the louvain (or leiden) algorithm are categories named 0, 1, 2, 3, 4 and you cannot construct a dictionary using '0':'X' because `SyntaxError: keyword can't be an expression`. Hope this helps,. Best,. A.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:503,testability,simpl,simply,503,"Just to answer those that, like me, are beginners in python, the solution provided by @ivirshup works perfectly (of course for `louvain` and `leiden,` and any other `adata.obs` that you want to remap):. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b""}.get(x, x)). .astype(""category""). ). ```. Where ""a"" is the name of the category you want to change, and ""b"" is the new name of the category that you want to change. If you have more categories you want to change simply add more entries to the dictionary like:. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b"", ""c"": ""d""}.get(x, x)). .astype(""category""). ). ```. @fidelram answer does not work in this specific case because the `adata.obs` from the louvain (or leiden) algorithm are categories named 0, 1, 2, 3, 4 and you cannot construct a dictionary using '0':'X' because `SyntaxError: keyword can't be an expression`. Hope this helps,. Best,. A.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:503,usability,simpl,simply,503,"Just to answer those that, like me, are beginners in python, the solution provided by @ivirshup works perfectly (of course for `louvain` and `leiden,` and any other `adata.obs` that you want to remap):. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b""}.get(x, x)). .astype(""category""). ). ```. Where ""a"" is the name of the category you want to change, and ""b"" is the new name of the category that you want to change. If you have more categories you want to change simply add more entries to the dictionary like:. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b"", ""c"": ""d""}.get(x, x)). .astype(""category""). ). ```. @fidelram answer does not work in this specific case because the `adata.obs` from the louvain (or leiden) algorithm are categories named 0, 1, 2, 3, 4 and you cannot construct a dictionary using '0':'X' because `SyntaxError: keyword can't be an expression`. Hope this helps,. Best,. A.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:960,usability,help,helps,960,"Just to answer those that, like me, are beginners in python, the solution provided by @ivirshup works perfectly (of course for `louvain` and `leiden,` and any other `adata.obs` that you want to remap):. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b""}.get(x, x)). .astype(""category""). ). ```. Where ""a"" is the name of the category you want to change, and ""b"" is the new name of the category that you want to change. If you have more categories you want to change simply add more entries to the dictionary like:. ```. adata.obs['new_clusters'] = (. adata.obs[""old_clusters""]. .map(lambda x: {""a"": ""b"", ""c"": ""d""}.get(x, x)). .astype(""category""). ). ```. @fidelram answer does not work in this specific case because the `adata.obs` from the louvain (or leiden) algorithm are categories named 0, 1, 2, 3, 4 and you cannot construct a dictionary using '0':'X' because `SyntaxError: keyword can't be an expression`. Hope this helps,. Best,. A.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:94,availability,cluster,clusters,94,"Hi guys, . Thank you for sharing your code and explanation. What if I want to rename multiple clusters [""a"",""c"",""d""] to ""b"" ? . I have tried a list of elements to change as a key, but it does not work for me. . Thanks in advance for your reply",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:94,deployability,cluster,clusters,94,"Hi guys, . Thank you for sharing your code and explanation. What if I want to rename multiple clusters [""a"",""c"",""d""] to ""b"" ? . I have tried a list of elements to change as a key, but it does not work for me. . Thanks in advance for your reply",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:187,reliability,doe,does,187,"Hi guys, . Thank you for sharing your code and explanation. What if I want to rename multiple clusters [""a"",""c"",""d""] to ""b"" ? . I have tried a list of elements to change as a key, but it does not work for me. . Thanks in advance for your reply",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:97,availability,cluster,clusters,97,"the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. old_to_new = {. 0:'Astrocytes 1',. 1:'Glutamatergic neurons 1',. 2:'Astrocytes 2',. 3:'Oligodendrocytes 1',. 4:'Inhibitory neurons 1',. 5:'Glutamatergic neurons 2',. 6:'Oligodendrocytes 1',. 7:'Unknown',. 8:'OPCs',. 9:'Glutamatergic neurons 3',. 10:'Microglia',. 11:'Inhibitory neurons 1',. 12:'Tanycytes',. 13:'Endothelial',. 14:'Astrocytes 3',. 15:'Oligodendrocytes 1',. 16:'Inhibitory neurons 2',. 17:'T cells',. 18:'Oligodendrocytes 2',. }. adata.obs['annotation'] = (. adata.obs['seurat_clusters']. .map(old_to_new). .astype('category'). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:97,deployability,cluster,clusters,97,"the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. old_to_new = {. 0:'Astrocytes 1',. 1:'Glutamatergic neurons 1',. 2:'Astrocytes 2',. 3:'Oligodendrocytes 1',. 4:'Inhibitory neurons 1',. 5:'Glutamatergic neurons 2',. 6:'Oligodendrocytes 1',. 7:'Unknown',. 8:'OPCs',. 9:'Glutamatergic neurons 3',. 10:'Microglia',. 11:'Inhibitory neurons 1',. 12:'Tanycytes',. 13:'Endothelial',. 14:'Astrocytes 3',. 15:'Oligodendrocytes 1',. 16:'Inhibitory neurons 2',. 17:'T cells',. 18:'Oligodendrocytes 2',. }. adata.obs['annotation'] = (. adata.obs['seurat_clusters']. .map(old_to_new). .astype('category'). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:49,interoperability,format,formating,49,"the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. old_to_new = {. 0:'Astrocytes 1',. 1:'Glutamatergic neurons 1',. 2:'Astrocytes 2',. 3:'Oligodendrocytes 1',. 4:'Inhibitory neurons 1',. 5:'Glutamatergic neurons 2',. 6:'Oligodendrocytes 1',. 7:'Unknown',. 8:'OPCs',. 9:'Glutamatergic neurons 3',. 10:'Microglia',. 11:'Inhibitory neurons 1',. 12:'Tanycytes',. 13:'Endothelial',. 14:'Astrocytes 3',. 15:'Oligodendrocytes 1',. 16:'Inhibitory neurons 2',. 17:'T cells',. 18:'Oligodendrocytes 2',. }. adata.obs['annotation'] = (. adata.obs['seurat_clusters']. .map(old_to_new). .astype('category'). )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:592,security,hardcod,hardcode,592,"I think anndata’s `rename_categories` should accept non-unique values as argument. Then one could simply do things like. ```py. cluster_markers = {. 'CD4 T': {'IL7R'},. 'CD14+\nMonocytes': {'CD14', 'LYZ'},. 'B': {'MS4A1'},. 'CD8 T': {'CD8A'},. 'NK': {'GNLY', 'NKG7'},. 'FCGR3A+\nMonocytes': {'FCGR3A', 'MS4A7'},. 'Dendritic': {'FCER1A', 'CST3'},. 'Mega-\nkaryocytes': {'PPBP'},. }. marker_matches = sc.tl.marker_gene_overlap(adata, cluster_markers). adata.rename_categories('leiden', marker_matches.idxmax()). ```. As it stands, things like the `pbmc3k` tutorial are super flaky because they hardcode things like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:98,testability,simpl,simply,98,"I think anndata’s `rename_categories` should accept non-unique values as argument. Then one could simply do things like. ```py. cluster_markers = {. 'CD4 T': {'IL7R'},. 'CD14+\nMonocytes': {'CD14', 'LYZ'},. 'B': {'MS4A1'},. 'CD8 T': {'CD8A'},. 'NK': {'GNLY', 'NKG7'},. 'FCGR3A+\nMonocytes': {'FCGR3A', 'MS4A7'},. 'Dendritic': {'FCER1A', 'CST3'},. 'Mega-\nkaryocytes': {'PPBP'},. }. marker_matches = sc.tl.marker_gene_overlap(adata, cluster_markers). adata.rename_categories('leiden', marker_matches.idxmax()). ```. As it stands, things like the `pbmc3k` tutorial are super flaky because they hardcode things like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:98,usability,simpl,simply,98,"I think anndata’s `rename_categories` should accept non-unique values as argument. Then one could simply do things like. ```py. cluster_markers = {. 'CD4 T': {'IL7R'},. 'CD14+\nMonocytes': {'CD14', 'LYZ'},. 'B': {'MS4A1'},. 'CD8 T': {'CD8A'},. 'NK': {'GNLY', 'NKG7'},. 'FCGR3A+\nMonocytes': {'FCGR3A', 'MS4A7'},. 'Dendritic': {'FCER1A', 'CST3'},. 'Mega-\nkaryocytes': {'PPBP'},. }. marker_matches = sc.tl.marker_gene_overlap(adata, cluster_markers). adata.rename_categories('leiden', marker_matches.idxmax()). ```. As it stands, things like the `pbmc3k` tutorial are super flaky because they hardcode things like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:0,energy efficiency,Cool,Cool,0,"Cool use of `.idxmax()` here, @flying-sheep! I would still inspect manually though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:101,availability,cluster,clusters,101,"> Hi guys,. > . > Thank you for sharing your code and explanation. What if I want to rename multiple clusters [""a"",""c"",""d""] to ""b"" ? I have tried a list of elements to change as a key, but it does not work for me. > . > Thanks in advance for your reply. I have the same question, anyone have the solution? Please let me know. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:101,deployability,cluster,clusters,101,"> Hi guys,. > . > Thank you for sharing your code and explanation. What if I want to rename multiple clusters [""a"",""c"",""d""] to ""b"" ? I have tried a list of elements to change as a key, but it does not work for me. > . > Thanks in advance for your reply. I have the same question, anyone have the solution? Please let me know. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:192,reliability,doe,does,192,"> Hi guys,. > . > Thank you for sharing your code and explanation. What if I want to rename multiple clusters [""a"",""c"",""d""] to ""b"" ? I have tried a list of elements to change as a key, but it does not work for me. > . > Thanks in advance for your reply. I have the same question, anyone have the solution? Please let me know. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:99,availability,cluster,clusters,99,"> the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. > . > old_to_new = { 0:'Astrocytes 1', 1:'Glutamatergic neurons 1', 2:'Astrocytes 2', 3:'Oligodendrocytes 1', 4:'Inhibitory neurons 1', 5:'Glutamatergic neurons 2', 6:'Oligodendrocytes 1', 7:'Unknown', 8:'OPCs', 9:'Glutamatergic neurons 3', 10:'Microglia', 11:'Inhibitory neurons 1', 12:'Tanycytes', 13:'Endothelial', 14:'Astrocytes 3', 15:'Oligodendrocytes 1', 16:'Inhibitory neurons 2', 17:'T cells', 18:'Oligodendrocytes 2', } adata.obs['annotation'] = ( adata.obs['seurat_clusters'] .map(old_to_new) .astype('category') ). For me, adding quotation marks to the cluster ID did the trick. I then just did. `adata.obs[""celltype""] = adata.obs.leiden.map(old_to_new)` like shown [here](https://www.sc-best-practices.org/cellular_structure/annotation.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:706,availability,cluster,cluster,706,"> the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. > . > old_to_new = { 0:'Astrocytes 1', 1:'Glutamatergic neurons 1', 2:'Astrocytes 2', 3:'Oligodendrocytes 1', 4:'Inhibitory neurons 1', 5:'Glutamatergic neurons 2', 6:'Oligodendrocytes 1', 7:'Unknown', 8:'OPCs', 9:'Glutamatergic neurons 3', 10:'Microglia', 11:'Inhibitory neurons 1', 12:'Tanycytes', 13:'Endothelial', 14:'Astrocytes 3', 15:'Oligodendrocytes 1', 16:'Inhibitory neurons 2', 17:'T cells', 18:'Oligodendrocytes 2', } adata.obs['annotation'] = ( adata.obs['seurat_clusters'] .map(old_to_new) .astype('category') ). For me, adding quotation marks to the cluster ID did the trick. I then just did. `adata.obs[""celltype""] = adata.obs.leiden.map(old_to_new)` like shown [here](https://www.sc-best-practices.org/cellular_structure/annotation.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:99,deployability,cluster,clusters,99,"> the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. > . > old_to_new = { 0:'Astrocytes 1', 1:'Glutamatergic neurons 1', 2:'Astrocytes 2', 3:'Oligodendrocytes 1', 4:'Inhibitory neurons 1', 5:'Glutamatergic neurons 2', 6:'Oligodendrocytes 1', 7:'Unknown', 8:'OPCs', 9:'Glutamatergic neurons 3', 10:'Microglia', 11:'Inhibitory neurons 1', 12:'Tanycytes', 13:'Endothelial', 14:'Astrocytes 3', 15:'Oligodendrocytes 1', 16:'Inhibitory neurons 2', 17:'T cells', 18:'Oligodendrocytes 2', } adata.obs['annotation'] = ( adata.obs['seurat_clusters'] .map(old_to_new) .astype('category') ). For me, adding quotation marks to the cluster ID did the trick. I then just did. `adata.obs[""celltype""] = adata.obs.leiden.map(old_to_new)` like shown [here](https://www.sc-best-practices.org/cellular_structure/annotation.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:706,deployability,cluster,cluster,706,"> the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. > . > old_to_new = { 0:'Astrocytes 1', 1:'Glutamatergic neurons 1', 2:'Astrocytes 2', 3:'Oligodendrocytes 1', 4:'Inhibitory neurons 1', 5:'Glutamatergic neurons 2', 6:'Oligodendrocytes 1', 7:'Unknown', 8:'OPCs', 9:'Glutamatergic neurons 3', 10:'Microglia', 11:'Inhibitory neurons 1', 12:'Tanycytes', 13:'Endothelial', 14:'Astrocytes 3', 15:'Oligodendrocytes 1', 16:'Inhibitory neurons 2', 17:'T cells', 18:'Oligodendrocytes 2', } adata.obs['annotation'] = ( adata.obs['seurat_clusters'] .map(old_to_new) .astype('category') ). For me, adding quotation marks to the cluster ID did the trick. I then just did. `adata.obs[""celltype""] = adata.obs.leiden.map(old_to_new)` like shown [here](https://www.sc-best-practices.org/cellular_structure/annotation.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:51,interoperability,format,formating,51,"> the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. > . > old_to_new = { 0:'Astrocytes 1', 1:'Glutamatergic neurons 1', 2:'Astrocytes 2', 3:'Oligodendrocytes 1', 4:'Inhibitory neurons 1', 5:'Glutamatergic neurons 2', 6:'Oligodendrocytes 1', 7:'Unknown', 8:'OPCs', 9:'Glutamatergic neurons 3', 10:'Microglia', 11:'Inhibitory neurons 1', 12:'Tanycytes', 13:'Endothelial', 14:'Astrocytes 3', 15:'Oligodendrocytes 1', 16:'Inhibitory neurons 2', 17:'T cells', 18:'Oligodendrocytes 2', } adata.obs['annotation'] = ( adata.obs['seurat_clusters'] .map(old_to_new) .astype('category') ). For me, adding quotation marks to the cluster ID did the trick. I then just did. `adata.obs[""celltype""] = adata.obs.leiden.map(old_to_new)` like shown [here](https://www.sc-best-practices.org/cellular_structure/annotation.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:846,reliability,pra,practices,846,"> the below worked for me, I think the Python dict formating has changed. Notice I am also merging clusters by assigning them the same name. > . > old_to_new = { 0:'Astrocytes 1', 1:'Glutamatergic neurons 1', 2:'Astrocytes 2', 3:'Oligodendrocytes 1', 4:'Inhibitory neurons 1', 5:'Glutamatergic neurons 2', 6:'Oligodendrocytes 1', 7:'Unknown', 8:'OPCs', 9:'Glutamatergic neurons 3', 10:'Microglia', 11:'Inhibitory neurons 1', 12:'Tanycytes', 13:'Endothelial', 14:'Astrocytes 3', 15:'Oligodendrocytes 1', 16:'Inhibitory neurons 2', 17:'T cells', 18:'Oligodendrocytes 2', } adata.obs['annotation'] = ( adata.obs['seurat_clusters'] .map(old_to_new) .astype('category') ). For me, adding quotation marks to the cluster ID did the trick. I then just did. `adata.obs[""celltype""] = adata.obs.leiden.map(old_to_new)` like shown [here](https://www.sc-best-practices.org/cellular_structure/annotation.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/926:180,availability,error,error,180,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:259,energy efficiency,reduc,reduce,259,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:98,interoperability,specif,specify,98,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:30,modifiability,paramet,parameter,30,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:180,performance,error,error,180,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:180,safety,error,error,180,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:266,safety,compl,complexity,266,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:266,security,compl,complexity,266,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:147,usability,behavi,behavior,147,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:180,usability,error,error,180,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:11,availability,error,error,11,"I think an error should be thrown, thus the user can figure out what to do. . @nh3 I don't understand the part about the `different summarization`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:11,performance,error,error,11,"I think an error should be thrown, thus the user can figure out what to do. . @nh3 I don't understand the part about the `different summarization`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:11,safety,error,error,11,"I think an error should be thrown, thus the user can figure out what to do. . @nh3 I don't understand the part about the `different summarization`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:91,testability,understand,understand,91,"I think an error should be thrown, thus the user can figure out what to do. . @nh3 I don't understand the part about the `different summarization`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:11,usability,error,error,11,"I think an error should be thrown, thus the user can figure out what to do. . @nh3 I don't understand the part about the `different summarization`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:44,usability,user,user,44,"I think an error should be thrown, thus the user can figure out what to do. . @nh3 I don't understand the part about the `different summarization`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/pull/927:2,safety,test,tested,2,I tested this. 🙂,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/927:2,testability,test,tested,2,I tested this. 🙂,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/928:109,integrability,event,eventually,109,"@flying-sheep do you still know the context of this PR? Seems small enough to be able to rebase and merge it eventually. Else, I'd close it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/928
https://github.com/scverse/scanpy/pull/928:36,testability,context,context,36,"@flying-sheep do you still know the context of this PR? Seems small enough to be able to rebase and merge it eventually. Else, I'd close it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/928
https://github.com/scverse/scanpy/pull/928:131,usability,close,close,131,"@flying-sheep do you still know the context of this PR? Seems small enough to be able to rebase and merge it eventually. Else, I'd close it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/928
https://github.com/scverse/scanpy/issues/929:79,performance,perform,performance,79,There are quite a few questions that need to be answered first:. 1. What's the performance difference here? 2. Numpy only uses its internal code in the off chance that `#ifndef HAVE_LOG1P`: In which circumstances does accuracy suffer when using that naive implementation? 3. Does numba.vectorize handle sparse matrices?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:213,reliability,doe,does,213,There are quite a few questions that need to be answered first:. 1. What's the performance difference here? 2. Numpy only uses its internal code in the off chance that `#ifndef HAVE_LOG1P`: In which circumstances does accuracy suffer when using that naive implementation? 3. Does numba.vectorize handle sparse matrices?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:275,reliability,Doe,Does,275,There are quite a few questions that need to be answered first:. 1. What's the performance difference here? 2. Numpy only uses its internal code in the off chance that `#ifndef HAVE_LOG1P`: In which circumstances does accuracy suffer when using that naive implementation? 3. Does numba.vectorize handle sparse matrices?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:79,usability,perform,performance,79,There are quite a few questions that need to be answered first:. 1. What's the performance difference here? 2. Numpy only uses its internal code in the off chance that `#ifndef HAVE_LOG1P`: In which circumstances does accuracy suffer when using that naive implementation? 3. Does numba.vectorize handle sparse matrices?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/pull/931:202,deployability,log,logarithm,202,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:90,performance,perform,performance,90,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:67,safety,Except,Except,67,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:202,safety,log,logarithm,202,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:202,security,log,logarithm,202,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:202,testability,log,logarithm,202,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:90,usability,perform,performance,90,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:77,energy efficiency,estimat,estimates,77,"Let's please test this thoroughly, I'm not sure about how stable fold change estimates are in base 2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:13,safety,test,test,13,"Let's please test this thoroughly, I'm not sure about how stable fold change estimates are in base 2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:13,testability,test,test,13,"Let's please test this thoroughly, I'm not sure about how stable fold change estimates are in base 2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:373,availability,operat,operations,373,"Also, I think when we save the log1p base info here `data.uns['log1p'] = {'base': base}`, we should always make the base more explicit even if it's `None`. Because if we change the default to base 2 at some point (which I'd really like), then anndatas saved before and after this change would refer to different defaults so exponentiations will be incorrect in LFC and HVG operations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/pull/931:0,energy efficiency,Current,Currently,0,"Currently it’s only an option, and nowhere the default, so I think having it in can’t hurt, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931
https://github.com/scverse/scanpy/issues/932:18,availability,cluster,clustering,18,"For heatmaps, the clustering is done on the values of the heatmap's cells. What values would be used for clustering the violin plots?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:105,availability,cluster,clustering,105,"For heatmaps, the clustering is done on the values of the heatmap's cells. What values would be used for clustering the violin plots?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:18,deployability,cluster,clustering,18,"For heatmaps, the clustering is done on the values of the heatmap's cells. What values would be used for clustering the violin plots?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:105,deployability,cluster,clustering,105,"For heatmaps, the clustering is done on the values of the heatmap's cells. What values would be used for clustering the violin plots?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:4,energy efficiency,heat,heatmaps,4,"For heatmaps, the clustering is done on the values of the heatmap's cells. What values would be used for clustering the violin plots?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:58,energy efficiency,heat,heatmap,58,"For heatmaps, the clustering is done on the values of the heatmap's cells. What values would be used for clustering the violin plots?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:142,availability,cluster,clustering,142,"If I understand correctly, for the violin plots and in general for any plot, the idea would be to order the genes according to a hierarchical clustering. Actually, the computation of a dendrogram for the genes is not too difficult, what is not so easy is to accomodate a dendrogram on the figures. That requires lot of changes to the code. . Maybe just ordering the genes would be useful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:142,deployability,cluster,clustering,142,"If I understand correctly, for the violin plots and in general for any plot, the idea would be to order the genes according to a hierarchical clustering. Actually, the computation of a dendrogram for the genes is not too difficult, what is not so easy is to accomodate a dendrogram on the figures. That requires lot of changes to the code. . Maybe just ordering the genes would be useful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:5,testability,understand,understand,5,"If I understand correctly, for the violin plots and in general for any plot, the idea would be to order the genes according to a hierarchical clustering. Actually, the computation of a dendrogram for the genes is not too difficult, what is not so easy is to accomodate a dendrogram on the figures. That requires lot of changes to the code. . Maybe just ordering the genes would be useful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:88,availability,cluster,cluster,88,I would like to ask how I can order the genes if they have different expression in each cluster? Should I order the genes based in overall expression?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:88,deployability,cluster,cluster,88,I would like to ask how I can order the genes if they have different expression in each cluster? Should I order the genes based in overall expression?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/933:242,availability,cluster,clusterings,242,"Without knowing more about the file, I'm guessing you're going to want to do something like:. ```python. # Assuming these are the tsne coordinates. adata.obsm[""X_tsne""] = adata.obs[[""Embeddings_X"", ""Embeddings_Y""]] . sc.pl.tsne(adata, color=""clusterings""). ```. After setting this you could probably save this file, then use `cellxgene` to view it more easily. . To subset, do something like:. ```python. subset = adata[adata.obs[""clusterings""].isin(cluster_ids), :]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:431,availability,cluster,clusterings,431,"Without knowing more about the file, I'm guessing you're going to want to do something like:. ```python. # Assuming these are the tsne coordinates. adata.obsm[""X_tsne""] = adata.obs[[""Embeddings_X"", ""Embeddings_Y""]] . sc.pl.tsne(adata, color=""clusterings""). ```. After setting this you could probably save this file, then use `cellxgene` to view it more easily. . To subset, do something like:. ```python. subset = adata[adata.obs[""clusterings""].isin(cluster_ids), :]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:242,deployability,cluster,clusterings,242,"Without knowing more about the file, I'm guessing you're going to want to do something like:. ```python. # Assuming these are the tsne coordinates. adata.obsm[""X_tsne""] = adata.obs[[""Embeddings_X"", ""Embeddings_Y""]] . sc.pl.tsne(adata, color=""clusterings""). ```. After setting this you could probably save this file, then use `cellxgene` to view it more easily. . To subset, do something like:. ```python. subset = adata[adata.obs[""clusterings""].isin(cluster_ids), :]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:431,deployability,cluster,clusterings,431,"Without knowing more about the file, I'm guessing you're going to want to do something like:. ```python. # Assuming these are the tsne coordinates. adata.obsm[""X_tsne""] = adata.obs[[""Embeddings_X"", ""Embeddings_Y""]] . sc.pl.tsne(adata, color=""clusterings""). ```. After setting this you could probably save this file, then use `cellxgene` to view it more easily. . To subset, do something like:. ```python. subset = adata[adata.obs[""clusterings""].isin(cluster_ids), :]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:366,integrability,sub,subset,366,"Without knowing more about the file, I'm guessing you're going to want to do something like:. ```python. # Assuming these are the tsne coordinates. adata.obsm[""X_tsne""] = adata.obs[[""Embeddings_X"", ""Embeddings_Y""]] . sc.pl.tsne(adata, color=""clusterings""). ```. After setting this you could probably save this file, then use `cellxgene` to view it more easily. . To subset, do something like:. ```python. subset = adata[adata.obs[""clusterings""].isin(cluster_ids), :]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:405,integrability,sub,subset,405,"Without knowing more about the file, I'm guessing you're going to want to do something like:. ```python. # Assuming these are the tsne coordinates. adata.obsm[""X_tsne""] = adata.obs[[""Embeddings_X"", ""Embeddings_Y""]] . sc.pl.tsne(adata, color=""clusterings""). ```. After setting this you could probably save this file, then use `cellxgene` to view it more easily. . To subset, do something like:. ```python. subset = adata[adata.obs[""clusterings""].isin(cluster_ids), :]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:135,interoperability,coordinat,coordinates,135,"Without knowing more about the file, I'm guessing you're going to want to do something like:. ```python. # Assuming these are the tsne coordinates. adata.obsm[""X_tsne""] = adata.obs[[""Embeddings_X"", ""Embeddings_Y""]] . sc.pl.tsne(adata, color=""clusterings""). ```. After setting this you could probably save this file, then use `cellxgene` to view it more easily. . To subset, do something like:. ```python. subset = adata[adata.obs[""clusterings""].isin(cluster_ids), :]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/935:74,deployability,releas,released,74,@LuckyMD is your fix (https://github.com/theislab/scanpy/pull/824) in the released scanpy or still only on master?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:143,availability,error,error,143,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:320,integrability,batch,batches,320,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:469,integrability,batch,batches,469,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:481,interoperability,heterogen,heterogeneous,481,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:270,modifiability,scenario,scenario,270,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:304,modifiability,variab,variable,304,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:143,performance,error,error,143,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:320,performance,batch,batches,320,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:469,performance,batch,batches,469,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:143,safety,error,error,143,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:143,usability,error,error,143,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:198,availability,down,down,198,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:94,integrability,batch,batches,94,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:171,integrability,batch,batches,171,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:210,integrability,batch,batch,210,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:342,integrability,batch,batch,342,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:67,interoperability,share,shared,67,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:157,interoperability,share,shared,157,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:94,performance,batch,batches,94,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:171,performance,batch,batches,171,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:210,performance,batch,batch,210,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:342,performance,batch,batch,342,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:. `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:19,deployability,depend,depends,19,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:615,deployability,depend,depending,615,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:19,integrability,depend,depends,19,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:90,integrability,batch,batch,90,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:151,integrability,batch,batch,151,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:313,integrability,batch,batches,313,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:434,integrability,batch,batches,434,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:470,integrability,batch,batches,470,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:615,integrability,depend,depending,615,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:341,interoperability,share,shared,341,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:420,interoperability,share,shared,420,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:456,interoperability,share,shared,456,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:19,modifiability,depend,depends,19,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:52,modifiability,variab,variable,52,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:97,modifiability,paramet,parameter,97,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:158,modifiability,paramet,parameter,158,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:615,modifiability,depend,depending,615,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:90,performance,batch,batch,90,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:151,performance,batch,batch,151,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:313,performance,batch,batches,313,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:434,performance,batch,batches,434,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:470,performance,batch,batches,470,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:19,safety,depend,depends,19,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:615,safety,depend,depending,615,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:19,testability,depend,depends,19,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:615,testability,depend,depending,615,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:12,testability,understand,understand,12,"OK. So If I understand it correct, when batch_key is used, adata.var['highly_variable'] is just adata.var['highly_variable_genes_intersection'] ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:137,integrability,batch,batches,137,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:279,integrability,batch,batches,279,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:367,integrability,batch,batches,367,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:118,interoperability,share,shared,118,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:347,interoperability,share,shared,347,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:137,performance,batch,batches,137,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:279,performance,batch,batches,279,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:367,performance,batch,batches,367,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:385,safety,safe,safe,385,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/936:5,deployability,version,version,5,"What version of scanpy are you on? This looks like 👹 #666 👹, but I thought that should be solved by 1.4.4.post1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:5,integrability,version,version,5,"What version of scanpy are you on? This looks like 👹 #666 👹, but I thought that should be solved by 1.4.4.post1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:5,modifiability,version,version,5,"What version of scanpy are you on? This looks like 👹 #666 👹, but I thought that should be solved by 1.4.4.post1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:62,usability,tool,tools,62,"I was using 1.4.4.post1. My workaround is: . ```. from scanpy.tools._utils import get_init_pos_from_paga as init. sc.tl.umap(adata,init_pos=init(adata)). sc.pl.umap(adata,color='louvain'). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:77,availability,error,error,77,"Ah, I think this was reported before in #769. Would you mind checking if the error still occurs on master?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:77,performance,error,error,77,"Ah, I think this was reported before in #769. Would you mind checking if the error still occurs on master?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:77,safety,error,error,77,"Ah, I think this was reported before in #769. Would you mind checking if the error still occurs on master?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:77,usability,error,error,77,"Ah, I think this was reported before in #769. Would you mind checking if the error still occurs on master?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/938:75,reliability,doe,doesn,75,"What if we didn't store it by default, and just computed it on the fly? It doesn't seem like it would be particularly computationally expensive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:150,availability,cluster,cluster,150,"I just needed to use centroids for some plotting, and found a pretty straightforward solution. This function will add labels to the centroids of each cluster, but uses [adjustText](https://github.com/Phlya/adjustText) so they don't overlap:. ```python. def gen_mpl_labels(adata, groupby, exclude=()):. medians = {}. for g, g_idx in adata.obs.groupby(groupby).groups.items():. if g in exclude:. continue. medians[g] = np.median(adata[g_idx].obsm[""X_umap""], axis=0). . texts = [plt.text(x=x, y=y, s=k) for k, (x, y) in medians.items()]. adjust_text(texts, text_from_points=False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:150,deployability,cluster,cluster,150,"I just needed to use centroids for some plotting, and found a pretty straightforward solution. This function will add labels to the centroids of each cluster, but uses [adjustText](https://github.com/Phlya/adjustText) so they don't overlap:. ```python. def gen_mpl_labels(adata, groupby, exclude=()):. medians = {}. for g, g_idx in adata.obs.groupby(groupby).groups.items():. if g in exclude:. continue. medians[g] = np.median(adata[g_idx].obsm[""X_umap""], axis=0). . texts = [plt.text(x=x, y=y, s=k) for k, (x, y) in medians.items()]. adjust_text(texts, text_from_points=False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:394,deployability,continu,continue,394,"I just needed to use centroids for some plotting, and found a pretty straightforward solution. This function will add labels to the centroids of each cluster, but uses [adjustText](https://github.com/Phlya/adjustText) so they don't overlap:. ```python. def gen_mpl_labels(adata, groupby, exclude=()):. medians = {}. for g, g_idx in adata.obs.groupby(groupby).groups.items():. if g in exclude:. continue. medians[g] = np.median(adata[g_idx].obsm[""X_umap""], axis=0). . texts = [plt.text(x=x, y=y, s=k) for k, (x, y) in medians.items()]. adjust_text(texts, text_from_points=False). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/pull/941:64,integrability,Compon,Components,64,"A quick, naive question: What is the advantage of ICA over PCA? Components 1,3,5,7 of the ICA grid plot don't look particularly independent to me...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:64,interoperability,Compon,Components,64,"A quick, naive question: What is the advantage of ICA over PCA? Components 1,3,5,7 of the ICA grid plot don't look particularly independent to me...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:64,modifiability,Compon,Components,64,"A quick, naive question: What is the advantage of ICA over PCA? Components 1,3,5,7 of the ICA grid plot don't look particularly independent to me...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:671,availability,cluster,clusters,671,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:917,availability,cluster,cluster,917,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:671,deployability,cluster,clusters,671,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:917,deployability,cluster,cluster,917,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:970,energy efficiency,load,loadings,970,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:412,integrability,compon,components,412,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
