id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/361:244,integrability,configur,configuration-,244,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:244,modifiability,configur,configuration-,244,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:84,performance,CPU,CPU,84,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:230,performance,cpu,cpus-and-gpus-configuration-,230,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:376,performance,cpu,cpu,376,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:867,performance,cpu,cpu,867,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:1258,performance,cpu,cpu,1258,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:1575,performance,cpu,cpu,1575,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:787,reliability,checkpoint,checkpoint,787,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:511,safety,input,input,511,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:244,security,configur,configuration-,244,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:804,security,model,models,804,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:815,security,model,model,815,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:511,usability,input,input,511,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:29,usability,close,close,29,"@aderzelle Now I'll actually close this issue. . If you try it out, let me know whether it worked or not. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:58,energy efficiency,cpu,cpus,58,"Hi @aderzelle ,. Another approach is to use `docker run --cpus=$THREADS` to constrain the number of CPUs. Hopefully this approach should work even if the previous one didn't.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:100,energy efficiency,CPU,CPUs,100,"Hi @aderzelle ,. Another approach is to use `docker run --cpus=$THREADS` to constrain the number of CPUs. Hopefully this approach should work even if the previous one didn't.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:58,performance,cpu,cpus,58,"Hi @aderzelle ,. Another approach is to use `docker run --cpus=$THREADS` to constrain the number of CPUs. Hopefully this approach should work even if the previous one didn't.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:100,performance,CPU,CPUs,100,"Hi @aderzelle ,. Another approach is to use `docker run --cpus=$THREADS` to constrain the number of CPUs. Hopefully this approach should work even if the previous one didn't.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/362:457,testability,understand,understand,457,"Hi @loipf , . You can use `--help` with the different binaries, for example:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help. ```. To see the various binaries, you can use:. ```. docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/. ```. to list all the binaries. Then, for example, you can run:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help. ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:29,usability,help,help,29,"Hi @loipf , . You can use `--help` with the different binaries, for example:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help. ```. To see the various binaries, you can use:. ```. docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/. ```. to list all the binaries. Then, for example, you can run:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help. ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:158,usability,help,help,158,"Hi @loipf , . You can use `--help` with the different binaries, for example:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help. ```. To see the various binaries, you can use:. ```. docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/. ```. to list all the binaries. Then, for example, you can run:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help. ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:421,usability,help,help,421,"Hi @loipf , . You can use `--help` with the different binaries, for example:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help. ```. To see the various binaries, you can use:. ```. docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/. ```. to list all the binaries. Then, for example, you can run:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help. ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:524,usability,clear,clear,524,"Hi @loipf , . You can use `--help` with the different binaries, for example:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help. ```. To see the various binaries, you can use:. ```. docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/. ```. to list all the binaries. Then, for example, you can run:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help. ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:530,usability,help,help,530,"Hi @loipf , . You can use `--help` with the different binaries, for example:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help. ```. To see the various binaries, you can use:. ```. docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/. ```. to list all the binaries. Then, for example, you can run:. ```. docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help. ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:75,deployability,releas,release,75,"Hi @loipf . I've made changes in internal code. It'll come out in the next release. After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:99,deployability,releas,release,99,"Hi @loipf . I've made changes in internal code. It'll come out in the next release. After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:146,usability,feedback,feedback,146,"Hi @loipf . I've made changes in internal code. It'll come out in the next release. After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:57,usability,document,documented,57,very nice! makes it a lot easier for new people and well documented,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:23,usability,help,helpful,23,Thanks @loipf for your helpful suggestion :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/pull/363:135,deployability,pipelin,pipeline,135,"Hi! We would like to propose optional OpenVINO backend for `call_variants` step. Do you accept external PRs? Also, this PR has Actions pipeline. Feel free to enable by https://github.com/google/deepvariant/actions to run it. /cc @pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:135,integrability,pipelin,pipeline,135,"Hi! We would like to propose optional OpenVINO backend for `call_variants` step. Do you accept external PRs? Also, this PR has Actions pipeline. Feel free to enable by https://github.com/google/deepvariant/actions to run it. /cc @pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:674,deployability,releas,release,674,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1378,deployability,updat,updates,1378,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:75,integrability,sub,substantial,75,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1182,interoperability,incompatib,incompatible,1182,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1033,performance,perform,perform,1033,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1588,performance,perform,performance,1588,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1378,safety,updat,updates,1378,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:121,security,modif,modifications,121,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:645,security,auth,authors,645,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1378,security,updat,updates,1378,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1725,security,assess,assessment,1725,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:725,testability,understand,understand,725,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:903,testability,understand,understand,903,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1445,testability,plan,planning,1445,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1690,testability,understand,understand,1690,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:717,usability,help,help,717,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1033,usability,perform,perform,1033,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1360,usability,interact,interact,1360,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1588,usability,perform,performance,1588,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1672,usability,help,helpful,1672,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:491,deployability,releas,release,491,"@AndrewCarroll, many thanks for such quick response! > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1789,deployability,updat,updates,1789,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2005,deployability,log,logic,2005,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2318,deployability,build,build,2318,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2422,deployability,log,logs,2422,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1147,energy efficiency,optim,optimizations,1147,"s. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2040,energy efficiency,reduc,reduce,2040,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1121,integrability,pub,publicly,1121,"not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1115,interoperability,share,share,1115," we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1408,interoperability,incompatib,incompatible,1408,"th community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2047,interoperability,conflict,conflicts,2047,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1050,performance,time,time,1050," > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce confl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1147,performance,optimiz,optimizations,1147,"s. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1259,performance,perform,perform,1259,"nto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2168,performance,perform,perform,2168,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2306,reliability,doe,does,2306,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1608,safety,test,test,1608,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1789,safety,updat,updates,1789,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1975,safety,isol,isolate,1975,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2005,safety,log,logic,2005,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2184,safety,test,tests,2184,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2422,safety,log,logs,2422,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:462,security,auth,authors,462,"@AndrewCarroll, many thanks for such quick response! > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1789,security,updat,updates,1789,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1975,security,iso,isolate,1975,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2005,security,log,logic,2005,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2422,security,log,logs,2422,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:726,testability,understand,understand,726,"@AndrewCarroll, many thanks for such quick response! > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:904,testability,understand,understand,904,"@AndrewCarroll, many thanks for such quick response! > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1608,testability,test,test,1608,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1856,testability,plan,planning,1856,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1975,testability,isol,isolate,1975,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2005,testability,log,logic,2005,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2184,testability,test,tests,2184,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2422,testability,log,logs,2422,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:718,usability,help,help,718,"@AndrewCarroll, many thanks for such quick response! > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:982,usability,efficien,efficiency,982,"@AndrewCarroll, many thanks for such quick response! > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1001,usability,learn,learning,1001,"ewCarroll, many thanks for such quick response! > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1259,usability,perform,perform,1259,"nto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1673,usability,user,users,1673,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1771,usability,interact,interact,1771,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2168,usability,perform,perform,2168,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2287,usability,workflow,workflows,2287,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:556,deployability,updat,update,556,"Hi @dkurt ,. thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement. 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:696,deployability,continu,continue,696,"Hi @dkurt ,. thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement. 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:107,energy efficiency,current,current,107,"Hi @dkurt ,. thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement. 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:556,safety,updat,update,556,"Hi @dkurt ,. thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement. 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:556,security,updat,update,556,"Hi @dkurt ,. thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement. 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:136,testability,plan,planning,136,"Hi @dkurt ,. thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement. 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:144,safety,compl,complexity,144,"And one more follow up on my comment above for @dkurt ,. When we consider adding code, it's important for me to trade off between how much more complexity it adds (for any future developer on the team) and how much benefit it brings (runtime or accuracy improvement). . So, if you have more context on what's the impact of this PR, I'll also take into account and adjust my priority as needed. . Thanks again for sending this PR. Looking through it, I'm impressed with the amount of thoughtfulness and understanding of our codebase!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:144,security,compl,complexity,144,"And one more follow up on my comment above for @dkurt ,. When we consider adding code, it's important for me to trade off between how much more complexity it adds (for any future developer on the team) and how much benefit it brings (runtime or accuracy improvement). . So, if you have more context on what's the impact of this PR, I'll also take into account and adjust my priority as needed. . Thanks again for sending this PR. Looking through it, I'm impressed with the amount of thoughtfulness and understanding of our codebase!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:196,security,team,team,196,"And one more follow up on my comment above for @dkurt ,. When we consider adding code, it's important for me to trade off between how much more complexity it adds (for any future developer on the team) and how much benefit it brings (runtime or accuracy improvement). . So, if you have more context on what's the impact of this PR, I'll also take into account and adjust my priority as needed. . Thanks again for sending this PR. Looking through it, I'm impressed with the amount of thoughtfulness and understanding of our codebase!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:291,testability,context,context,291,"And one more follow up on my comment above for @dkurt ,. When we consider adding code, it's important for me to trade off between how much more complexity it adds (for any future developer on the team) and how much benefit it brings (runtime or accuracy improvement). . So, if you have more context on what's the impact of this PR, I'll also take into account and adjust my priority as needed. . Thanks again for sending this PR. Looking through it, I'm impressed with the amount of thoughtfulness and understanding of our codebase!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:502,testability,understand,understanding,502,"And one more follow up on my comment above for @dkurt ,. When we consider adding code, it's important for me to trade off between how much more complexity it adds (for any future developer on the team) and how much benefit it brings (runtime or accuracy improvement). . So, if you have more context on what's the impact of this PR, I'll also take into account and adjust my priority as needed. . Thanks again for sending this PR. Looking through it, I'm impressed with the amount of thoughtfulness and understanding of our codebase!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:848,availability,slo,sloppy,848,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:15,deployability,updat,update,15,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:116,deployability,build,building,116,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:141,deployability,version,version,141,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:166,deployability,observ,observe,166,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:371,deployability,build,build,371,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:490,deployability,log,log,490,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:704,deployability,updat,updating,704,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1278,deployability,updat,updating,1278,"t/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2602,deployability,log,log,2602,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:938,energy efficiency,core,core,938,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1425,energy efficiency,optim,optimizations,1425,"ants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:141,integrability,version,version,141,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1374,integrability,batch,batch,1374,"the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1685,integrability,batch,batches,1685,"structions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that str",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1805,integrability,batch,batches,1805,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1925,integrability,batch,batches,1925,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2045,integrability,batch,batches,2045,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2166,integrability,batch,batches,2166,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2287,integrability,batch,batches,2287,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2408,integrability,batch,batches,2408,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:205,interoperability,Specif,Specifically,205,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:141,modifiability,version,version,141,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1038,performance,Tune,Tune,1038," you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1374,performance,batch,batch,1374,"the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1425,performance,optimiz,optimizations,1425,"ants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1685,performance,batch,batches,1685,"structions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that str",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1805,performance,batch,batches,1805,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1925,performance,batch,batches,1925,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2045,performance,batch,batches,2045,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2166,performance,batch,batches,2166,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2287,performance,batch,batches,2287,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2408,performance,batch,batches,2408,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2508,performance,time,timestamp,2508,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2713,performance,time,time,2713,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:848,reliability,slo,sloppy,848,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:15,safety,updat,update,15,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:490,safety,log,log,490,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:704,safety,updat,updating,704,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1278,safety,updat,updating,1278,"t/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2602,safety,log,log,2602,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:15,security,updat,update,15,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:490,security,log,log,490,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:704,security,updat,updating,704,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1278,security,updat,updating,1278,"t/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2602,security,log,log,2602,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:166,testability,observ,observe,166,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:490,testability,log,log,490,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2602,testability,log,log,2602,"parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]. ... ```. The strange thing is: . There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? . But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:178,usability,behavi,behavior,178,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```. ... W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>. 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>. W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>. I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz. I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]. I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:35,energy efficiency,current,current,35,"@pichuan, It might be an effect of current implementation - all the processing is done at iterator initialization and then `__getitem__` returns predicted results without delay. We will take a look at overall efficiency and check what can be improved here, thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:145,energy efficiency,predict,predicted,145,"@pichuan, It might be an effect of current implementation - all the processing is done at iterator initialization and then `__getitem__` returns predicted results without delay. We will take a look at overall efficiency and check what can be improved here, thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:145,safety,predict,predicted,145,"@pichuan, It might be an effect of current implementation - all the processing is done at iterator initialization and then `__getitem__` returns predicted results without delay. We will take a look at overall efficiency and check what can be improved here, thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:209,usability,efficien,efficiency,209,"@pichuan, It might be an effect of current implementation - all the processing is done at iterator initialization and then `__getitem__` returns predicted results without delay. We will take a look at overall efficiency and check what can be improved here, thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1886,availability,Down,Download,1886,"on is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:52,deployability,build,build,52,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:993,deployability,pipelin,pipeline,993,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1031,deployability,Build,Build,1031,"delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1063,deployability,build,build-test,1063,"t so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1143,deployability,build,build-test,1143," numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PAT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1166,deployability,build,build,1166,"evCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1599,deployability,patch,patches,1599,"05s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2188,deployability,Instal,Install,2188,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2395,deployability,instal,install,2395,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2405,deployability,upgrad,upgrade,2405,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:418,energy efficiency,cpu,cpu,418,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1752,energy efficiency,model,models,1752,".97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1763,energy efficiency,model,model,1763,"r 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1774,energy efficiency,model,model,1774," (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA128",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:993,integrability,pipelin,pipeline,993,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1380,integrability,repositor,repository,1380,"dge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAM",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1380,interoperability,repositor,repository,1380,"dge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAM",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:83,modifiability,portab,portable,83,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2405,modifiability,upgrad,upgrade,2405,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:418,performance,cpu,cpu,418,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:932,performance,time,time,932,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1900,performance,parallel,parallel,1900,"nterpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1952,performance,parallel,parallel,1952," us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2128,performance,parallel,parallel,2128,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2156,performance,parallel,parallel,2156,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:237,safety,test,testdata,237,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1069,safety,test,test,1069,"o it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1149,safety,test,test,1149,"mbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1599,safety,patch,patches,1599,"05s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1994,safety,permiss,permissions,1994,"e. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2493,safety,test,testdata,2493,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1599,security,patch,patches,1599,"05s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1752,security,model,models,1752,".97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1763,security,model,model,1763,"r 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1774,security,model,model,1774," (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA128",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:237,testability,test,testdata,237,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:963,testability,understand,understand,963,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1069,testability,test,test,1069,"o it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1149,testability,test,test,1149,"mbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2493,testability,test,testdata,2493,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2733,testability,unit,unittest,2733,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions). ```bash. wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb . dpkg -x parallel_20161222-1_all.deb parallel. export PATH=$HOME/parallel/usr/bin:$PATH. ```. 5. Install TensorFlow MKL-DNN. ```bash. WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl. wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}"". pip3 install --upgrade ""/tmp/${WHEEL_NAME}"". ```. 6. Run. ```bash. export INPUT_DIR=""${PWD}/quickstart-testdata"". export OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \. --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=${OUTPUT_DIR}/output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \. --call_variants_extra_args=""use_openvino=True"" \. --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:542,usability,user,user,542,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:592,usability,user,user,592,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:642,usability,user,user,642,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:706,usability,user,user,706,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:763,usability,user,user,763,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:829,usability,user,user,829,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:919,usability,user,user,919,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:952,usability,help,help,952,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |. | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,. ```bash. ./build_release_binaries.sh. tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*. tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*. ```. 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries. ```bash. git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1. cd deepvariant. tar -xf bazel-deepvariant.tar.gz. tar -xf bazel-genfiles.tar.gz. ```. 3. Apply some patches to resolve local paths:. ```bash. sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py. sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py. ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts. ```. 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:14,energy efficiency,optim,optimizations,14,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:279,energy efficiency,cpu,cpu,279,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:14,performance,optimiz,optimizations,14,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:279,performance,cpu,cpu,279,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:484,performance,time,times,484,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:32,safety,test,tested,32,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:660,safety,input,input,660,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:32,testability,test,tested,32,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:660,usability,input,input,660,"Hi! Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch? | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |. | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./reference/GRCh38_no_alt_analysis_set.fasta \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr20"" \. --call_variants_extra_args=""use_openvino=True"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:752,energy efficiency,cpu,cpu-only-machine-on-google-cloud-platform,752,"Hi @dkurt , chr20 is better than the Quick Start! In v1.0, we consolidate all our runtime comparison into this doc: https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md which shows the runtime on all chromosomes. I think the difference will be more noticeable if you compare to one of the bigger ones, for example WGS:. ```. # WGS (should take about 7 hours). curl -O https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh. bash run_wgs_case_study_docker.sh. ```. I will certainly plan to compare on that so I know how this works in my setting. . @dkurt I have a question for you: If I get a machine like this: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform , do you think I will be able to see the same improvement that you're seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:785,interoperability,platform,platform,785,"Hi @dkurt , chr20 is better than the Quick Start! In v1.0, we consolidate all our runtime comparison into this doc: https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md which shows the runtime on all chromosomes. I think the difference will be more noticeable if you compare to one of the bigger ones, for example WGS:. ```. # WGS (should take about 7 hours). curl -O https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh. bash run_wgs_case_study_docker.sh. ```. I will certainly plan to compare on that so I know how this works in my setting. . @dkurt I have a question for you: If I get a machine like this: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform , do you think I will be able to see the same improvement that you're seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:752,performance,cpu,cpu-only-machine-on-google-cloud-platform,752,"Hi @dkurt , chr20 is better than the Quick Start! In v1.0, we consolidate all our runtime comparison into this doc: https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md which shows the runtime on all chromosomes. I think the difference will be more noticeable if you compare to one of the bigger ones, for example WGS:. ```. # WGS (should take about 7 hours). curl -O https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh. bash run_wgs_case_study_docker.sh. ```. I will certainly plan to compare on that so I know how this works in my setting. . @dkurt I have a question for you: If I get a machine like this: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform , do you think I will be able to see the same improvement that you're seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:532,testability,plan,plan,532,"Hi @dkurt , chr20 is better than the Quick Start! In v1.0, we consolidate all our runtime comparison into this doc: https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md which shows the runtime on all chromosomes. I think the difference will be more noticeable if you compare to one of the bigger ones, for example WGS:. ```. # WGS (should take about 7 hours). curl -O https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh. bash run_wgs_case_study_docker.sh. ```. I will certainly plan to compare on that so I know how this works in my setting. . @dkurt I have a question for you: If I get a machine like this: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform , do you think I will be able to see the same improvement that you're seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:738,usability,command,command-for-a-cpu-only-machine-on-google-cloud-platform,738,"Hi @dkurt , chr20 is better than the Quick Start! In v1.0, we consolidate all our runtime comparison into this doc: https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md which shows the runtime on all chromosomes. I think the difference will be more noticeable if you compare to one of the bigger ones, for example WGS:. ```. # WGS (should take about 7 hours). curl -O https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh. bash run_wgs_case_study_docker.sh. ```. I will certainly plan to compare on that so I know how this works in my setting. . @dkurt I have a question for you: If I get a machine like this: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform , do you think I will be able to see the same improvement that you're seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:52,deployability,configurat,configuration,52,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:46,energy efficiency,core,cores,46,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:113,energy efficiency,core,cores,113,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:228,energy efficiency,cpu,cpu,228,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:458,energy efficiency,core,cores,458,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:985,energy efficiency,core,cores,985,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:52,integrability,configur,configuration,52,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:52,modifiability,configur,configuration,52,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:228,performance,cpu,cpu,228,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:583,safety,input,input,583,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:653,safety,input,input,653,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:25,security,access,access,25,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:52,security,configur,configuration,52,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:957,security,team,team,957,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:972,security,access,access,972,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:583,usability,input,input,583,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:653,usability,input,input,653,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |. |---|---|---|---|. | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |. | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash. python3 ./bazel-deepvariant/scripts/run_deepvariant.py \. --model_type=WGS \. --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \. --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \. --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \. --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \. --num_shards=16 \. --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \. --call_variants_extra_args=""use_openvino=True"". ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:41,availability,state,state,41,I have published an image for the latest state of PR at https://hub.docker.com/r/dkurtaev/deepvariant. May I ask to validate it?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7,integrability,pub,published,7,I have published an image for the latest state of PR at https://hub.docker.com/r/dkurtaev/deepvariant. May I ask to validate it?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:41,integrability,state,state,41,I have published an image for the latest state of PR at https://hub.docker.com/r/dkurtaev/deepvariant. May I ask to validate it?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:116,safety,valid,validate,116,I have published an image for the latest state of PR at https://hub.docker.com/r/dkurtaev/deepvariant. May I ask to validate it?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:116,security,validat,validate,116,I have published an image for the latest state of PR at https://hub.docker.com/r/dkurtaev/deepvariant. May I ask to validate it?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:65,usability,command,command,65,"@dkurt Happy to try it out. Do I just pull it run with a regular command like:. https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh. ? Any more flags I need to add? It seems like I'll probably need to add `--call_variants_extra_args=""use_openvino=True""`. Anything else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:246,energy efficiency,cpu,cpuinfo,246,"@pichuan, you're right, just `--call_variants_extra_args=""use_openvino=True""`. May I ask to try to run on just a single chromosome first? To check if OpenVINO works faster than TensorFlow. It not, is that possible to share `lscpu` and `cat /proc/cpuinfo` information?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:217,interoperability,share,share,217,"@pichuan, you're right, just `--call_variants_extra_args=""use_openvino=True""`. May I ask to try to run on just a single chromosome first? To check if OpenVINO works faster than TensorFlow. It not, is that possible to share `lscpu` and `cat /proc/cpuinfo` information?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:246,performance,cpu,cpuinfo,246,"@pichuan, you're right, just `--call_variants_extra_args=""use_openvino=True""`. May I ask to try to run on just a single chromosome first? To check if OpenVINO works faster than TensorFlow. It not, is that possible to share `lscpu` and `cat /proc/cpuinfo` information?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1111,availability,echo,echo,1111,"_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1779,availability,echo,echo,1779,"@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2529,availability,echo,echo,2529," \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3198,availability,echo,echo,3198," aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4154,availability,echo,echo,4154,"hr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:437,deployability,log,log,437,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1773,deployability,log,log,1773,"5,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3192,deployability,log,log,3192,",14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4148,deployability,log,log,4148,"l -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA nod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4216,deployability,log,log,4216,"untime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5209,deployability,api,apic,5209,"vino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6108,deployability,api,apicid,6108,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6128,deployability,api,apicid,6128,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6243,deployability,api,apic,6243,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6986,deployability,manag,management,6986,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4435,energy efficiency,cloud,cloud-platform,4435,"dy_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4517,energy efficiency,cloud,cloud,4517,"docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4619,energy efficiency,cpu,cpu-platform,4619,"IR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4702,energy efficiency,CPU,CPU,4702,"N}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4761,energy efficiency,CPU,CPU,4761,"pvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4781,energy efficiency,CPU,CPU,4781,"pvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4814,energy efficiency,core,core,4814,"gs=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4823,energy efficiency,Core,Core,4823,"s=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the firs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4903,energy efficiency,CPU,CPU,4903,""" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4918,energy efficiency,Model,Model,4918,"100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4929,energy efficiency,Model,Model,4929,"rusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. mode",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4958,energy efficiency,CPU,CPU,4958,"n/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4986,energy efficiency,CPU,CPU,4986,"/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5153,energy efficiency,CPU,CPU,5153,"cho ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5356,energy efficiency,cpu,cpuid,5356,"stances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5768,energy efficiency,cpu,cpuinfo,5768, On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5849,energy efficiency,cpu,cpuinfo,5849,(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf m,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5900,energy efficiency,cpu,cpu,5900, CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size :,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5916,energy efficiency,model,model,5916,odel: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignm,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5928,energy efficiency,model,model,5928,del name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. ad,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5958,energy efficiency,CPU,CPU,5958,U @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physica,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6004,energy efficiency,cpu,cpu,6004,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6079,energy efficiency,core,core,6079,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6092,energy efficiency,cpu,cpu,6092,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6096,energy efficiency,core,cores,6096,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6172,energy efficiency,cpu,cpuid,6172,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6390,energy efficiency,cpu,cpuid,6390,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6980,energy efficiency,power,power,6980,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6986,energy efficiency,manag,management,6986,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5209,integrability,api,apic,5209,"vino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6108,integrability,api,apicid,6108,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6128,integrability,api,apicid,6128,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6243,integrability,api,apic,6243,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4441,interoperability,platform,platform,4441,"docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4623,interoperability,platform,platform,4623,"}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4680,interoperability,Architectur,Architecture,4680,"iant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4835,interoperability,socket,socket,4835,"odel_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4847,interoperability,Socket,Socket,4847,"S \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5209,interoperability,api,apic,5209,"vino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6108,interoperability,api,apicid,6108,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6128,interoperability,api,apicid,6128,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6243,interoperability,api,apic,6243,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4571,performance,disk,disk-size,4571,"@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4619,performance,cpu,cpu-platform,4619,"IR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4702,performance,CPU,CPU,4702,"N}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4761,performance,CPU,CPU,4761,"pvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4781,performance,CPU,CPU,4781,"pvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4903,performance,CPU,CPU,4903,""" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4958,performance,CPU,CPU,4958,"n/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4986,performance,CPU,CPU,4986,"/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5079,performance,cach,cache,5079,"feval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. cor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5095,performance,cach,cache,5095,"e=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5110,performance,cach,cache,5110,"r1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apici",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5127,performance,cach,cache,5127,"""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial ap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5153,performance,CPU,CPU,5153,"cho ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5356,performance,cpu,cpuid,5356,"stances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5768,performance,cpu,cpuinfo,5768, On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5849,performance,cpu,cpuinfo,5849,(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf m,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5900,performance,cpu,cpu,5900, CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size :,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5958,performance,CPU,CPU,5958,U @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physica,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6004,performance,cpu,cpu,6004,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6024,performance,cach,cache,6024,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6092,performance,cpu,cpu,6092,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6172,performance,cpu,cpuid,6172,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6390,performance,cpu,cpuid,6390,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5297,reliability,rdt,rdtscp,5297," got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6331,reliability,rdt,rdtscp,6331,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2,safety,test,tested,2,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:301,safety,Test,Tested,301,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:437,safety,log,log,437,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:857,safety,test,testda,857,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:939,safety,test,testdata,939,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1177,safety,input,input,1177,"variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1488,safety,input,input,1488,"ker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1518,safety,input,input,1518,"The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:la",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1773,safety,log,log,1773,"5,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2275,safety,test,testda,2275,"in/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2357,safety,test,testdata,2357,"eepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2595,safety,input,input,2595,"f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2907,safety,input,input,2907,"r image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2937,safety,input,input,2937,"he code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3192,safety,log,log,3192,",14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3632,safety,input,input,3632,"- google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""In",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3863,safety,input,input,3863,"ions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3893,safety,input,input,3893,"\. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: Genuine",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4148,safety,log,log,4148,"l -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA nod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4216,safety,log,log,4216,"untime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6986,safety,manag,management,6986,"8. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:437,security,log,log,437,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1773,security,log,log,1773,"5,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3192,security,log,log,3192,",14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4148,security,log,log,4148,"l -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA nod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4216,security,log,log,4216,"untime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4835,security,soc,socket,4835,"odel_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4847,security,Soc,Socket,4847,"S \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4918,security,Model,Model,4918,"100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4929,security,Model,Model,4929,"rusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. mode",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5916,security,model,model,5916,odel: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignm,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5928,security,model,model,5928,del name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```. $ cat /proc/cpuinfo . processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 85. model name : Intel(R) Xeon(R) CPU @ 2.00GHz. stepping : 3. microcode : 0x1. cpu MHz : 2000.178. cache size : 39424 KB. physical id : 0. siblings : 64. core id : 0. cpu cores : 32. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 13. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities. bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa. bogomips : 4000.35. clflush size : 64. cache_alignment : 64. ad,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2,testability,test,tested,2,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:301,testability,Test,Tested,301,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:437,testability,log,log,437,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:857,testability,test,testda,857,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:939,testability,test,testdata,939,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1773,testability,log,log,1773,"5,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2275,testability,test,testda,2275,"in/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2357,testability,test,testdata,2357,"eepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3192,testability,log,log,3192,",14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4148,testability,log,log,4148,"l -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA nod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4216,testability,log,log,4216,"untime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:246,usability,command,commands,246,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:291,usability,Command,Commands,291,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:355,usability,command,command,355,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details. With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1177,usability,input,input,1177,"variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:. ----. # Commands. Tested on the same machine:. All below were done with command like:. ```. scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log. ```. with some code diffs below:. 1. Use your Docker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1488,usability,input,input,1488,"ker image, use_openvino=true. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1518,usability,input,input,1518,"The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:la",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2595,usability,input,input,2595,"f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m38.326s. real 15m12.564s. real 7m15.173s. ```. 2. Use your Docker image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2907,usability,input,input,2907,"r image, use_openvino=false. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2937,usability,input,input,2937,"he code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..78712d8 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda. aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}"". . ## Pull the docker image. -sudo docker pull google/deepvariant:""${BIN_VERSION}"". +sudo docker pull dkurtaev/deepvariant:latest. . echo ""Run DeepVariant..."". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. - google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3632,usability,input,input,3632,"- google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + dkurtaev/deepvariant:latest \. + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""In",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3863,usability,input,input,3863,"ions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3893,usability,input,input,3893,"\. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime:. ```. $ grep '^real' /tmp/open. real 7m20.986s. real 21m24.429s. real 6m32.705s. ```. 3. Use v1.0.0 image. The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: Genuine",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4327,usability,command,command,4327,". The code diff:. ```. $ git diff. diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4377,usability,USER,USER,4377,"scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh. index 3dc9712..88fb0c1 100755. --- a/scripts/run_wgs_case_study_docker.sh. +++ b/scripts/run_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4543,usability,custom,custom-,4543,"n_wgs_case_study_docker.sh. @@ -72,7 +72,7 @@ sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. - /opt/deepvariant/bin/run_deepvariant \. + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \. --model_type=WGS \. --ref=""/input/${REF}.gz"" \. --reads=""/input/${BAM}"" \. @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \. -f ""${INPUT_DIR}/${TRUTH_BED}"" \. -r ""${UNCOMPRESSED_REF}"" \. -o ""${OUTPUT_DIR}/happy.output"" \. - --engine=vcfeval. + --engine=vcfeval -l chr1. ) 2>&1 | tee ""${LOG_DIR}/happy.log"". echo ""Done."". ```. Runtime. ```. $ grep '^real' /tmp/openvino.log. real 7m26.887s. real 20m40.889s. real 6m25.257s. ```. ---. # Machine details. I got the machine with this command:. ```. gcloud compute instances create ""${USER}-openvino-expt"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""custom-64-131072"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. ## `lscpu`. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 64. On-line CPU(s) list: 0-63. Thread(s) per core: 2. Core(s) per socket: 32. Socket(s): 1. NUMA node(s): 1. Vendor ID: GenuineIntel. CPU family: 6. Model: 85. Model name: Intel(R) Xeon(R) CPU @ 2.00GHz. Stepping: 3. CPU MHz: 2000.178. BogoMIPS: 4000.35. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 1024K. L3 cache: 39424K. NUMA node0 CPU(s): 0-63. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:488,availability,checkpoint,checkpoint,488,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:845,availability,operat,operations,845,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:905,availability,operat,operations,905,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1143,availability,servic,service,1143,"I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1151,availability,servic,service,1151,"ed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1171,availability,servic,service,1171,"ants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1294,availability,servic,service,1294,"Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best pe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1302,availability,servic,service,1302,"the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performanc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2028,availability,slo,sloppy,2028,"file_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:149,deployability,observ,observed,149,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:280,deployability,log,logs,280,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1143,deployability,servic,service,1143,"I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1151,deployability,servic,service,1151,"ed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1171,deployability,servic,service,1171,"ants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1294,deployability,servic,service,1294,"Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best pe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1302,deployability,servic,service,1302,"the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performanc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1359,deployability,Version,Version,1359,"s --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1858,deployability,version,version,1858,"AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 cal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1884,deployability,updat,updating,1884,"m in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2584,deployability,version,version,2584,"6410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2610,deployability,updat,updating,2610,"cation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9033,deployability,version,version,9033,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9059,deployability,updat,updating,9059,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:505,energy efficiency,model,models,505,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:516,energy efficiency,model,model,516,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:685,energy efficiency,core,core,685,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:751,energy efficiency,optim,optimized,751,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:804,energy efficiency,CPU,CPU,804,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1014,energy efficiency,core,core,1014,"my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1059,energy efficiency,CPU,CPU,1059," results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1063,energy efficiency,Frequenc,Frequency,1063,"s in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.dat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1409,energy efficiency,core,core,1409,"ut.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2147,energy efficiency,core,core,2147,"e/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2757,energy efficiency,optim,optimizations,2757,"ow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batche",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9206,energy efficiency,optim,optimizations,9206,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1143,integrability,servic,service,1143,"I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1151,integrability,servic,service,1151,"ed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1171,integrability,servic,service,1171,"ants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1294,integrability,servic,service,1294,"Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best pe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1302,integrability,servic,service,1302,"the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performanc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1359,integrability,Version,Version,1359,"s --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1858,integrability,version,version,1858,"AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 cal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2528,integrability,batch,batching,2528,"allelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2584,integrability,version,version,2584,"6410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2706,integrability,batch,batch,2706,"ta_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3035,integrability,batch,batches,3035,"ution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3155,integrability,batch,batches,3155,"runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3275,integrability,batch,batches,3275,"threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I112",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3395,integrability,batch,batches,3395,"a/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3516,integrability,batch,batches,3516,"tal.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3637,integrability,batch,batches,3637,".map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3758,integrability,batch,batches,3758,izations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3880,integrability,batch,batches,3880,ting calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4002,integrability,batch,batches,4002,2] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4124,integrability,batch,batches,4124,essed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4246,integrability,batch,batches,4246,sed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4368,integrability,batch,batches,4368,d 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4490,integrability,batch,batches,4490,60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4612,integrability,batch,batches,4612,5001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4734,integrability,batch,batches,4734,001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4856,integrability,batch,batches,4856,001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4978,integrability,batch,batches,4978,001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5100,integrability,batch,batches,5100,001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5222,integrability,batch,batches,5222,001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5344,integrability,batch,batches,5344,001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5466,integrability,batch,batches,5466,001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5588,integrability,batch,batches,5588,001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5710,integrability,batch,batches,5710,001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5832,integrability,batch,batches,5832,001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5954,integrability,batch,batches,5954,001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6076,integrability,batch,batches,6076,001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6198,integrability,batch,batches,6198,001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6320,integrability,batch,batches,6320,001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100],MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6442,integrability,batch,batches,6442,001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6564,integrability,batch,batches,6564,001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 10,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6686,integrability,batch,batches,6686,001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 1,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6808,integrability,batch,batches,6808,001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6930,integrability,batch,batches,6930,001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7052,integrability,batch,batches,7052,001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec pe,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7174,integrability,batch,batches,7174,001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7297,integrability,batch,batches,7297,01 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7420,integrability,batch,batches,7420,1 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7543,integrability,batch,batches,7543, examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7666,integrability,batch,batches,7666,examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7789,integrability,batch,batches,7789,xamples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 1396748568716,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7912,integrability,batch,batches,7912,amples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381:,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8035,integrability,batch,batches,8035,mples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future versio,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8158,integrability,batch,batches,8158,"ples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8281,integrability,batch,batches,8281,"les in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8404,integrability,batch,batches,8404,"les in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Befor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8527,integrability,batch,batches,8527,"les in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8977,integrability,batch,batching,8977,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9033,integrability,version,version,9033,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9155,integrability,batch,batch,9155,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:690,interoperability,platform,platform,690,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1019,interoperability,platform,platform,1019,"ious comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1215,interoperability,platform,platform,1215,"inning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1143,modifiability,servic,service,1143,"I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1151,modifiability,servic,service,1151,"ed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1171,modifiability,servic,service,1171,"ants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1294,modifiability,servic,service,1294,"Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best pe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1302,modifiability,servic,service,1302,"the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performanc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1359,modifiability,Version,Version,1359,"s --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1858,modifiability,version,version,1858,"AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 cal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2584,modifiability,version,version,2584,"6410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9033,modifiability,version,version,9033,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:325,performance,time,time,325,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:751,performance,optimiz,optimized,751,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:804,performance,CPU,CPU,804,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:824,performance,perform,performance,824,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1059,performance,CPU,CPU,1059," results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1509,performance,Tune,Tune,1509,"/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1558,performance,perform,performance,1558,"5 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2247,performance,Tune,Tune,2247,"8 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Proce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2296,performance,perform,performance,2296,"service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2528,performance,batch,batching,2528,"allelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2706,performance,batch,batch,2706,"ta_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2757,performance,optimiz,optimizations,2757,"ow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batche",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3035,performance,batch,batches,3035,"ution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3155,performance,batch,batches,3155,"runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3275,performance,batch,batches,3275,"threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I112",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3395,performance,batch,batches,3395,"a/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3516,performance,batch,batches,3516,"tal.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3637,performance,batch,batches,3637,".map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3758,performance,batch,batches,3758,izations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:3880,performance,batch,batches,3880,ting calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4002,performance,batch,batches,4002,2] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4124,performance,batch,batches,4124,essed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4246,performance,batch,batches,4246,sed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4368,performance,batch,batches,4368,d 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4490,performance,batch,batches,4490,60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4612,performance,batch,batches,4612,5001 examples in 147 batches [0.011 sec per 100]. I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4734,performance,batch,batches,4734,001 examples in 176 batches [0.011 sec per 100]. I1128 03:47:06.086514 139674856871680 call_variants.py:452] Processed 105001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4856,performance,batch,batches,4856,001 examples in 206 batches [0.011 sec per 100]. I1128 03:47:07.738636 139674856871680 call_variants.py:452] Processed 120001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:4978,performance,batch,batches,4978,001 examples in 235 batches [0.011 sec per 100]. I1128 03:47:09.394680 139674856871680 call_variants.py:452] Processed 135001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5100,performance,batch,batches,5100,001 examples in 264 batches [0.011 sec per 100]. I1128 03:47:11.054500 139674856871680 call_variants.py:452] Processed 150001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5222,performance,batch,batches,5222,001 examples in 293 batches [0.011 sec per 100]. I1128 03:47:12.715886 139674856871680 call_variants.py:452] Processed 165001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5344,performance,batch,batches,5344,001 examples in 323 batches [0.011 sec per 100]. I1128 03:47:14.370283 139674856871680 call_variants.py:452] Processed 180001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5466,performance,batch,batches,5466,001 examples in 352 batches [0.011 sec per 100]. I1128 03:47:16.030028 139674856871680 call_variants.py:452] Processed 195001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5588,performance,batch,batches,5588,001 examples in 381 batches [0.011 sec per 100]. I1128 03:47:17.690865 139674856871680 call_variants.py:452] Processed 210001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5710,performance,batch,batches,5710,001 examples in 411 batches [0.011 sec per 100]. I1128 03:47:19.339626 139674856871680 call_variants.py:452] Processed 225001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5832,performance,batch,batches,5832,001 examples in 440 batches [0.011 sec per 100]. I1128 03:47:20.994633 139674856871680 call_variants.py:452] Processed 240001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:5954,performance,batch,batches,5954,001 examples in 469 batches [0.011 sec per 100]. I1128 03:47:22.652035 139674856871680 call_variants.py:452] Processed 255001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6076,performance,batch,batches,6076,001 examples in 499 batches [0.011 sec per 100]. I1128 03:47:24.307619 139674856871680 call_variants.py:452] Processed 270001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6198,performance,batch,batches,6198,001 examples in 528 batches [0.011 sec per 100]. I1128 03:47:25.964561 139674856871680 call_variants.py:452] Processed 285001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6320,performance,batch,batches,6320,001 examples in 557 batches [0.011 sec per 100]. I1128 03:47:27.619805 139674856871680 call_variants.py:452] Processed 300001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100],MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6442,performance,batch,batches,6442,001 examples in 586 batches [0.011 sec per 100]. I1128 03:47:29.276331 139674856871680 call_variants.py:452] Processed 315001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6564,performance,batch,batches,6564,001 examples in 616 batches [0.011 sec per 100]. I1128 03:47:30.922740 139674856871680 call_variants.py:452] Processed 330001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 10,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6686,performance,batch,batches,6686,001 examples in 645 batches [0.011 sec per 100]. I1128 03:47:32.582401 139674856871680 call_variants.py:452] Processed 345001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 1,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6808,performance,batch,batches,6808,001 examples in 674 batches [0.011 sec per 100]. I1128 03:47:34.248395 139674856871680 call_variants.py:452] Processed 360001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6930,performance,batch,batches,6930,001 examples in 704 batches [0.011 sec per 100]. I1128 03:47:35.905924 139674856871680 call_variants.py:452] Processed 375001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7052,performance,batch,batches,7052,001 examples in 733 batches [0.011 sec per 100]. I1128 03:47:37.563962 139674856871680 call_variants.py:452] Processed 390001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec pe,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7174,performance,batch,batches,7174,001 examples in 762 batches [0.011 sec per 100]. I1128 03:47:39.216807 139674856871680 call_variants.py:452] Processed 405001 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7297,performance,batch,batches,7297,01 examples in 792 batches [0.011 sec per 100]. I1128 03:47:40.874265 139674856871680 call_variants.py:452] Processed 420001 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7420,performance,batch,batches,7420,1 examples in 821 batches [0.011 sec per 100]. I1128 03:47:42.549129 139674856871680 call_variants.py:452] Processed 435001 examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7543,performance,batch,batches,7543, examples in 850 batches [0.011 sec per 100]. I1128 03:47:44.205866 139674856871680 call_variants.py:452] Processed 450001 examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7666,performance,batch,batches,7666,examples in 879 batches [0.011 sec per 100]. I1128 03:47:45.870136 139674856871680 call_variants.py:452] Processed 465001 examples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7789,performance,batch,batches,7789,xamples in 909 batches [0.011 sec per 100]. I1128 03:47:47.526660 139674856871680 call_variants.py:452] Processed 480001 examples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 1396748568716,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:7912,performance,batch,batches,7912,amples in 938 batches [0.011 sec per 100]. I1128 03:47:49.185387 139674856871680 call_variants.py:452] Processed 495001 examples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381:,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8035,performance,batch,batches,8035,mples in 967 batches [0.011 sec per 100]. I1128 03:47:50.852418 139674856871680 call_variants.py:452] Processed 510001 examples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future versio,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8158,performance,batch,batches,8158,"ples in 997 batches [0.011 sec per 100]. I1128 03:47:52.511156 139674856871680 call_variants.py:452] Processed 525001 examples in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8281,performance,batch,batches,8281,"les in 1026 batches [0.011 sec per 100]. I1128 03:47:54.166230 139674856871680 call_variants.py:452] Processed 540001 examples in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8404,performance,batch,batches,8404,"les in 1055 batches [0.011 sec per 100]. I1128 03:47:55.828215 139674856871680 call_variants.py:452] Processed 555001 examples in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Befor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8527,performance,batch,batches,8527,"les in 1084 batches [0.011 sec per 100]. I1128 03:47:57.485885 139674856871680 call_variants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8977,performance,batch,batching,8977,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9155,performance,batch,batch,9155,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9206,performance,optimiz,optimizations,9206,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9432,performance,time,timestamp,9432,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9452,performance,time,timestamp,9452,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:488,reliability,checkpoint,checkpoint,488,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2028,reliability,slo,sloppy,2028,"file_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:280,safety,log,logs,280,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:613,safety,input,input,613,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1884,safety,updat,updating,1884,"m in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2610,safety,updat,updating,2610,"cation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9059,safety,updat,updating,9059,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:280,security,log,logs,280,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:505,security,model,models,505,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:516,security,model,model,516,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1884,security,updat,updating,1884,"m in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2610,security,updat,updating,2610,"cation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]. I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]. I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]. I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:9059,security,updat,updating,9059,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:149,testability,observ,observed,149,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:280,testability,log,logs,280,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:40,usability,confirm,confirmed,40,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:310,usability,command,command,310,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:613,usability,input,input,613,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:824,usability,perform,performance,824,"Following up on my previous comment,. I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experime",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1558,usability,perform,performance,1558,"5 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]. 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz. 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:. 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:2296,usability,perform,performance,2296,"service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]. I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]. I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:8688,usability,user,user,8688,"ants.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]. I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]. I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]. I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]. I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]. I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]. I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]. I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]. I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s. user 763m44.970s. sys 58m35.140s. ```. You can see these lines:. ```. W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz. ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:132,deployability,observ,observed,132,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:263,deployability,log,logs,263,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:362,performance,perform,performance,362,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:263,safety,log,logs,263,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:263,security,log,logs,263,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:132,testability,observ,observed,132,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:263,testability,log,logs,263,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:362,usability,perform,performance,362,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:414,usability,user,users,414,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:37,deployability,observ,observed,37,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:168,deployability,log,logs,168,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:414,deployability,Updat,Updated,414,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:271,performance,perform,performance,271,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:168,safety,log,logs,168,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:414,safety,Updat,Updated,414,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:168,security,log,logs,168,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:414,security,Updat,Updated,414,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:37,testability,observ,observed,37,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:168,testability,log,logs,168,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:271,usability,perform,performance,271,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:323,usability,user,users,323,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:382,usability,progress,progress,382,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:360,deployability,releas,release,360,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:448,deployability,releas,release,448,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:486,deployability,build,build,486,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:496,deployability,releas,release,496,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:631,deployability,releas,releases,631,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:116,energy efficiency,reduc,reduction,116,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:434,performance,time,time,434,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:337,safety,review,review,337,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:659,safety,safe,safe,659,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:337,testability,review,review,337,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:478,testability,plan,plan,478,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:190,usability,progress,progress,190,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s. So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines? Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:42,deployability,build,build,42,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:300,energy efficiency,CPU,CPU,300,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:433,integrability,pub,public,433,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:300,performance,CPU,CPU,300,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:34,safety,safe,safe,34,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:381,safety,except,except,381,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:124,usability,user,users,124,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:369,usability,Statu,StatusCode,369,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:446,usability,confirm,confirm,446,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:. ```python. try:. from openvino.inference_engine import IECore, StatusCode. except:. pass. ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:25,availability,down,downside,25,@dkurt Thanks! One small downside is that the Docker image will have to include the old ckpt format as well as the new format. One question for you - do you know if the converted model format can be read and used with regular Estimator as well?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:179,energy efficiency,model,model,179,@dkurt Thanks! One small downside is that the Docker image will have to include the old ckpt format as well as the new format. One question for you - do you know if the converted model format can be read and used with regular Estimator as well?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:226,energy efficiency,Estimat,Estimator,226,@dkurt Thanks! One small downside is that the Docker image will have to include the old ckpt format as well as the new format. One question for you - do you know if the converted model format can be read and used with regular Estimator as well?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:93,interoperability,format,format,93,@dkurt Thanks! One small downside is that the Docker image will have to include the old ckpt format as well as the new format. One question for you - do you know if the converted model format can be read and used with regular Estimator as well?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:119,interoperability,format,format,119,@dkurt Thanks! One small downside is that the Docker image will have to include the old ckpt format as well as the new format. One question for you - do you know if the converted model format can be read and used with regular Estimator as well?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:185,interoperability,format,format,185,@dkurt Thanks! One small downside is that the Docker image will have to include the old ckpt format as well as the new format. One question for you - do you know if the converted model format can be read and used with regular Estimator as well?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:179,security,model,model,179,@dkurt Thanks! One small downside is that the Docker image will have to include the old ckpt format as well as the new format. One question for you - do you know if the converted model format can be read and used with regular Estimator as well?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:63,availability,checkpoint,checkpoint,63,"@pichuan, I'll take a look. Can you please refer what is a new checkpoint format? I've tried only checkpoints that were since r0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:98,availability,checkpoint,checkpoints,98,"@pichuan, I'll take a look. Can you please refer what is a new checkpoint format? I've tried only checkpoints that were since r0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:74,interoperability,format,format,74,"@pichuan, I'll take a look. Can you please refer what is a new checkpoint format? I've tried only checkpoints that were since r0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:63,reliability,checkpoint,checkpoint,63,"@pichuan, I'll take a look. Can you please refer what is a new checkpoint format? I've tried only checkpoints that were since r0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:98,reliability,checkpoint,checkpoints,98,"@pichuan, I'll take a look. Can you please refer what is a new checkpoint format? I've tried only checkpoints that were since r0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1446,deployability,build,building,1446,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:96,energy efficiency,model,models,96,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:161,energy efficiency,model,model,161,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:213,energy efficiency,model,model,213,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:285,energy efficiency,model,model,285,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:343,energy efficiency,model,model,343,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:400,energy efficiency,model,model,400,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:456,energy efficiency,model,model,456,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:569,energy efficiency,model,model,569,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:620,energy efficiency,model,model,620,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:676,energy efficiency,model,model,676,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:704,energy efficiency,Estimat,Estimator,704,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:725,energy efficiency,current,currently,725,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:846,energy efficiency,model,model,846,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:918,energy efficiency,model,model,918,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:976,energy efficiency,model,model,976,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1152,energy efficiency,load,load,1152,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:462,interoperability,xml,xml,462,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:682,interoperability,xml,xml,682,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:789,interoperability,format,format,789,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1126,interoperability,format,format,1126,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1106,modifiability,interm,intermediate,1106,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1152,performance,load,load,1152,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:96,security,model,models,96,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:161,security,model,model,161,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:213,security,model,model,213,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:285,security,model,model,285,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:343,security,model,model,343,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:400,security,model,model,400,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:456,security,model,model,456,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:569,security,model,model,569,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:620,security,model,model,620,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:676,security,model,model,676,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:846,security,model,model,846,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:918,security,model,model,918,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:976,security,model,model,976,"@dkurt Sorry for the confusion. I meant:. ```. $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/. total 449M. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. These are the extra files after enabling OpenVINO:. ```. -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin. -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping. -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml. ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):. ```. -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001. -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index. -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta. ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work! (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:258,deployability,instal,installation,258,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:110,energy efficiency,model,model,110,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:232,energy efficiency,reduc,reduce,232,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:285,integrability,compon,components,285,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:89,interoperability,convers,conversion,89,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:163,interoperability,xml,xml,163,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:285,interoperability,compon,components,285,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:46,modifiability,interm,intermediate,46,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:285,modifiability,compon,components,285,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:110,security,model,model,110,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```. rm model.pb;. ```. However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:201,availability,down,downside,201,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:108,energy efficiency,model,model,108,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:255,energy efficiency,CPU,CPU,255,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:292,energy efficiency,CPU,CPU,292,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:255,performance,CPU,CPU,255,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:292,performance,CPU,CPU,292,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:422,safety,test,test,422,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:108,security,model,model,108,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:422,testability,test,test,422,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:183,availability,Checkpoint,Checkpoint,183,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:205,availability,restor,restore,205,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:638,availability,down,downside,638,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:291,deployability,contain,contains,291,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:435,deployability,API,API,435,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:110,energy efficiency,model,model,110,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:213,energy efficiency,model,model,213,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:378,energy efficiency,model,model,378,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:455,energy efficiency,Estimat,Estimator,455,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:692,energy efficiency,CPU,CPU,692,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:729,energy efficiency,CPU,CPU,729,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:964,energy efficiency,cloud,cloud,964,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1153,energy efficiency,CPU,CPU,1153,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:435,integrability,API,API,435,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:435,interoperability,API,API,435,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:498,interoperability,convers,conversion,498,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:944,interoperability,standard,standard-,944,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:340,modifiability,interm,intermediate,340,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:365,modifiability,layer,layer,365,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:692,performance,CPU,CPU,692,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:729,performance,CPU,CPU,729,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1153,performance,CPU,CPU,1153,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:183,reliability,Checkpoint,Checkpoint,183,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:205,reliability,restor,restore,205,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:558,reliability,doe,doesn,558,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:859,safety,test,test,859,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:110,security,model,model,110,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:213,security,model,model,213,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:378,security,model,model,378,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:546,testability,simpl,simpler,546,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:859,testability,test,test,859,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:546,usability,simpl,simpler,546,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:450,deployability,build,building,450,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:535,deployability,build,building,535,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:653,deployability,releas,release,653,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:683,deployability,build,building,683,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:709,deployability,build,build-arg,709,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1060,deployability,build,building,1060,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:51,energy efficiency,cloud,cloud,51,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:240,energy efficiency,CPU,CPU,240,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:400,energy efficiency,GPU,GPU,400,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:544,energy efficiency,GPU,GPU,544,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:673,energy efficiency,CPU,CPU,673,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:889,energy efficiency,GPU,GPU,889,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1073,energy efficiency,GPU,GPU,1073,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1277,energy efficiency,CPU,CPU,1277,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1316,energy efficiency,GPU,GPU,1316,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:31,interoperability,standard,standard-,31,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:240,performance,CPU,CPU,240,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:400,performance,GPU,GPU,400,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:544,performance,GPU,GPU,544,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:673,performance,CPU,CPU,673,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:889,performance,GPU,GPU,889,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1073,performance,GPU,GPU,1073,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1277,performance,CPU,CPU,1277,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1316,performance,GPU,GPU,1316,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1299,reliability,doe,doesn,1299,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:331,safety,test,testing,331,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1425,safety,review,review,1425,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1484,safety,review,review,1484,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:331,testability,test,testing,331,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1425,testability,review,review,1425,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1484,testability,review,review,1484,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1141,usability,user,users,1141,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately? Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:. For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though. Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either. I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6,deployability,updat,update,6,"Quick update and FYI for you @dkurt . I ran with the internal latest code (which we switched all metrics to be on HG003 BAMs). Here are the improvements with `--use_openvino=true`. * wgs: 233m14.191s --> 204m35.065s. * wes: 1m41.381s --> 1m31.513s. * pacbio: 193m20.407s --> 169m45.878s. * hybrid_pacbio_illumina: 241m7.426s --> 189m40.148s. This was after your ""Process OpenVINO in thread (#8)"" change yesterday.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:251,modifiability,pac,pacbio,251,"Quick update and FYI for you @dkurt . I ran with the internal latest code (which we switched all metrics to be on HG003 BAMs). Here are the improvements with `--use_openvino=true`. * wgs: 233m14.191s --> 204m35.065s. * wes: 1m41.381s --> 1m31.513s. * pacbio: 193m20.407s --> 169m45.878s. * hybrid_pacbio_illumina: 241m7.426s --> 189m40.148s. This was after your ""Process OpenVINO in thread (#8)"" change yesterday.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6,safety,updat,update,6,"Quick update and FYI for you @dkurt . I ran with the internal latest code (which we switched all metrics to be on HG003 BAMs). Here are the improvements with `--use_openvino=true`. * wgs: 233m14.191s --> 204m35.065s. * wes: 1m41.381s --> 1m31.513s. * pacbio: 193m20.407s --> 169m45.878s. * hybrid_pacbio_illumina: 241m7.426s --> 189m40.148s. This was after your ""Process OpenVINO in thread (#8)"" change yesterday.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:6,security,updat,update,6,"Quick update and FYI for you @dkurt . I ran with the internal latest code (which we switched all metrics to be on HG003 BAMs). Here are the improvements with `--use_openvino=true`. * wgs: 233m14.191s --> 204m35.065s. * wes: 1m41.381s --> 1m31.513s. * pacbio: 193m20.407s --> 169m45.878s. * hybrid_pacbio_illumina: 241m7.426s --> 189m40.148s. This was after your ""Process OpenVINO in thread (#8)"" change yesterday.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:186,performance,time,time,186,"@pichuan, I'll think and propose a flexible solution for OpenVINO acceleration. Can you please add some details about the latest numbers? Is that regression or improvement? Because last time we saw 266m46.183s --> 198m46.734s for WGS (call_variants) now it's 233m14.191s --> 204m35.065s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:146,testability,regress,regression,146,"@pichuan, I'll think and propose a flexible solution for OpenVINO acceleration. Can you please add some details about the latest numbers? Is that regression or improvement? Because last time we saw 266m46.183s --> 198m46.734s for WGS (call_variants) now it's 233m14.191s --> 204m35.065s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:265,deployability,log,logging,265,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:715,deployability,observ,observing,715,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:42,energy efficiency,reduc,reduction,42,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:573,energy efficiency,cpu,cpu-only-machine-on-google-cloud-platform,573,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:606,interoperability,platform,platform,606,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:148,performance,time,time,148,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:299,performance,time,time,299,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:355,performance,time,time,355,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:573,performance,cpu,cpu-only-machine-on-google-cloud-platform,573,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:974,performance,time,time,974,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:10,reliability,doe,does,10,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:265,safety,log,logging,265,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:265,security,log,logging,265,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:265,testability,log,logging,265,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:715,testability,observ,observing,715,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:830,testability,verif,verified,830,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:474,usability,command,command,474,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:559,usability,command,command-for-a-cpu-only-machine-on-google-cloud-platform,559,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: . 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). . 2. The second thing that has changed is your change to improve the logging. 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:110,deployability,log,logging,110,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:316,deployability,configurat,configuration,316,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:332,deployability,build,build,332,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:316,integrability,configur,configuration,316,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:316,modifiability,configur,configuration,316,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:244,performance,time,time,244,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:110,safety,log,logging,110,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:110,security,log,logging,110,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:316,security,configur,configuration,316,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:110,testability,log,logging,110,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:138,usability,confirm,confirm,138,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:165,usability,efficien,efficiency,165,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome! I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:27,deployability,updat,update,27,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:326,deployability,releas,release,326,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:666,deployability,build,build,666,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:698,deployability,build,build-arg,698,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:958,deployability,releas,release,958,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:76,energy efficiency,current,current,76,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:416,energy efficiency,Current,Currently,416,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:897,energy efficiency,CPU,CPU,897,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1210,energy efficiency,current,current,1210,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:897,performance,CPU,CPU,897,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:27,safety,updat,update,27,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:236,safety,review,reviewed,236,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:27,security,updat,update,27,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:59,security,team,team,59,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:236,testability,review,reviewed,236,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:263,testability,plan,plan,263,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:523,testability,understand,understand,523,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:658,testability,plan,plan,658,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:745,testability,plan,plan,745,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:764,usability,document,documentation,764,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:781,usability,user,users,781,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1022,usability,behavi,behavior,1022,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1218,usability,statu,status,1218,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion. 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False. 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:23,availability,sli,slight,23,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:248,availability,sli,slight,248,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:358,availability,sli,slight,358,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:415,energy efficiency,load,loaded,415,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:415,performance,load,loaded,415,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:448,performance,time,time,448,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:23,reliability,sli,slight,23,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:248,reliability,sli,slight,248,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:358,reliability,sli,slight,358,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```. -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:142,deployability,releas,release,142,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:421,deployability,patch,patch,421,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:712,deployability,instal,install,712,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1199,deployability,scale,scale,1199,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:341,energy efficiency,model,modeling,341,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:569,energy efficiency,model,models,569,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:599,energy efficiency,model,model,599,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:622,energy efficiency,model,model,622,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:846,energy efficiency,model,modeling,846,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:975,energy efficiency,model,modeling,975,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1048,energy efficiency,model,model,1048,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1110,energy efficiency,model,models,1110,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1119,energy efficiency,model,model,1119,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1199,energy efficiency,scale,scale,1199,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1216,energy efficiency,model,model,1216,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1350,energy efficiency,model,modeling,1350,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:640,interoperability,format,format,640,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1065,modifiability,pac,pacbio,1065,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1199,modifiability,scal,scale,1199,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:720,performance,network,networkx,720,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1199,performance,scale,scale,1199,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:52,safety,review,reviewed,52,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:374,safety,safe,safer,374,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:421,safety,patch,patch,421,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:740,safety,test,test-generator,740,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:341,security,model,modeling,341,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:421,security,patch,patch,421,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:569,security,model,models,569,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:599,security,model,model,599,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:622,security,model,model,622,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:720,security,network,networkx,720,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:846,security,model,modeling,846,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:975,security,model,modeling,975,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1048,security,model,model,1048,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1110,security,model,models,1110,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1119,security,model,model,1119,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1216,security,model,model,1216,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1350,security,model,modeling,1350,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:52,testability,review,reviewed,52,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:79,testability,plan,plan,79,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:740,testability,test,test-generator,740,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```. sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. ```. It would be safer to replace with something like this:. ```patch. diff --git a/Dockerfile b/Dockerfile. index 0432fd8..a57364d 100644. --- a/Dockerfile. +++ b/Dockerfile. @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*. # Convert model to OpenVINO format. RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. python3 -m pip install networkx defusedxml test-generator==0.1.1; \. - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \. + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \. for model in wgs wes pacbio hybrid_pacbio_illumina; do \. cd /opt/models/${model}; \. @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \. --scale 128; \. rm model.pb; \. done \. + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \. fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:30,usability,close,closer,30,"@gunjanbaid, I'll take a look closer. In our experiments maximal absolute difference between TensorFlow and OpenVINO about 10e-6. Maybe this is some corner case where such difference may lead to misclassification.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:23,deployability,updat,update,23,"@dkurt Thanks. Another update for you - I am now trying to incorporate your on-the-fly conversion code:. https://github.com/google/deepvariant/pull/363/commits/f0ed01891c3e612d4c7093e5e844f855beae707a. I think it'll be cleaner, and also removes the need for https://github.com/google/deepvariant/pull/363#issuecomment-736278644. I do have a question for the code. I'll comment inline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:87,interoperability,convers,conversion,87,"@dkurt Thanks. Another update for you - I am now trying to incorporate your on-the-fly conversion code:. https://github.com/google/deepvariant/pull/363/commits/f0ed01891c3e612d4c7093e5e844f855beae707a. I think it'll be cleaner, and also removes the need for https://github.com/google/deepvariant/pull/363#issuecomment-736278644. I do have a question for the code. I'll comment inline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:23,safety,updat,update,23,"@dkurt Thanks. Another update for you - I am now trying to incorporate your on-the-fly conversion code:. https://github.com/google/deepvariant/pull/363/commits/f0ed01891c3e612d4c7093e5e844f855beae707a. I think it'll be cleaner, and also removes the need for https://github.com/google/deepvariant/pull/363#issuecomment-736278644. I do have a question for the code. I'll comment inline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:23,security,updat,update,23,"@dkurt Thanks. Another update for you - I am now trying to incorporate your on-the-fly conversion code:. https://github.com/google/deepvariant/pull/363/commits/f0ed01891c3e612d4c7093e5e844f855beae707a. I think it'll be cleaner, and also removes the need for https://github.com/google/deepvariant/pull/363#issuecomment-736278644. I do have a question for the code. I'll comment inline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:233,energy efficiency,model,models,233,"> I think it'll be cleaner, and also removes the need for #363 (comment). I do have a question for the code. I'll comment inline. Yeah, I also think that it makes the procedure more transparent. > Would the result be wrong for those models, if the placeholder here has a different height (100) than the actual image? Good point! Just added a commit which uses `input_fn` to get input dimensions and number of channels. So OpenVINO model will work on dataset's resolution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:431,energy efficiency,model,model,431,"> I think it'll be cleaner, and also removes the need for #363 (comment). I do have a question for the code. I'll comment inline. Yeah, I also think that it makes the procedure more transparent. > Would the result be wrong for those models, if the placeholder here has a different height (100) than the actual image? Good point! Just added a commit which uses `input_fn` to get input dimensions and number of channels. So OpenVINO model will work on dataset's resolution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:378,safety,input,input,378,"> I think it'll be cleaner, and also removes the need for #363 (comment). I do have a question for the code. I'll comment inline. Yeah, I also think that it makes the procedure more transparent. > Would the result be wrong for those models, if the placeholder here has a different height (100) than the actual image? Good point! Just added a commit which uses `input_fn` to get input dimensions and number of channels. So OpenVINO model will work on dataset's resolution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:233,security,model,models,233,"> I think it'll be cleaner, and also removes the need for #363 (comment). I do have a question for the code. I'll comment inline. Yeah, I also think that it makes the procedure more transparent. > Would the result be wrong for those models, if the placeholder here has a different height (100) than the actual image? Good point! Just added a commit which uses `input_fn` to get input dimensions and number of channels. So OpenVINO model will work on dataset's resolution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:431,security,model,model,431,"> I think it'll be cleaner, and also removes the need for #363 (comment). I do have a question for the code. I'll comment inline. Yeah, I also think that it makes the procedure more transparent. > Would the result be wrong for those models, if the placeholder here has a different height (100) than the actual image? Good point! Just added a commit which uses `input_fn` to get input dimensions and number of channels. So OpenVINO model will work on dataset's resolution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:378,usability,input,input,378,"> I think it'll be cleaner, and also removes the need for #363 (comment). I do have a question for the code. I'll comment inline. Yeah, I also think that it makes the procedure more transparent. > Would the result be wrong for those models, if the placeholder here has a different height (100) than the actual image? Good point! Just added a commit which uses `input_fn` to get input dimensions and number of channels. So OpenVINO model will work on dataset's resolution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:15,deployability,updat,update,15,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:305,deployability,log,logging,305,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:162,performance,time,times,162,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:15,safety,updat,update,15,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:305,safety,log,logging,305,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:15,security,updat,update,15,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:305,security,log,logging,305,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:305,testability,log,logging,305,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:121,usability,confirm,confirmed,121,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:151,usability,command,command,151,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:334,usability,confirm,confirmed,334,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:17,deployability,updat,update,17,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:315,deployability,log,logging,315,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1011,deployability,releas,release,1011,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1083,deployability,log,logging,1083,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1165,deployability,log,logging,1165,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1044,integrability,messag,message,1044,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1044,interoperability,messag,message,1044,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:170,performance,time,times,170,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:17,safety,updat,update,17,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:315,safety,log,logging,315,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1083,safety,log,logging,1083,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1165,safety,log,logging,1165,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:17,security,updat,update,17,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:315,security,log,logging,315,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1083,security,log,logging,1083,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1165,security,log,logging,1165,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:315,testability,log,logging,315,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1083,testability,log,logging,1083,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1165,testability,log,logging,1165,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:129,usability,confirm,confirmed,129,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:159,usability,command,command,159,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:350,usability,confirm,confirmed,350,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:708,usability,confirm,confirmed,708,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:1064,usability,user,users,1064,"> @dkurt A quick update:. > . > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on). > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. > . > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). > . > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉. (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:41,deployability,patch,patch,41,Fixed non-deterministic behavior by last patch.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:41,safety,patch,patch,41,Fixed non-deterministic behavior by last patch.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:41,security,patch,patch,41,Fixed non-deterministic behavior by last patch.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:24,usability,behavi,behavior,24,Fixed non-deterministic behavior by last patch.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/pull/363:79,deployability,releas,releases,79,"@dkurt FYI , the OpenVINO changes are in https://github.com/google/deepvariant/releases/tag/v1.1.0. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/issues/364:94,integrability,topic,topic,94,"I will close this issue, and repeat this question where I previously had a thread on the same topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/364
https://github.com/google/deepvariant/issues/364:7,usability,close,close,7,"I will close this issue, and repeat this question where I previously had a thread on the same topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/364
https://github.com/google/deepvariant/pull/365:13,security,sign,signed,13,@googlebot I signed it!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:323,deployability,patch,patch,323,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:411,deployability,releas,release,411,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:474,deployability,releas,release,474,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:312,integrability,sub,submit,312,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:230,safety,review,review,230,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:237,safety,test,test,237,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:323,safety,patch,patch,323,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:323,security,patch,patch,323,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:230,testability,review,review,230,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:237,testability,test,test,237,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:495,energy efficiency,load,loading,495,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:. ``` . return (input_examples. | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)). | 'Groupby' >> beam.GroupByKey(). | 'DropKey' >> beam.FlatMap(lambda x: x[1])). ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner? I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:495,performance,load,loading,495,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:. ``` . return (input_examples. | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)). | 'Groupby' >> beam.GroupByKey(). | 'DropKey' >> beam.FlatMap(lambda x: x[1])). ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner? I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:528,performance,memor,memory,528,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:. ``` . return (input_examples. | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)). | 'Groupby' >> beam.GroupByKey(). | 'DropKey' >> beam.FlatMap(lambda x: x[1])). ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner? I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:750,performance,memor,memory,750,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:. ``` . return (input_examples. | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)). | 'Groupby' >> beam.GroupByKey(). | 'DropKey' >> beam.FlatMap(lambda x: x[1])). ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner? I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:191,safety,test,test,191,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:. ``` . return (input_examples. | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)). | 'Groupby' >> beam.GroupByKey(). | 'DropKey' >> beam.FlatMap(lambda x: x[1])). ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner? I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:191,testability,test,test,191,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:. ``` . return (input_examples. | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)). | 'Groupby' >> beam.GroupByKey(). | 'DropKey' >> beam.FlatMap(lambda x: x[1])). ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner? I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:528,usability,memor,memory,528,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:. ``` . return (input_examples. | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)). | 'Groupby' >> beam.GroupByKey(). | 'DropKey' >> beam.FlatMap(lambda x: x[1])). ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner? I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:750,usability,memor,memory,750,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:. ``` . return (input_examples. | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)). | 'Groupby' >> beam.GroupByKey(). | 'DropKey' >> beam.FlatMap(lambda x: x[1])). ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner? I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:61,deployability,observ,observed,61,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:92,energy efficiency,load,loaded,92,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:451,energy efficiency,optim,optimization,451,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:270,integrability,sub,subset,270,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:92,performance,load,loaded,92,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:104,performance,memor,memory,104,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:191,performance,memor,memory,191,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:451,performance,optimiz,optimization,451,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:16,testability,understand,understanding,16,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:61,testability,observ,observed,61,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:104,usability,memor,memory,104,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:191,usability,memor,memory,191,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:360,usability,support,support,360,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1112,deployability,roll,rolled,1112,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1337,deployability,depend,depends,1337,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:398,energy efficiency,current,current,398,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1337,integrability,depend,depends,1337,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1337,modifiability,depend,depends,1337,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:555,safety,input,input,555,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1337,safety,depend,depends,1337,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:534,testability,simpl,simply,534,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1337,testability,depend,depends,1337,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:534,usability,simpl,simply,534,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:555,usability,input,input,555,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1227,usability,tool,tools,1227,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1363,usability,behavi,behaving,1363,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:. ```. input_data = readers | ""FlattenInputs"" >> beam.Flatten(). partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>). for i, p in enumerate(partitions):. writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...). ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:90,usability,confirm,confirm,90,"Thanks @anands-repo for the PR. FYI: In the short term, we likely won't have bandwidth to confirm that this works. I'll leave this open and open an internal issue to track it. If/when we have a chance to confirm it works and merge it internally, we'll let you know (and will attribute it to your PR).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:204,usability,confirm,confirm,204,"Thanks @anands-repo for the PR. FYI: In the short term, we likely won't have bandwidth to confirm that this works. I'll leave this open and open an internal issue to track it. If/when we have a chance to confirm it works and merge it internally, we'll let you know (and will attribute it to your PR).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1554,modifiability,pac,pace,1554,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:317,safety,review,reviewed,317,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1068,safety,test,tested,1068,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1173,safety,test,testcase,1173,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1130,security,modif,modified,1130,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1509,security,team,team,1509,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:317,testability,review,reviewed,317,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:986,testability,context,context,986,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1068,testability,test,tested,1068,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1173,testability,test,testcase,1173,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1436,testability,understand,understand,1436,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:653,usability,person,personal,653,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1024,usability,minim,minimum,1024,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:1254,usability,clear,clear,1254,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:18,usability,close,close,18,"Hi, I am going to close this pull request as it's nearly 4 years older.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/issues/366:38,energy efficiency,model,model,38,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:107,energy efficiency,optim,optimally,107,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:206,energy efficiency,model,model,206,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:5,integrability,Coupl,Couple,5,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:55,interoperability,specif,specifically,55,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:5,modifiability,Coupl,Couple,5,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:99,performance,perform optim,perform optimally,99,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:341,performance,perform,performs,341,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:38,security,model,model,38,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:206,security,model,model,206,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:5,testability,Coupl,Couple,5,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:406,testability,coverag,coverage,406,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:99,usability,perform,perform,99,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:291,usability,command,command,291,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:308,usability,help,help,308,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:341,usability,perform,performs,341,"Hi,. Couple of points:. * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. . * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. . * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:417,deployability,log,logfile,417,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:448,deployability,log,log,448,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:525,deployability,log,log,525,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:240,modifiability,PAC,PACBIO,240,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:417,safety,log,logfile,417,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:448,safety,log,log,448,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:525,safety,log,log,525,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:417,security,log,logfile,417,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:448,security,log,log,448,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:525,security,log,log,525,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:417,testability,log,logfile,417,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:448,testability,log,log,448,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:525,testability,log,log,525,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:27,usability,command,command,27,"thanks for your responses. command line：. docker run \. -v /sfs-grand-med-research/:/sfs-grand-med-research/ \. swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=human_g1k_v37.main_chrom.fasta \. --reads=202022.hg19.pbmm2.sort.MT.bam \. --output_vcf=202022.MT.vcf.gz \. --num_shards=16 \. --intermediate_results_dir=/tmp/. logfile：. [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:294,deployability,fail,fail,294,"Hi @hongssy . I think I may know why this variant isn't being called. To call variants, Deepvariant constructs a pileup image of a 221 bp region of the genome (110 bp to the left and right of the candidate position). If the start or end of the contig falls within this window, DeepVariant will fail, and as a result we do not generate candidates when this is the case. Normally, the ends of a contig are telomeric or scaffold sequence, so variants can't meaningfully be called there anyway. . However, the mitochondrial genome is circular, so all of the sequence will have valid mappings (especially with HiFi data, where a read can span the entire genome). This means that DeepVariant will not be able to call the final ~110 bp and the first ~110 bp of the genome. In your IGV image, I think you can see this artifact as the softclip sequence before the first position. . I had not appreciated that this limitation existed until now, and will need to think about the best way to address it. I am sorry that I do not have a fast fix to this issue. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:810,deployability,artifact,artifact,810,"Hi @hongssy . I think I may know why this variant isn't being called. To call variants, Deepvariant constructs a pileup image of a 221 bp region of the genome (110 bp to the left and right of the candidate position). If the start or end of the contig falls within this window, DeepVariant will fail, and as a result we do not generate candidates when this is the case. Normally, the ends of a contig are telomeric or scaffold sequence, so variants can't meaningfully be called there anyway. . However, the mitochondrial genome is circular, so all of the sequence will have valid mappings (especially with HiFi data, where a read can span the entire genome). This means that DeepVariant will not be able to call the final ~110 bp and the first ~110 bp of the genome. In your IGV image, I think you can see this artifact as the softclip sequence before the first position. . I had not appreciated that this limitation existed until now, and will need to think about the best way to address it. I am sorry that I do not have a fast fix to this issue. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:294,reliability,fail,fail,294,"Hi @hongssy . I think I may know why this variant isn't being called. To call variants, Deepvariant constructs a pileup image of a 221 bp region of the genome (110 bp to the left and right of the candidate position). If the start or end of the contig falls within this window, DeepVariant will fail, and as a result we do not generate candidates when this is the case. Normally, the ends of a contig are telomeric or scaffold sequence, so variants can't meaningfully be called there anyway. . However, the mitochondrial genome is circular, so all of the sequence will have valid mappings (especially with HiFi data, where a read can span the entire genome). This means that DeepVariant will not be able to call the final ~110 bp and the first ~110 bp of the genome. In your IGV image, I think you can see this artifact as the softclip sequence before the first position. . I had not appreciated that this limitation existed until now, and will need to think about the best way to address it. I am sorry that I do not have a fast fix to this issue. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:573,safety,valid,valid,573,"Hi @hongssy . I think I may know why this variant isn't being called. To call variants, Deepvariant constructs a pileup image of a 221 bp region of the genome (110 bp to the left and right of the candidate position). If the start or end of the contig falls within this window, DeepVariant will fail, and as a result we do not generate candidates when this is the case. Normally, the ends of a contig are telomeric or scaffold sequence, so variants can't meaningfully be called there anyway. . However, the mitochondrial genome is circular, so all of the sequence will have valid mappings (especially with HiFi data, where a read can span the entire genome). This means that DeepVariant will not be able to call the final ~110 bp and the first ~110 bp of the genome. In your IGV image, I think you can see this artifact as the softclip sequence before the first position. . I had not appreciated that this limitation existed until now, and will need to think about the best way to address it. I am sorry that I do not have a fast fix to this issue. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/366:5,usability,close,close,5,"I'll close this issue now, but feel free to reopen if you have any other questions @hongssy.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366
https://github.com/google/deepvariant/issues/367:41,availability,consist,consistent,41,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:228,availability,error,error,228,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:178,deployability,Fail,Failed,178,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:107,integrability,coupl,couple,107,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:234,integrability,messag,message,234,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:234,interoperability,messag,message,234,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:107,modifiability,coupl,couple,107,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:228,performance,error,error,228,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:312,performance,reindex,reindex,312,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:178,reliability,Fail,Failed,178,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:266,reliability,doe,does,266,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:228,safety,error,error,228,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:107,testability,coupl,couple,107,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:41,usability,consist,consistent,41,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:228,usability,error,error,228,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference? * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/368:49,deployability,build,build,49,deepvariant_pb2 is a generated file. You need to build make_examples in order for this file to be generated.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/368
https://github.com/google/deepvariant/issues/369:546,energy efficiency,model,model-case-study,546,"Hi,. Below are the answer for the first 3 questions. The rest will be answered later. 1. Yes. 2. I'm not sure I fully understand the question. But, DeepVariant is trained on a mix of sorted and unsorted data. You may run DeepVariant make_examples with either hap-sorted or unsorted BAMs. There are 2 flags that need to be set in order for DeepVariant to take advantage of hap-sorted data. . 3. Yes. If you haven't already you may check the case study for PacBio data [here](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md). In that tutorial the process of preparing hap-sorted BAM files is explained. 4,5. Will be answered later...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:455,modifiability,Pac,PacBio,455,"Hi,. Below are the answer for the first 3 questions. The rest will be answered later. 1. Yes. 2. I'm not sure I fully understand the question. But, DeepVariant is trained on a mix of sorted and unsorted data. You may run DeepVariant make_examples with either hap-sorted or unsorted BAMs. There are 2 flags that need to be set in order for DeepVariant to take advantage of hap-sorted data. . 3. Yes. If you haven't already you may check the case study for PacBio data [here](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md). In that tutorial the process of preparing hap-sorted BAM files is explained. 4,5. Will be answered later...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:539,modifiability,pac,pacbio-model-case-study,539,"Hi,. Below are the answer for the first 3 questions. The rest will be answered later. 1. Yes. 2. I'm not sure I fully understand the question. But, DeepVariant is trained on a mix of sorted and unsorted data. You may run DeepVariant make_examples with either hap-sorted or unsorted BAMs. There are 2 flags that need to be set in order for DeepVariant to take advantage of hap-sorted data. . 3. Yes. If you haven't already you may check the case study for PacBio data [here](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md). In that tutorial the process of preparing hap-sorted BAM files is explained. 4,5. Will be answered later...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:546,security,model,model-case-study,546,"Hi,. Below are the answer for the first 3 questions. The rest will be answered later. 1. Yes. 2. I'm not sure I fully understand the question. But, DeepVariant is trained on a mix of sorted and unsorted data. You may run DeepVariant make_examples with either hap-sorted or unsorted BAMs. There are 2 flags that need to be set in order for DeepVariant to take advantage of hap-sorted data. . 3. Yes. If you haven't already you may check the case study for PacBio data [here](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md). In that tutorial the process of preparing hap-sorted BAM files is explained. 4,5. Will be answered later...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:118,testability,understand,understand,118,"Hi,. Below are the answer for the first 3 questions. The rest will be answered later. 1. Yes. 2. I'm not sure I fully understand the question. But, DeepVariant is trained on a mix of sorted and unsorted data. You may run DeepVariant make_examples with either hap-sorted or unsorted BAMs. There are 2 flags that need to be set in order for DeepVariant to take advantage of hap-sorted data. . 3. Yes. If you haven't already you may check the case study for PacBio data [here](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md). In that tutorial the process of preparing hap-sorted BAM files is explained. 4,5. Will be answered later...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:63,energy efficiency,model,model,63,"Hi @akolesnikov , thanks for the responses. A follow-up - if a model is not intended to be used for generating the haplotagging VCF, but only intended for the final variant call, then is it acceptable to train using only haplotagged images?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:63,security,model,model,63,"Hi @akolesnikov , thanks for the responses. A follow-up - if a model is not intended to be used for generating the haplotagging VCF, but only intended for the final variant call, then is it acceptable to train using only haplotagged images?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:452,deployability,contain,contains,452,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:139,energy efficiency,model,models,139,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:151,energy efficiency,model,model,151,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:195,energy efficiency,model,model,195,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:308,energy efficiency,model,model,308,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:369,energy efficiency,model,model,369,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:446,energy efficiency,model,model,446,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:354,interoperability,distribut,distribute,354,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:83,modifiability,design decis,design decision,83,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:410,modifiability,Pac,PacBio,410,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:139,security,model,models,139,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:151,security,model,model,151,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:195,security,model,model,195,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:308,security,model,model,308,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:369,security,model,model,369,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:446,security,model,model,446,"@anands-repo if you wish to only train with HP sorted images, that is a reasonable design decision. In our case, we could have trained two models, one model only with HP sorted images, the other model without sorting by HP. Based on empirical evidence, we find that mixing the training data and training one model was reasonable. That also allowed us to distribute one model, rather than two separate ones. 4. PacBio training data for the hybrid model contains training examples with HP sorted images and unsorted images. Sorting by HP is done in make_examples. 5. The genomes used include HG002, HG004-HG007.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:74,energy efficiency,model,model,74,Thanks for the explanations @gunjanbaid . It makes sense to have a single model for PacBio for both haplotagging and variant calling. I will close the issue for now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:84,modifiability,Pac,PacBio,84,Thanks for the explanations @gunjanbaid . It makes sense to have a single model for PacBio for both haplotagging and variant calling. I will close the issue for now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:74,security,model,model,74,Thanks for the explanations @gunjanbaid . It makes sense to have a single model for PacBio for both haplotagging and variant calling. I will close the issue for now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/369:141,usability,close,close,141,Thanks for the explanations @gunjanbaid . It makes sense to have a single model for PacBio for both haplotagging and variant calling. I will close the issue for now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/369
https://github.com/google/deepvariant/issues/370:994,deployability,log,log,994,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded. Something like this:. ```. /opt/deepvariant/bin/make_examples \. --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \. --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \. --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \. --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:151,interoperability,specif,specific,151,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded. Something like this:. ```. /opt/deepvariant/bin/make_examples \. --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \. --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \. --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \. --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:102,modifiability,paramet,parameter,102,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded. Something like this:. ```. /opt/deepvariant/bin/make_examples \. --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \. --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \. --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \. --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:507,modifiability,paramet,parameter,507,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded. Something like this:. ```. /opt/deepvariant/bin/make_examples \. --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \. --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \. --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \. --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:455,performance,parallel,parallel,455,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded. Something like this:. ```. /opt/deepvariant/bin/make_examples \. --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \. --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \. --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \. --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:994,safety,log,log,994,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded. Something like this:. ```. /opt/deepvariant/bin/make_examples \. --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \. --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \. --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \. --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:994,security,log,log,994,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded. Something like this:. ```. /opt/deepvariant/bin/make_examples \. --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \. --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \. --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \. --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:994,testability,log,log,994,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded. Something like this:. ```. /opt/deepvariant/bin/make_examples \. --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \. --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \. --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \. --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:14,usability,close,close,14,"@nikostr I'll close this issue, but feel free to reopen if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/371:42,deployability,log,logit,42,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:167,energy efficiency,model,modeling,167,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:811,energy efficiency,current,currently,811,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:713,interoperability,FORMAT,FORMAT,713,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:48,modifiability,layer,layer,48,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:65,modifiability,layer,layers,65,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:295,modifiability,layer,layers,295,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:42,safety,log,logit,42,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:638,safety,input,input,638,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:24,security,Access,Accessing,24,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:42,security,log,logit,42,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:167,security,model,modeling,167,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:42,testability,log,logit,42,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:790,testability,plan,plans,790,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:542,usability,support,support,542,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:638,usability,input,input,638,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:74,deployability,log,logit,74,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:190,deployability,Modul,Module,190,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:158,integrability,interfac,interface,158,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:255,integrability,endpoint,endpoint,255,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:158,interoperability,interfac,interface,158,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:80,modifiability,layer,layer,80,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:158,modifiability,interfac,interface,158,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:190,modifiability,Modul,Module,190,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:74,safety,log,logit,74,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:190,safety,Modul,Module,190,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:60,security,access,accessing,60,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:74,security,log,logit,74,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:74,testability,log,logit,74,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:89,testability,understand,understand,89,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:153,usability,user,user,153,"@AndrewCarroll 2 was exactly what I meant, thank you! About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:99,deployability,modul,module,99,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:358,deployability,log,logits,358,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:63,integrability,abstract,abstracting,63,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:81,integrability,compon,components,81,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:81,interoperability,compon,components,81,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:63,modifiability,abstract,abstracting,63,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:81,modifiability,compon,components,81,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:99,modifiability,modul,module,99,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:29,performance,time,time,29,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:99,safety,modul,module,99,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:358,safety,log,logits,358,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:358,security,log,logits,358,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/371:358,testability,log,logits,358,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/372:86,availability,error,error,86,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/372:92,integrability,messag,message,92,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/372:92,interoperability,messag,message,92,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/372:86,performance,error,error,86,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/372:11,reliability,doe,does,11,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/372:86,safety,error,error,86,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/372:86,usability,error,error,86,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/374:112,interoperability,format,formatting,112,@YanJiunLiu could you attach a small snippet of the BED file that is not working for you? Perhaps there is some formatting issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:47,deployability,fail,fail,47,"This is the bed file that execute make example fail . <img width=""672"" alt=""截圖 2020-10-29 上午11 12 18"" src=""https://user-images.githubusercontent.com/63279862/97521250-23409f00-19d8-11eb-8489-f0d0968d872d.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:47,reliability,fail,fail,47,"This is the bed file that execute make example fail . <img width=""672"" alt=""截圖 2020-10-29 上午11 12 18"" src=""https://user-images.githubusercontent.com/63279862/97521250-23409f00-19d8-11eb-8489-f0d0968d872d.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:115,usability,user,user-images,115,"This is the bed file that execute make example fail . <img width=""672"" alt=""截圖 2020-10-29 上午11 12 18"" src=""https://user-images.githubusercontent.com/63279862/97521250-23409f00-19d8-11eb-8489-f0d0968d872d.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:98,usability,user,user-images,98,"Ｗhen I delete the “5000” , it works. <img width=""672"" alt=""截圖 2020-10-29 上午11 24 34"" src=""https://user-images.githubusercontent.com/63279862/97521833-5afc1680-19d9-11eb-8427-0c2b1abb7316.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:5,testability,plan,plan,5,I'll plan to take a look but might be later this week. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:46,availability,down,down,46,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:416,availability,error,error,416,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:743,deployability,version,version,743,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:3,energy efficiency,current,current,3,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:743,integrability,version,version,743,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:175,interoperability,format,format,175,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:197,interoperability,format,format,197,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:302,interoperability,format,format,302,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:552,interoperability,format,format,552,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:743,modifiability,version,version,743,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:416,performance,error,error,416,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:309,reliability,doe,doesn,309,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:416,safety,error,error,416,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:559,safety,detect,detection,559,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:146,security,ident,identify,146,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:559,security,detect,detection,559,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:885,security,team,teammates,885,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:416,usability,error,error,416,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:651,usability,person,personally,651,"My current investigation shows that this went down the path here:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:. https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105. But FAI format doesn't get read here, so it got into the last branch of the switch:. ```. default:. errno = EFTYPE;. goto error;. ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:145,availability,error,error,145,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:46,deployability,updat,update,46,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:134,deployability,fail,fails,134,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:65,integrability,messag,message,65,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:151,integrability,messag,message,151,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:65,interoperability,messag,message,65,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:151,interoperability,messag,message,151,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:145,performance,error,error,145,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:134,reliability,fail,fails,134,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:46,safety,updat,update,46,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:145,safety,error,error,145,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:46,security,updat,update,46,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:172,testability,understand,understandable,172,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:145,usability,error,error,145,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/376:113,energy efficiency,GPU,GPU,113,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:121,energy efficiency,CPU,CPU,121,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:446,energy efficiency,model,model,446,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:113,performance,GPU,GPU,113,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:121,performance,CPU,CPU,121,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:446,security,model,model,446,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:227,testability,simpl,simplify,227,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:37,usability,support,supports,37,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:189,usability,support,supports,189,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:227,usability,simpl,simplify,227,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:528,usability,feedback,feedback,528,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:580,usability,user,users,580,"Hi @anands-repo . Our codebase still supports TPU like before! We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial. If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know. (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:40,energy efficiency,GPU,GPU-based,40,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:83,energy efficiency,current,currently,83,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:115,energy efficiency,GPU,GPU,115,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:163,energy efficiency,GPU,GPU-based,163,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:40,performance,GPU,GPU-based,40,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:115,performance,GPU,GPU,115,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:163,performance,GPU,GPU-based,163,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:93,reliability,doe,doesn,93,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:20,usability,confirm,confirming,20,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:101,usability,support,support,101,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:139,usability,efficien,efficient,139,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:254,usability,feedback,feedback,254,"@pichuan Thanks for confirming! I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/376:60,usability,document,documentation,60,"Sounds good. You might also want to refer to the latest TPU documentation to see if anything changed, but overall it should still work. If you have any questions, feel free to open another issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376
https://github.com/google/deepvariant/issues/377:145,performance,time,timeframe,145,"Hi @sjin09 . Thank you for your question. We are actively working on a solution for this area (trio calling) and expect to have something in the timeframe of a few months from now. For now, the most accurate way to identify de novos with DeepVariant is to run DeepVariant v1.0, use GLnexus to merge the inputs, and identify de novo variants from the joint call file. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/377:303,safety,input,inputs,303,"Hi @sjin09 . Thank you for your question. We are actively working on a solution for this area (trio calling) and expect to have something in the timeframe of a few months from now. For now, the most accurate way to identify de novos with DeepVariant is to run DeepVariant v1.0, use GLnexus to merge the inputs, and identify de novo variants from the joint call file. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/377:215,security,ident,identify,215,"Hi @sjin09 . Thank you for your question. We are actively working on a solution for this area (trio calling) and expect to have something in the timeframe of a few months from now. For now, the most accurate way to identify de novos with DeepVariant is to run DeepVariant v1.0, use GLnexus to merge the inputs, and identify de novo variants from the joint call file. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/377:315,security,ident,identify,315,"Hi @sjin09 . Thank you for your question. We are actively working on a solution for this area (trio calling) and expect to have something in the timeframe of a few months from now. For now, the most accurate way to identify de novos with DeepVariant is to run DeepVariant v1.0, use GLnexus to merge the inputs, and identify de novo variants from the joint call file. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/377:303,usability,input,inputs,303,"Hi @sjin09 . Thank you for your question. We are actively working on a solution for this area (trio calling) and expect to have something in the timeframe of a few months from now. For now, the most accurate way to identify de novos with DeepVariant is to run DeepVariant v1.0, use GLnexus to merge the inputs, and identify de novo variants from the joint call file. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/377:76,usability,custom,custom,76,"Hello @AndrewCarroll . We look forward to the development. We will create a custom de novo mutation caller for now. Many thanks,. Sangjin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/377:4,deployability,updat,updates,4,Any updates regarding this?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/377:4,safety,updat,updates,4,Any updates regarding this?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/377:4,security,updat,updates,4,Any updates regarding this?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/378:184,testability,plan,plan,184,"There isn't an option right now but it should be possible to make changes to the code to do this. Unfortunately this is not a highly prioritized feature (even internally), so we won't plan to implement it anytime soon. If you end up implementing it, let us know! Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:442,availability,checkpoint,checkpoints,442,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:515,availability,checkpoint,checkpoint,515,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:595,availability,checkpoint,checkpoint,595,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:695,availability,checkpoint,checkpoint,695,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:281,deployability,log,logs,281,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:304,deployability,log,log,304,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:634,interoperability,coordinat,coordination,634,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:750,performance,concurren,concurrently,750,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:442,reliability,checkpoint,checkpoints,442,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:515,reliability,checkpoint,checkpoint,515,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:595,reliability,checkpoint,checkpoint,595,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:695,reliability,checkpoint,checkpoint,695,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:281,safety,log,logs,281,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:304,safety,log,log,304,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:281,security,log,logs,281,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:304,security,log,log,304,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:581,security,modif,modifies,581,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:281,testability,log,logs,281,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:304,testability,log,log,304,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:24,usability,help,helper,24,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:117,usability,tool,tools,117,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:. 1. Create a target empty working directory: `$WORKDIR`. 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`. 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/379:59,performance,perform,performs,59,"Hi, glad to see my issue is helping you. Since DeepVariant performs local-realignment on indel sites, it is possible that DeepVarianat sees a variant site differently with CIGAR in BAM file.As shown in channel 6 of this site, there's another site very close, sometimes DeepVariant treats an insertion-deletion haplotype as an snp-snp one. It would be helpful if you can provide the variant information along with the referred example.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:28,usability,help,helping,28,"Hi, glad to see my issue is helping you. Since DeepVariant performs local-realignment on indel sites, it is possible that DeepVarianat sees a variant site differently with CIGAR in BAM file.As shown in channel 6 of this site, there's another site very close, sometimes DeepVariant treats an insertion-deletion haplotype as an snp-snp one. It would be helpful if you can provide the variant information along with the referred example.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:59,usability,perform,performs,59,"Hi, glad to see my issue is helping you. Since DeepVariant performs local-realignment on indel sites, it is possible that DeepVarianat sees a variant site differently with CIGAR in BAM file.As shown in channel 6 of this site, there's another site very close, sometimes DeepVariant treats an insertion-deletion haplotype as an snp-snp one. It would be helpful if you can provide the variant information along with the referred example.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:252,usability,close,close,252,"Hi, glad to see my issue is helping you. Since DeepVariant performs local-realignment on indel sites, it is possible that DeepVarianat sees a variant site differently with CIGAR in BAM file.As shown in channel 6 of this site, there's another site very close, sometimes DeepVariant treats an insertion-deletion haplotype as an snp-snp one. It would be helpful if you can provide the variant information along with the referred example.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:351,usability,help,helpful,351,"Hi, glad to see my issue is helping you. Since DeepVariant performs local-realignment on indel sites, it is possible that DeepVarianat sees a variant site differently with CIGAR in BAM file.As shown in channel 6 of this site, there's another site very close, sometimes DeepVariant treats an insertion-deletion haplotype as an snp-snp one. It would be helpful if you can provide the variant information along with the referred example.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:554,testability,simpl,simple,554,"Thanks for the comment, @jumpyknight . What you suggest is interesting, but do you think it is plausible even if the insertion is of length 5? If that was the case, I would have expected to have more pixels 'lit up' as snps, or more pixeld 'darkened' as insertions, right after the position at which the insertion took place. However, no such behaviour takes place (referring again to the 6th channel). I'm not sure what infomation is relevant here so I'll post a bunch of stuff:. 1. The variant: as I said it is at chr20-10001435, it is labeled to be a simple SNP, hom-alt 1/1. 2. The bam-file read I mentioned: . - Starts at: 10001358. - Cigar: 78M, 5I, 18M. That means that we have. 10001358 ... 10001435 X X X X X 10001436 ... 10001453. M ... M I I I I I M ... M. Where M indicated Match and I indicates Insertion. - It is the forward read, with mapping quality 60, . - Has the following tags: [(RG, NA12878), (XT, U), (NM, 5), (SM, 37), (AM, 37), (X0, 1), (X1, 0), (XM, 0), (XO, 1), (XG, 5), (MD, 96)].",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:343,usability,behavi,behaviour,343,"Thanks for the comment, @jumpyknight . What you suggest is interesting, but do you think it is plausible even if the insertion is of length 5? If that was the case, I would have expected to have more pixels 'lit up' as snps, or more pixeld 'darkened' as insertions, right after the position at which the insertion took place. However, no such behaviour takes place (referring again to the 6th channel). I'm not sure what infomation is relevant here so I'll post a bunch of stuff:. 1. The variant: as I said it is at chr20-10001435, it is labeled to be a simple SNP, hom-alt 1/1. 2. The bam-file read I mentioned: . - Starts at: 10001358. - Cigar: 78M, 5I, 18M. That means that we have. 10001358 ... 10001435 X X X X X 10001436 ... 10001453. M ... M I I I I I M ... M. Where M indicated Match and I indicates Insertion. - It is the forward read, with mapping quality 60, . - Has the following tags: [(RG, NA12878), (XT, U), (NM, 5), (SM, 37), (AM, 37), (X0, 1), (X1, 0), (XM, 0), (XO, 1), (XG, 5), (MD, 96)].",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:554,usability,simpl,simple,554,"Thanks for the comment, @jumpyknight . What you suggest is interesting, but do you think it is plausible even if the insertion is of length 5? If that was the case, I would have expected to have more pixels 'lit up' as snps, or more pixeld 'darkened' as insertions, right after the position at which the insertion took place. However, no such behaviour takes place (referring again to the 6th channel). I'm not sure what infomation is relevant here so I'll post a bunch of stuff:. 1. The variant: as I said it is at chr20-10001435, it is labeled to be a simple SNP, hom-alt 1/1. 2. The bam-file read I mentioned: . - Starts at: 10001358. - Cigar: 78M, 5I, 18M. That means that we have. 10001358 ... 10001435 X X X X X 10001436 ... 10001453. M ... M I I I I I M ... M. Where M indicated Match and I indicates Insertion. - It is the forward read, with mapping quality 60, . - Has the following tags: [(RG, NA12878), (XT, U), (NM, 5), (SM, 37), (AM, 37), (X0, 1), (X1, 0), (XM, 0), (XO, 1), (XG, 5), (MD, 96)].",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:776,usability,indicat,indicated,776,"Thanks for the comment, @jumpyknight . What you suggest is interesting, but do you think it is plausible even if the insertion is of length 5? If that was the case, I would have expected to have more pixels 'lit up' as snps, or more pixeld 'darkened' as insertions, right after the position at which the insertion took place. However, no such behaviour takes place (referring again to the 6th channel). I'm not sure what infomation is relevant here so I'll post a bunch of stuff:. 1. The variant: as I said it is at chr20-10001435, it is labeled to be a simple SNP, hom-alt 1/1. 2. The bam-file read I mentioned: . - Starts at: 10001358. - Cigar: 78M, 5I, 18M. That means that we have. 10001358 ... 10001435 X X X X X 10001436 ... 10001453. M ... M I I I I I M ... M. Where M indicated Match and I indicates Insertion. - It is the forward read, with mapping quality 60, . - Has the following tags: [(RG, NA12878), (XT, U), (NM, 5), (SM, 37), (AM, 37), (X0, 1), (X1, 0), (XM, 0), (XO, 1), (XG, 5), (MD, 96)].",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:798,usability,indicat,indicates,798,"Thanks for the comment, @jumpyknight . What you suggest is interesting, but do you think it is plausible even if the insertion is of length 5? If that was the case, I would have expected to have more pixels 'lit up' as snps, or more pixeld 'darkened' as insertions, right after the position at which the insertion took place. However, no such behaviour takes place (referring again to the 6th channel). I'm not sure what infomation is relevant here so I'll post a bunch of stuff:. 1. The variant: as I said it is at chr20-10001435, it is labeled to be a simple SNP, hom-alt 1/1. 2. The bam-file read I mentioned: . - Starts at: 10001358. - Cigar: 78M, 5I, 18M. That means that we have. 10001358 ... 10001435 X X X X X 10001436 ... 10001453. M ... M I I I I I M ... M. Where M indicated Match and I indicates Insertion. - It is the forward read, with mapping quality 60, . - Has the following tags: [(RG, NA12878), (XT, U), (NM, 5), (SM, 37), (AM, 37), (X0, 1), (X1, 0), (XM, 0), (XO, 1), (XG, 5), (MD, 96)].",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:433,availability,consist,consistent,433,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:47,energy efficiency,model,models,47,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:315,energy efficiency,model,model,315,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:708,energy efficiency,optim,optimal,708,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1099,energy efficiency,model,models,1099,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1354,energy efficiency,model,models,1354,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1488,energy efficiency,model,models,1488,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:360,integrability,event,event,360,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:649,integrability,event,event,649,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:704,integrability,sub,sub-optimal,704,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:961,integrability,event,events,961,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1024,modifiability,Pac,PacBio,1024,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:87,performance,content,content,87,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:474,performance,content,content,474,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:786,performance,network,network,786,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:67,reliability,doe,does,67,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:47,security,model,models,47,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:315,security,model,model,315,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:786,security,network,network,786,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1099,security,model,models,1099,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1354,security,model,models,1354,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1488,security,model,models,1488,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:176,usability,indicat,indicates,176,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:249,usability,visual,visually,249,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:306,usability,learn,learning,306,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:433,usability,consist,consistent,433,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:546,usability,support,supports,546,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:34,testability,understand,understand,34,Thanks @AndrewCarroll . I think I understand the encoding better now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/380:243,deployability,build,building,243,"Hi @narendrachaudhary51, thanks for the question. The recommended way to run DeepVariant is with our Docker image (see [quickstart](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md)). We don't explicitly support building from source on CentOS 7, although it should be possible with some modifications. I can refer you to issues #95 and #29, which you may find useful. Closing this bug for now, feel free to reopen or submit new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:448,integrability,sub,submit,448,"Hi @narendrachaudhary51, thanks for the question. The recommended way to run DeepVariant is with our Docker image (see [quickstart](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md)). We don't explicitly support building from source on CentOS 7, although it should be possible with some modifications. I can refer you to issues #95 and #29, which you may find useful. Closing this bug for now, feel free to reopen or submit new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:318,security,modif,modifications,318,"Hi @narendrachaudhary51, thanks for the question. The recommended way to run DeepVariant is with our Docker image (see [quickstart](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md)). We don't explicitly support building from source on CentOS 7, although it should be possible with some modifications. I can refer you to issues #95 and #29, which you may find useful. Closing this bug for now, feel free to reopen or submit new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:235,usability,support,support,235,"Hi @narendrachaudhary51, thanks for the question. The recommended way to run DeepVariant is with our Docker image (see [quickstart](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md)). We don't explicitly support building from source on CentOS 7, although it should be possible with some modifications. I can refer you to issues #95 and #29, which you may find useful. Closing this bug for now, feel free to reopen or submit new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/381:268,availability,avail,available,268,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:425,availability,avail,available,425,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:497,modifiability,paramet,parameters,497,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:268,reliability,availab,available,268,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:425,reliability,availab,available,425,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:268,safety,avail,available,268,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:425,safety,avail,available,425,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:268,security,availab,available,268,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:425,security,availab,available,425,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:320,testability,trace,trace,320,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:399,availability,error,error,399,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:459,availability,checkpoint,checkpoint,459,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:492,availability,checkpoint,checkpoint,492,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:227,deployability,releas,released,227,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:34,energy efficiency,model,model,34,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:107,energy efficiency,model,model,107,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:126,energy efficiency,model,models,126,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:291,energy efficiency,model,model,291,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:567,energy efficiency,model,model,567,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:751,energy efficiency,model,model,751,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:580,interoperability,compatib,compatible,580,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:797,interoperability,compatib,compatible,797,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:27,modifiability,Pac,PacBio,27,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:399,performance,error,error,399,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:459,reliability,checkpoint,checkpoint,459,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:492,reliability,checkpoint,checkpoint,492,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:547,reliability,Doe,Does,547,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:399,safety,error,error,399,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:34,security,model,model,34,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:107,security,model,model,107,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:126,security,model,models,126,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:291,security,model,model,291,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:567,security,model,model,567,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:696,security,sign,significant,696,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:751,security,model,model,751,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:399,usability,error,error,399,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:202,deployability,version,versions,202,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:277,deployability,version,version,277,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:400,deployability,version,version,400,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:680,deployability,releas,release,680,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:181,energy efficiency,model,model,181,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:262,energy efficiency,model,model,262,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:362,energy efficiency,model,model,362,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:456,energy efficiency,model,model,456,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:202,integrability,version,versions,202,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:277,integrability,version,version,277,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:400,integrability,version,version,400,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:215,interoperability,compatib,compatible,215,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:202,modifiability,version,versions,202,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:277,modifiability,version,version,277,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:400,modifiability,version,version,400,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:727,modifiability,Pac,PacBio,727,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:181,security,model,model,181,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:262,security,model,model,262,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:362,security,model,model,362,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:456,security,model,model,456,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:425,testability,understand,understand,425,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:398,deployability,releas,release,398,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:122,energy efficiency,model,model,122,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:266,energy efficiency,model,model,266,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:369,energy efficiency,model,model,369,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:382,energy efficiency,current,current,382,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:362,modifiability,Pac,PacBio,362,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:188,performance,perform,performance,188,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:169,safety,test,test,169,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:429,safety,except,except,429,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:122,security,model,model,122,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:266,security,model,model,266,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:369,security,model,model,369,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:169,testability,test,test,169,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:188,usability,perform,performance,188,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:734,availability,checkpoint,checkpoint,734,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:15,deployability,releas,release,15,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:220,deployability,artifact,artifacts,220,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:358,deployability,version,version,358,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:393,deployability,version,version,393,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:7,energy efficiency,current,current,7,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:350,energy efficiency,current,current,350,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:504,energy efficiency,model,model,504,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:553,energy efficiency,model,model,553,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:759,energy efficiency,model,model,759,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:358,integrability,version,version,358,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:393,integrability,version,version,393,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:358,modifiability,version,version,358,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:393,modifiability,version,version,393,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:559,performance,perform,performance,559,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:656,performance,perform,performance,656,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:701,performance,tune,tune,701,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:734,reliability,checkpoint,checkpoint,734,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:595,safety,test,test,595,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:504,security,model,model,504,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:553,security,model,model,553,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:715,security,ident,identifies,715,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:759,security,model,model,759,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:405,testability,plan,plan,405,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:595,testability,test,test,595,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:559,usability,perform,performance,559,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:656,usability,perform,performance,656,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/383:44,deployability,observ,observe,44,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:266,energy efficiency,load,loaded,266,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:293,energy efficiency,model,model,293,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:413,energy efficiency,model,models,413,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:252,modifiability,variab,variables,252,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:266,performance,load,loaded,266,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:311,reliability,doe,does,311,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:293,security,model,model,293,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:413,security,model,models,413,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:44,testability,observ,observe,44,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:109,usability,user,users,109,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:327,usability,learn,learn,327,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/384:252,availability,sli,slight,252,"Hi @gevro . Yes, DeepVariant excludes reads that are marked with the duplicate flag in the BAM file. Empirically, we have found that marking duplicates makes very little difference in the samples we have looked at for 30x+ coverage. But it does have a slight effect on 15x-25x coverage range.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/384
https://github.com/google/deepvariant/issues/384:240,reliability,doe,does,240,"Hi @gevro . Yes, DeepVariant excludes reads that are marked with the duplicate flag in the BAM file. Empirically, we have found that marking duplicates makes very little difference in the samples we have looked at for 30x+ coverage. But it does have a slight effect on 15x-25x coverage range.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/384
https://github.com/google/deepvariant/issues/384:252,reliability,sli,slight,252,"Hi @gevro . Yes, DeepVariant excludes reads that are marked with the duplicate flag in the BAM file. Empirically, we have found that marking duplicates makes very little difference in the samples we have looked at for 30x+ coverage. But it does have a slight effect on 15x-25x coverage range.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/384
https://github.com/google/deepvariant/issues/384:223,testability,coverag,coverage,223,"Hi @gevro . Yes, DeepVariant excludes reads that are marked with the duplicate flag in the BAM file. Empirically, we have found that marking duplicates makes very little difference in the samples we have looked at for 30x+ coverage. But it does have a slight effect on 15x-25x coverage range.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/384
https://github.com/google/deepvariant/issues/384:277,testability,coverag,coverage,277,"Hi @gevro . Yes, DeepVariant excludes reads that are marked with the duplicate flag in the BAM file. Empirically, we have found that marking duplicates makes very little difference in the samples we have looked at for 30x+ coverage. But it does have a slight effect on 15x-25x coverage range.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/384
https://github.com/google/deepvariant/issues/385:17,availability,error,error,17,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:48,deployability,instal,installing,48,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:92,deployability,stack,stackoverflow,92,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:153,deployability,modul,module-named-setuptools,153,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:153,modifiability,modul,module-named-setuptools,153,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:17,performance,error,error,17,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:17,safety,error,error,17,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:153,safety,modul,module-named-setuptools,153,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:17,usability,error,error,17,"@gevro Given the error information, can you try installing setuptools? For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:29,availability,error,error,29,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:184,deployability,modul,module,184,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:318,deployability,modul,module,318,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:462,deployability,modul,module,462,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:790,deployability,modul,module,790,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:918,deployability,modul,module,918,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1217,deployability,modul,module,1217,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1415,deployability,modul,module,1415,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1703,deployability,Compos,CompositeTensor,1703,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1242,energy efficiency,core,core,1242,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1367,energy efficiency,core,core,1367,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:184,modifiability,modul,module,184,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:270,modifiability,pac,packages,270,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:318,modifiability,modul,module,318,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:410,modifiability,pac,packages,410,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:462,modifiability,modul,module,462,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:731,modifiability,pac,packages,731,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:790,modifiability,modul,module,790,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:865,modifiability,pac,packages,865,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:918,modifiability,modul,module,918,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1089,modifiability,pac,package,1089,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1158,modifiability,pac,packages,1158,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1217,modifiability,modul,module,1217,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1342,modifiability,pac,packages,1342,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1415,modifiability,modul,module,1415,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1703,modifiability,Compos,CompositeTensor,1703,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:29,performance,error,error,29,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:29,safety,error,error,29,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:184,safety,modul,module,184,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:318,safety,modul,module,318,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:462,safety,modul,module,462,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:790,safety,modul,module,790,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:918,safety,modul,module,918,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1217,safety,modul,module,1217,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:1415,safety,modul,module,1415,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:37,testability,Trace,Traceback,37,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:29,usability,error,error,29,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:494,usability,tool,tools,494,"I tried, but then I get this error:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>. import tensorflow as tf. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>. from tensorflow_core import *. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load. File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__. module = self._load(). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load. module = _importlib.import_module(self.__name__). File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>. from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo. File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>. serialized_options=None, file=DESCRIPTOR),. File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__. return _message.default_pool.FindFieldByName(full_name). KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:31,security,access,accessing,31,"For some reason singularity is accessing my local python libs, instead of the singularity image's libraries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:24,usability,command,command,24,@gevro can you send the command you are using to run singularity?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:336,availability,error,error,336,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ ~/bin/deepvariant/deepvariant_1.0.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=reference.fasta --reads=input.bam --regions=regions.bed --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz --num_shards=16. I also tried with the -e flag, and removing -B. Same error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:336,performance,error,error,336,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ ~/bin/deepvariant/deepvariant_1.0.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=reference.fasta --reads=input.bam --regions=regions.bed --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz --num_shards=16. I also tried with the -e flag, and removing -B. Same error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:177,safety,input,input,177,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ ~/bin/deepvariant/deepvariant_1.0.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=reference.fasta --reads=input.bam --regions=regions.bed --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz --num_shards=16. I also tried with the -e flag, and removing -B. Same error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:336,safety,error,error,336,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ ~/bin/deepvariant/deepvariant_1.0.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=reference.fasta --reads=input.bam --regions=regions.bed --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz --num_shards=16. I also tried with the -e flag, and removing -B. Same error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:177,usability,input,input,177,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ ~/bin/deepvariant/deepvariant_1.0.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=reference.fasta --reads=input.bam --regions=regions.bed --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz --num_shards=16. I also tried with the -e flag, and removing -B. Same error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:336,usability,error,error,336,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ ~/bin/deepvariant/deepvariant_1.0.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=reference.fasta --reads=input.bam --regions=regions.bed --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz --num_shards=16. I also tried with the -e flag, and removing -B. Same error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:30,deployability,Stack,Stackoverflow,30,"@gevro this may be related:. [Stackoverflow #62693948](https://stackoverflow.com/questions/62693948/why-does-mounting-the-home-directory-in-singularity-container-break-a-python-imp). What directory are you running singularity from? I have run into issues in the past running singularity from my home directory. Try the following:. (1) Move all data files to a dedicated folder if they are not already. (2) Move to that directory, and run singularity using `--cleanenv`, `--no-home`, and binding only the directory you are located in (`-B $(pwd):$(pwd)`):. ```bash. singularity run --cleanenv \. --no-home \. -B $(pwd):$(pwd) \. ~/bin/deepvariant/deepvariant_1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=reference.fasta \. --reads=input.bam \. --regions=regions.bed \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:63,deployability,stack,stackoverflow,63,"@gevro this may be related:. [Stackoverflow #62693948](https://stackoverflow.com/questions/62693948/why-does-mounting-the-home-directory-in-singularity-container-break-a-python-imp). What directory are you running singularity from? I have run into issues in the past running singularity from my home directory. Try the following:. (1) Move all data files to a dedicated folder if they are not already. (2) Move to that directory, and run singularity using `--cleanenv`, `--no-home`, and binding only the directory you are located in (`-B $(pwd):$(pwd)`):. ```bash. singularity run --cleanenv \. --no-home \. -B $(pwd):$(pwd) \. ~/bin/deepvariant/deepvariant_1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=reference.fasta \. --reads=input.bam \. --regions=regions.bed \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:152,deployability,contain,container-break-a-python-imp,152,"@gevro this may be related:. [Stackoverflow #62693948](https://stackoverflow.com/questions/62693948/why-does-mounting-the-home-directory-in-singularity-container-break-a-python-imp). What directory are you running singularity from? I have run into issues in the past running singularity from my home directory. Try the following:. (1) Move all data files to a dedicated folder if they are not already. (2) Move to that directory, and run singularity using `--cleanenv`, `--no-home`, and binding only the directory you are located in (`-B $(pwd):$(pwd)`):. ```bash. singularity run --cleanenv \. --no-home \. -B $(pwd):$(pwd) \. ~/bin/deepvariant/deepvariant_1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=reference.fasta \. --reads=input.bam \. --regions=regions.bed \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:487,interoperability,bind,binding,487,"@gevro this may be related:. [Stackoverflow #62693948](https://stackoverflow.com/questions/62693948/why-does-mounting-the-home-directory-in-singularity-container-break-a-python-imp). What directory are you running singularity from? I have run into issues in the past running singularity from my home directory. Try the following:. (1) Move all data files to a dedicated folder if they are not already. (2) Move to that directory, and run singularity using `--cleanenv`, `--no-home`, and binding only the directory you are located in (`-B $(pwd):$(pwd)`):. ```bash. singularity run --cleanenv \. --no-home \. -B $(pwd):$(pwd) \. ~/bin/deepvariant/deepvariant_1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=reference.fasta \. --reads=input.bam \. --regions=regions.bed \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:487,modifiability,bind,binding,487,"@gevro this may be related:. [Stackoverflow #62693948](https://stackoverflow.com/questions/62693948/why-does-mounting-the-home-directory-in-singularity-container-break-a-python-imp). What directory are you running singularity from? I have run into issues in the past running singularity from my home directory. Try the following:. (1) Move all data files to a dedicated folder if they are not already. (2) Move to that directory, and run singularity using `--cleanenv`, `--no-home`, and binding only the directory you are located in (`-B $(pwd):$(pwd)`):. ```bash. singularity run --cleanenv \. --no-home \. -B $(pwd):$(pwd) \. ~/bin/deepvariant/deepvariant_1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=reference.fasta \. --reads=input.bam \. --regions=regions.bed \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:104,reliability,doe,does-mounting-the-home-directory-in-singularity-container-break-a-python-imp,104,"@gevro this may be related:. [Stackoverflow #62693948](https://stackoverflow.com/questions/62693948/why-does-mounting-the-home-directory-in-singularity-container-break-a-python-imp). What directory are you running singularity from? I have run into issues in the past running singularity from my home directory. Try the following:. (1) Move all data files to a dedicated folder if they are not already. (2) Move to that directory, and run singularity using `--cleanenv`, `--no-home`, and binding only the directory you are located in (`-B $(pwd):$(pwd)`):. ```bash. singularity run --cleanenv \. --no-home \. -B $(pwd):$(pwd) \. ~/bin/deepvariant/deepvariant_1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=reference.fasta \. --reads=input.bam \. --regions=regions.bed \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:764,safety,input,input,764,"@gevro this may be related:. [Stackoverflow #62693948](https://stackoverflow.com/questions/62693948/why-does-mounting-the-home-directory-in-singularity-container-break-a-python-imp). What directory are you running singularity from? I have run into issues in the past running singularity from my home directory. Try the following:. (1) Move all data files to a dedicated folder if they are not already. (2) Move to that directory, and run singularity using `--cleanenv`, `--no-home`, and binding only the directory you are located in (`-B $(pwd):$(pwd)`):. ```bash. singularity run --cleanenv \. --no-home \. -B $(pwd):$(pwd) \. ~/bin/deepvariant/deepvariant_1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=reference.fasta \. --reads=input.bam \. --regions=regions.bed \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:764,usability,input,input,764,"@gevro this may be related:. [Stackoverflow #62693948](https://stackoverflow.com/questions/62693948/why-does-mounting-the-home-directory-in-singularity-container-break-a-python-imp). What directory are you running singularity from? I have run into issues in the past running singularity from my home directory. Try the following:. (1) Move all data files to a dedicated folder if they are not already. (2) Move to that directory, and run singularity using `--cleanenv`, `--no-home`, and binding only the directory you are located in (`-B $(pwd):$(pwd)`):. ```bash. singularity run --cleanenv \. --no-home \. -B $(pwd):$(pwd) \. ~/bin/deepvariant/deepvariant_1.0.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=reference.fasta \. --reads=input.bam \. --regions=regions.bed \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz \. --num_shards=16. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:159,deployability,contain,container,159,"Yup that fixed it! Singularity is a fairly problematic system given the complexity of how it treats the native environment and folders/permissions outside the container. I wish it was made more seamless like docker, because it's a hassle to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:72,safety,compl,complexity,72,"Yup that fixed it! Singularity is a fairly problematic system given the complexity of how it treats the native environment and folders/permissions outside the container. I wish it was made more seamless like docker, because it's a hassle to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:135,safety,permiss,permissions,135,"Yup that fixed it! Singularity is a fairly problematic system given the complexity of how it treats the native environment and folders/permissions outside the container. I wish it was made more seamless like docker, because it's a hassle to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/385:72,security,compl,complexity,72,"Yup that fixed it! Singularity is a fairly problematic system given the complexity of how it treats the native environment and folders/permissions outside the container. I wish it was made more seamless like docker, because it's a hassle to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/386:234,deployability,stage,stage,234,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:485,energy efficiency,model,model,485,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:503,energy efficiency,estimat,estimate,503,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:430,interoperability,specif,specifically,430,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:270,performance,network,network,270,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:133,security,ident,identifies,133,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:270,security,network,network,270,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:485,security,model,model,485,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:101,testability,simpl,simple,101,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:101,usability,simpl,simple,101,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:200,availability,down,downstream,200,"Thank you! I'm still curious-- if RefCall entries are rejected by 'call_variants', why are they still reported in the final VCF? Do they enable some sort of helpful QC checks? If so, which? Since any downstream application would filter these out, I'm a bit unclear why an internal variant nomination process is included in the final output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:229,integrability,filter,filter,229,"Thank you! I'm still curious-- if RefCall entries are rejected by 'call_variants', why are they still reported in the final VCF? Do they enable some sort of helpful QC checks? If so, which? Since any downstream application would filter these out, I'm a bit unclear why an internal variant nomination process is included in the final output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:157,usability,help,helpful,157,"Thank you! I'm still curious-- if RefCall entries are rejected by 'call_variants', why are they still reported in the final VCF? Do they enable some sort of helpful QC checks? If so, which? Since any downstream application would filter these out, I'm a bit unclear why an internal variant nomination process is included in the final output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:382,integrability,filter,filtering,382,"Hi @gevro. The inclusion of RefCall allows us to have a full record of every variant that DeepVariant was asked to classify. This is useful for QC (helping to distinguish between not enough coverage to nominate a candidate versus some other issue making the position difficult. Since the confidence values of DeepVariant are well-calibrated, It allows users to select for different filtering criteria in the event they want to increase recall.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:408,integrability,event,event,408,"Hi @gevro. The inclusion of RefCall allows us to have a full record of every variant that DeepVariant was asked to classify. This is useful for QC (helping to distinguish between not enough coverage to nominate a candidate versus some other issue making the position difficult. Since the confidence values of DeepVariant are well-calibrated, It allows users to select for different filtering criteria in the event they want to increase recall.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:190,testability,coverag,coverage,190,"Hi @gevro. The inclusion of RefCall allows us to have a full record of every variant that DeepVariant was asked to classify. This is useful for QC (helping to distinguish between not enough coverage to nominate a candidate versus some other issue making the position difficult. Since the confidence values of DeepVariant are well-calibrated, It allows users to select for different filtering criteria in the event they want to increase recall.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:148,usability,help,helping,148,"Hi @gevro. The inclusion of RefCall allows us to have a full record of every variant that DeepVariant was asked to classify. This is useful for QC (helping to distinguish between not enough coverage to nominate a candidate versus some other issue making the position difficult. Since the confidence values of DeepVariant are well-calibrated, It allows users to select for different filtering criteria in the event they want to increase recall.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/386:352,usability,user,users,352,"Hi @gevro. The inclusion of RefCall allows us to have a full record of every variant that DeepVariant was asked to classify. This is useful for QC (helping to distinguish between not enough coverage to nominate a candidate versus some other issue making the position difficult. Since the confidence values of DeepVariant are well-calibrated, It allows users to select for different filtering criteria in the event they want to increase recall.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386
https://github.com/google/deepvariant/issues/388:104,availability,sli,slice,104,"Hi @gevro . Is the snippet of the BAM file share-able? If so, would you be able to post just this small slice of it here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:43,interoperability,share,share-able,43,"Hi @gevro . Is the snippet of the BAM file share-able? If so, would you be able to post just this small slice of it here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:104,reliability,sli,slice,104,"Hi @gevro . Is the snippet of the BAM file share-able? If so, would you be able to post just this small slice of it here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:65,usability,feedback,feedback,65,"Thank you, we're taking a look at the example and expect to have feedback relatively soon (with some delay for holidays in the US).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:225,modifiability,paramet,parameters,225,"We tried to reproduce the problem with the sample.bam. From the original post it was not obvious what variant was missing. I assume it is the one at position chr3:76,275,581. Here is what we get when running with all default parameters:. ```. chr3 76275186 . A C 64.3 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 60.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:59:37:0,36:0.972973:60,64,0. chr3 76275581 . C CAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGT 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:19:38:0,12:0.315789:0,34,18. chr3 76275753 . T A 66.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:61:41:0,41:1:66,62,0. chr3 76275830 . T C 96.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:96,67,0. chr3 76275879 . A C 40.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. The variant at position chr3:76,275,581 is an insertion which doesn't match the original alignment in sample.bam. That happened as a result of a local realignment (which is a part of a default make_example workflow). The image below shows the original (top) and realigned (bottom) reads for this region. ![image](https://user-images.githubusercontent.com/1168691/100527570-8dd82c80-3188-11eb-9880-740b724fef43.png). We tried to run make_examples without local realignement `--norealign_reads` flag. In that case all 6 variants are reported as expected. ```. chr3 76275186 . A C 64.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 61.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:60:36:0,36:1:61,65,0. chr3 76275581 . C T 62.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:58:38:0,38:1:62,60,0. chr3 76275753 . T A 68.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:68,64,0. chr3 76275830 . T C 99 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:99,67,0. chr3 76275879 . A C 40.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. It is not recommenced to run DeepVariant with Illumna data with `--norealign_reads` since accuracy drops significantly when local realignment is skipped. In most of the cases (if not all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:844,reliability,doe,doesn,844,"We tried to reproduce the problem with the sample.bam. From the original post it was not obvious what variant was missing. I assume it is the one at position chr3:76,275,581. Here is what we get when running with all default parameters:. ```. chr3 76275186 . A C 64.3 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 60.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:59:37:0,36:0.972973:60,64,0. chr3 76275581 . C CAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGT 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:19:38:0,12:0.315789:0,34,18. chr3 76275753 . T A 66.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:61:41:0,41:1:66,62,0. chr3 76275830 . T C 96.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:96,67,0. chr3 76275879 . A C 40.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. The variant at position chr3:76,275,581 is an insertion which doesn't match the original alignment in sample.bam. That happened as a result of a local realignment (which is a part of a default make_example workflow). The image below shows the original (top) and realigned (bottom) reads for this region. ![image](https://user-images.githubusercontent.com/1168691/100527570-8dd82c80-3188-11eb-9880-740b724fef43.png). We tried to run make_examples without local realignement `--norealign_reads` flag. In that case all 6 variants are reported as expected. ```. chr3 76275186 . A C 64.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 61.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:60:36:0,36:1:61,65,0. chr3 76275581 . C T 62.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:58:38:0,38:1:62,60,0. chr3 76275753 . T A 68.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:68,64,0. chr3 76275830 . T C 99 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:99,67,0. chr3 76275879 . A C 40.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. It is not recommenced to run DeepVariant with Illumna data with `--norealign_reads` since accuracy drops significantly when local realignment is skipped. In most of the cases (if not all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:1919,security,sign,significantly,1919,"Here is what we get when running with all default parameters:. ```. chr3 76275186 . A C 64.3 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 60.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:59:37:0,36:0.972973:60,64,0. chr3 76275581 . C CAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGT 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:19:38:0,12:0.315789:0,34,18. chr3 76275753 . T A 66.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:61:41:0,41:1:66,62,0. chr3 76275830 . T C 96.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:96,67,0. chr3 76275879 . A C 40.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. The variant at position chr3:76,275,581 is an insertion which doesn't match the original alignment in sample.bam. That happened as a result of a local realignment (which is a part of a default make_example workflow). The image below shows the original (top) and realigned (bottom) reads for this region. ![image](https://user-images.githubusercontent.com/1168691/100527570-8dd82c80-3188-11eb-9880-740b724fef43.png). We tried to run make_examples without local realignement `--norealign_reads` flag. In that case all 6 variants are reported as expected. ```. chr3 76275186 . A C 64.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 61.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:60:36:0,36:1:61,65,0. chr3 76275581 . C T 62.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:58:38:0,38:1:62,60,0. chr3 76275753 . T A 68.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:68,64,0. chr3 76275830 . T C 99 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:99,67,0. chr3 76275879 . A C 40.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. It is not recommenced to run DeepVariant with Illumna data with `--norealign_reads` since accuracy drops significantly when local realignment is skipped. In most of the cases (if not all) local realignment leads to the most accurate reads alignment. In this particular case it looks like the given set of reads represents an insertion at position chr3:76275581.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:988,usability,workflow,workflow,988,"We tried to reproduce the problem with the sample.bam. From the original post it was not obvious what variant was missing. I assume it is the one at position chr3:76,275,581. Here is what we get when running with all default parameters:. ```. chr3 76275186 . A C 64.3 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 60.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:59:37:0,36:0.972973:60,64,0. chr3 76275581 . C CAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGT 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:19:38:0,12:0.315789:0,34,18. chr3 76275753 . T A 66.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:61:41:0,41:1:66,62,0. chr3 76275830 . T C 96.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:96,67,0. chr3 76275879 . A C 40.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. The variant at position chr3:76,275,581 is an insertion which doesn't match the original alignment in sample.bam. That happened as a result of a local realignment (which is a part of a default make_example workflow). The image below shows the original (top) and realigned (bottom) reads for this region. ![image](https://user-images.githubusercontent.com/1168691/100527570-8dd82c80-3188-11eb-9880-740b724fef43.png). We tried to run make_examples without local realignement `--norealign_reads` flag. In that case all 6 variants are reported as expected. ```. chr3 76275186 . A C 64.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 61.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:60:36:0,36:1:61,65,0. chr3 76275581 . C T 62.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:58:38:0,38:1:62,60,0. chr3 76275753 . T A 68.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:68,64,0. chr3 76275830 . T C 99 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:99,67,0. chr3 76275879 . A C 40.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. It is not recommenced to run DeepVariant with Illumna data with `--norealign_reads` since accuracy drops significantly when local realignment is skipped. In most of the cases (if not all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:1103,usability,user,user-images,1103," was missing. I assume it is the one at position chr3:76,275,581. Here is what we get when running with all default parameters:. ```. chr3 76275186 . A C 64.3 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 60.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:59:37:0,36:0.972973:60,64,0. chr3 76275581 . C CAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGT 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:19:38:0,12:0.315789:0,34,18. chr3 76275753 . T A 66.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:61:41:0,41:1:66,62,0. chr3 76275830 . T C 96.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:96,67,0. chr3 76275879 . A C 40.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. The variant at position chr3:76,275,581 is an insertion which doesn't match the original alignment in sample.bam. That happened as a result of a local realignment (which is a part of a default make_example workflow). The image below shows the original (top) and realigned (bottom) reads for this region. ![image](https://user-images.githubusercontent.com/1168691/100527570-8dd82c80-3188-11eb-9880-740b724fef43.png). We tried to run make_examples without local realignement `--norealign_reads` flag. In that case all 6 variants are reported as expected. ```. chr3 76275186 . A C 64.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:62:43:0,43:1:64,66,0. chr3 76275271 . CT C 61.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:60:36:0,36:1:61,65,0. chr3 76275581 . C T 62.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:58:38:0,38:1:62,60,0. chr3 76275753 . T A 68.2 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:68,64,0. chr3 76275830 . T C 99 PASS . GT:GQ:DP:AD:VAF:PL 1/1:67:39:0,39:1:99,67,0. chr3 76275879 . A C 40.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:41:34:21,13:0.382353:40,0,60. ```. It is not recommenced to run DeepVariant with Illumna data with `--norealign_reads` since accuracy drops significantly when local realignment is skipped. In most of the cases (if not all) local realignment leads to the most accurate reads alignment. In this particular case it looks like the giv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:389,modifiability,Pac,PacBio,389,"Thanks so much! I think I understand the issue now, but Deepvariant is still giving a wrong answer. You can see that this particular location in the genome has a tandem repeat of 6 copies of a 47 bp sequence:. https://genome.ucsc.edu/cgi-bin/hgc?hgsid=960400993_qiSzxsvvUkaPrDYeYJ2KhGLAK2ay&c=chr3&l=76220845&r=76221817&o=76221234&t=76221509&g=simpleRepeat&i=trf. But I actually have long PacBio reads for this sample, with the zmw consensus sequence at the same location of:. TGAGCTTAATCATAGAACATGGTAATACTAGGAGACATCATGAAGGATCCCTGTGTTGTAGATATACTCTTCTTTACTTCCATTGAGAAGTAGTAGTTCAATTTCCCCAGGTAGTCTGAATCAATAACCCCAGGCAATATTGACTGTTTCTGTGGTGAAAGCATTCCTCCATCTAGAACTAAGTCCTCTTGCCCAACAGAAGATAAAGTCATGAGCATGGGAAGCAAAAATTTTGCTAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:2563,reliability,doe,does,2563,"GCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGTTTTAGGTATTTTCTATCCAGCAGGAGACTGCCTTTCCCTGGCACTGTCTGTGACCAATTATTACTTTAGAAAGACAGTTAACAACCATCTGACCATCACCTGATGGTGCCTGACATTCCTGGTGTGTGTGGGTGGGGGAGAGCCCTCTCCTACCCGGCTCATACCAGACTACTATAACAGATCCATGTCAATAAATTCAGACTGATCCAAGTTTATGTTTGTTTCACCATTATCCCACACTTTTTTTTTTTCAGATGGAGTCTCCTTCTGTCGCCCACGCTGGGGTGCAGTGGCATGATCTGGGCTCACTGCAACA. You can see that this maps all the way across this tandem repeat, and there is definitely not an insertion. Rather, there is an SNV. So it looks like the local realignment if calling an insertion instead of an SNV, maybe because the next repeat does not have the SNV so it prefer an indel instead of an SNV. I would think that local realignment has a higher penalty for large insertions than one SNV, so I'm not sure why local realignment settled on a large insertion?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:26,testability,understand,understand,26,"Thanks so much! I think I understand the issue now, but Deepvariant is still giving a wrong answer. You can see that this particular location in the genome has a tandem repeat of 6 copies of a 47 bp sequence:. https://genome.ucsc.edu/cgi-bin/hgc?hgsid=960400993_qiSzxsvvUkaPrDYeYJ2KhGLAK2ay&c=chr3&l=76220845&r=76221817&o=76221234&t=76221509&g=simpleRepeat&i=trf. But I actually have long PacBio reads for this sample, with the zmw consensus sequence at the same location of:. TGAGCTTAATCATAGAACATGGTAATACTAGGAGACATCATGAAGGATCCCTGTGTTGTAGATATACTCTTCTTTACTTCCATTGAGAAGTAGTAGTTCAATTTCCCCAGGTAGTCTGAATCAATAACCCCAGGCAATATTGACTGTTTCTGTGGTGAAAGCATTCCTCCATCTAGAACTAAGTCCTCTTGCCCAACAGAAGATAAAGTCATGAGCATGGGAAGCAAAAATTTTGCTAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:344,testability,simpl,simpleRepeat,344,"Thanks so much! I think I understand the issue now, but Deepvariant is still giving a wrong answer. You can see that this particular location in the genome has a tandem repeat of 6 copies of a 47 bp sequence:. https://genome.ucsc.edu/cgi-bin/hgc?hgsid=960400993_qiSzxsvvUkaPrDYeYJ2KhGLAK2ay&c=chr3&l=76220845&r=76221817&o=76221234&t=76221509&g=simpleRepeat&i=trf. But I actually have long PacBio reads for this sample, with the zmw consensus sequence at the same location of:. TGAGCTTAATCATAGAACATGGTAATACTAGGAGACATCATGAAGGATCCCTGTGTTGTAGATATACTCTTCTTTACTTCCATTGAGAAGTAGTAGTTCAATTTCCCCAGGTAGTCTGAATCAATAACCCCAGGCAATATTGACTGTTTCTGTGGTGAAAGCATTCCTCCATCTAGAACTAAGTCCTCTTGCCCAACAGAAGATAAAGTCATGAGCATGGGAAGCAAAAATTTTGCTAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:344,usability,simpl,simpleRepeat,344,"Thanks so much! I think I understand the issue now, but Deepvariant is still giving a wrong answer. You can see that this particular location in the genome has a tandem repeat of 6 copies of a 47 bp sequence:. https://genome.ucsc.edu/cgi-bin/hgc?hgsid=960400993_qiSzxsvvUkaPrDYeYJ2KhGLAK2ay&c=chr3&l=76220845&r=76221817&o=76221234&t=76221509&g=simpleRepeat&i=trf. But I actually have long PacBio reads for this sample, with the zmw consensus sequence at the same location of:. TGAGCTTAATCATAGAACATGGTAATACTAGGAGACATCATGAAGGATCCCTGTGTTGTAGATATACTCTTCTTTACTTCCATTGAGAAGTAGTAGTTCAATTTCCCCAGGTAGTCTGAATCAATAACCCCAGGCAATATTGACTGTTTCTGTGGTGAAAGCATTCCTCCATCTAGAACTAAGTCCTCTTGCCCAACAGAAGATAAAGTCATGAGCATGGGAAGCAAAAATTTTGCTAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:2591,usability,prefer,prefer,2591,"GCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGTTTTAGGTATTTTCTATCCAGCAGGAGACTGCCTTTCCCTGGCACTGTCTGTGACCAATTATTACTTTAGAAAGACAGTTAACAACCATCTGACCATCACCTGATGGTGCCTGACATTCCTGGTGTGTGTGGGTGGGGGAGAGCCCTCTCCTACCCGGCTCATACCAGACTACTATAACAGATCCATGTCAATAAATTCAGACTGATCCAAGTTTATGTTTGTTTCACCATTATCCCACACTTTTTTTTTTTCAGATGGAGTCTCCTTCTGTCGCCCACGCTGGGGTGCAGTGGCATGATCTGGGCTCACTGCAACA. You can see that this maps all the way across this tandem repeat, and there is definitely not an insertion. Rather, there is an SNV. So it looks like the local realignment if calling an insertion instead of an SNV, maybe because the next repeat does not have the SNV so it prefer an indel instead of an SNV. I would think that local realignment has a higher penalty for large insertions than one SNV, so I'm not sure why local realignment settled on a large insertion?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:24,deployability,build,builds,24,"Local realignment first builds a DeBruijn graph by aligning all read k-mers. There is a graph output from make_examples (I didn't check it) which can show how the final graph looks like. In this particular case most likely graph was build with multiple haplotypes: with insertion and with a SNP. Then, when reads are realigned, they are realigned to haplotypes first. Most likely reads were better aligned to the insertion haplotype. Most likely reads could be aligned to the insertion haplotype with zero mismatches. That's why the final alignment was chosen like that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:233,deployability,build,build,233,"Local realignment first builds a DeBruijn graph by aligning all read k-mers. There is a graph output from make_examples (I didn't check it) which can show how the final graph looks like. In this particular case most likely graph was build with multiple haplotypes: with insertion and with a SNP. Then, when reads are realigned, they are realigned to haplotypes first. Most likely reads were better aligned to the insertion haplotype. Most likely reads could be aligned to the insertion haplotype with zero mismatches. That's why the final alignment was chosen like that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/388:506,interoperability,mismatch,mismatches,506,"Local realignment first builds a DeBruijn graph by aligning all read k-mers. There is a graph output from make_examples (I didn't check it) which can show how the final graph looks like. In this particular case most likely graph was build with multiple haplotypes: with insertion and with a SNP. Then, when reads are realigned, they are realigned to haplotypes first. Most likely reads were better aligned to the insertion haplotype. Most likely reads could be aligned to the insertion haplotype with zero mismatches. That's why the final alignment was chosen like that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/390:224,availability,error,error,224,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:224,performance,error,error,224,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:224,safety,error,error,224,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:348,safety,input,input,348,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:485,safety,input,input,485,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:515,safety,input,input,515,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:24,usability,command,command,24,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:58,usability,command,commands,58,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:118,usability,command,command,118,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:183,usability,command,command,183,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:224,usability,error,error,224,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:348,usability,input,input,348,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:485,usability,input,input,485,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:515,usability,input,input,515,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```. BAMName=""VK446chrYx19"". INPUT_DIR=""${PWD}/"". OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \. -v ""${INPUT_DIR}:/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""1.0.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/Hg19ChrY.fa \. --reads=/input/${BAMName}.bam \. --output_vcf=/output/${BAMName}.vcf.gz \. --output_gvcf=/output/${BAMName}.g.vcf.gz \. --num_shards=24. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/390:49,usability,close,close,49,"@marcovth I'm assuming this is resolved, so I'll close this issue. Feel free to reopen if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390
https://github.com/google/deepvariant/issues/391:5,deployability,version,version,5,Your version of CentOS does not contain a recent enough GLIBC. It is not listed in your strings output. To my knowledge there is no way to change this without updating your OS. . I have had success using singularity without sudo to run the Docker image as described here: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:32,deployability,contain,contain,32,Your version of CentOS does not contain a recent enough GLIBC. It is not listed in your strings output. To my knowledge there is no way to change this without updating your OS. . I have had success using singularity without sudo to run the Docker image as described here: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:159,deployability,updat,updating,159,Your version of CentOS does not contain a recent enough GLIBC. It is not listed in your strings output. To my knowledge there is no way to change this without updating your OS. . I have had success using singularity without sudo to run the Docker image as described here: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:5,integrability,version,version,5,Your version of CentOS does not contain a recent enough GLIBC. It is not listed in your strings output. To my knowledge there is no way to change this without updating your OS. . I have had success using singularity without sudo to run the Docker image as described here: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:5,modifiability,version,version,5,Your version of CentOS does not contain a recent enough GLIBC. It is not listed in your strings output. To my knowledge there is no way to change this without updating your OS. . I have had success using singularity without sudo to run the Docker image as described here: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:23,reliability,doe,does,23,Your version of CentOS does not contain a recent enough GLIBC. It is not listed in your strings output. To my knowledge there is no way to change this without updating your OS. . I have had success using singularity without sudo to run the Docker image as described here: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:159,safety,updat,updating,159,Your version of CentOS does not contain a recent enough GLIBC. It is not listed in your strings output. To my knowledge there is no way to change this without updating your OS. . I have had success using singularity without sudo to run the Docker image as described here: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:159,security,updat,updating,159,Your version of CentOS does not contain a recent enough GLIBC. It is not listed in your strings output. To my knowledge there is no way to change this without updating your OS. . I have had success using singularity without sudo to run the Docker image as described here: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/392:285,availability,error,errors,285,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:112,energy efficiency,model,model,112,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:468,energy efficiency,model,model,468,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:105,modifiability,Pac,PacBio,105,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:508,modifiability,Pac,PacBio,508,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:285,performance,error,errors,285,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:285,safety,error,errors,285,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:112,security,model,model,112,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:468,security,model,model,468,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:285,usability,error,errors,285,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:323,usability,help,help,323,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:656,usability,command,command,656,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/pull/393:217,deployability,updat,updating,217,"@dkurt FYI, we might not incorporate the changes with threading. I am a bit wary of it in general. If there's any reason I should consider it (speed?), let know. I have one separate question for you: I'm looking into updating to Ubuntu18.04 (context: see https://github.com/google/deepvariant/issues/394 ). What should I use instead of `intel-openvino-dev-ubuntu16-2020.4.287` here https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/run-prereq.sh#L239 ? . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:217,safety,updat,updating,217,"@dkurt FYI, we might not incorporate the changes with threading. I am a bit wary of it in general. If there's any reason I should consider it (speed?), let know. I have one separate question for you: I'm looking into updating to Ubuntu18.04 (context: see https://github.com/google/deepvariant/issues/394 ). What should I use instead of `intel-openvino-dev-ubuntu16-2020.4.287` here https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/run-prereq.sh#L239 ? . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:217,security,updat,updating,217,"@dkurt FYI, we might not incorporate the changes with threading. I am a bit wary of it in general. If there's any reason I should consider it (speed?), let know. I have one separate question for you: I'm looking into updating to Ubuntu18.04 (context: see https://github.com/google/deepvariant/issues/394 ). What should I use instead of `intel-openvino-dev-ubuntu16-2020.4.287` here https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/run-prereq.sh#L239 ? . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:242,testability,context,context,242,"@dkurt FYI, we might not incorporate the changes with threading. I am a bit wary of it in general. If there's any reason I should consider it (speed?), let know. I have one separate question for you: I'm looking into updating to Ubuntu18.04 (context: see https://github.com/google/deepvariant/issues/394 ). What should I use instead of `intel-openvino-dev-ubuntu16-2020.4.287` here https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/run-prereq.sh#L239 ? . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:248,availability,avail,available,248,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:451,availability,echo,echo,451,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:213,deployability,version,version,213,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:591,deployability,updat,update,591,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:612,deployability,instal,install,612,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:628,deployability,instal,install-recommends,628,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:213,integrability,version,version,213,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:293,integrability,PUB,PUB-KEY-INTEL-OPENVINO-,293,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:367,integrability,PUB,PUB-KEY-INTEL-OPENVINO-,367,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:417,integrability,PUB,PUB-KEY-INTEL-OPENVINO-,417,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:213,modifiability,version,version,213,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:184,performance,perform,perform,184,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:248,reliability,availab,available,248,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:248,safety,avail,available,248,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:591,safety,updat,update,591,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:248,security,availab,available,248,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:329,security,apt,apt,329,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:401,security,apt,apt-key,401,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:469,security,apt,apt,469,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:533,security,apt,apt,533,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:583,security,apt,apt-get,583,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:591,security,updat,update,591,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:604,security,apt,apt-get,604,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:184,usability,perform,perform,184,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```. sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021. sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list. sudo apt-get update. sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110. sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:86,deployability,log,logging,86,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:78,energy efficiency,current,current,78,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:105,energy efficiency,optim,optimal,105,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:171,performance,time,time,171,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:86,safety,log,logging,86,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:86,security,log,logging,86,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:86,testability,log,logging,86,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/issues/394:101,deployability,build,build,101,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:227,deployability,build,build-prereq,227,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:499,deployability,build,build,499,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:643,deployability,Releas,Releases,643,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:688,deployability,updat,update,688,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:719,deployability,build,build,719,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:751,deployability,releas,releases,751,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:948,deployability,updat,updating,948,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:1036,deployability,build,build,1036,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:175,energy efficiency,Current,Currently,175,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:661,energy efficiency,current,currently,661,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:590,interoperability,standard,standard,590,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:710,interoperability,standard,standard,710,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:808,interoperability,standard,standard,808,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:989,interoperability,share,share,989,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:363,modifiability,maintain,maintained,363,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:312,performance,time,time,312,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:786,performance,time,time,786,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:205,safety,test,tested,205,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:281,safety,test,test,281,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:352,safety,test,tested,352,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:363,safety,maintain,maintained,363,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:688,safety,updat,update,688,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:849,safety,test,test,849,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:937,safety,test,testing,937,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:948,safety,updat,updating,948,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:688,security,updat,update,688,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:948,security,updat,updating,948,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:57,testability,understand,understand,57,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:205,testability,test,tested,205,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:281,testability,test,test,281,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:352,testability,test,tested,352,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:849,testability,test,test,849,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:937,testability,test,testing,937,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:47,usability,confirm,confirm,47,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:599,usability,support,support,599,"@Stikus Thanks for reporting the issue. Let me confirm I understand the issue correctly-. You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct? Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts. As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:21,deployability,build,build,21,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:32,deployability,releas,releases,32,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:137,deployability,version,version,137,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:167,deployability,instal,install,167,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:323,deployability,build,build,323,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:137,integrability,version,version,137,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:137,modifiability,version,version,137,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:103,performance,time,time,103,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:435,reliability,doe,doesn,435,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:542,usability,support,support,542,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:332,availability,echo,echo,332,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:623,availability,state,statement,623,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:15,deployability,updat,update,15,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:338,deployability,Instal,Installing,338,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:414,deployability,instal,install,414,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:501,deployability,instal,install,501,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:654,deployability,version,versions,654,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:789,deployability,releas,release,789,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:63,energy efficiency,current,currently,63,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:623,integrability,state,statement,623,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:654,integrability,version,versions,654,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:654,modifiability,version,versions,654,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:15,safety,updat,update,15,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:687,safety,test,testing,687,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:15,security,updat,update,15,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:687,testability,test,testing,687,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:752,testability,simpl,simplifying,752,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:752,usability,simpl,simplifying,752,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:. ```. # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from. # source. # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085. if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then. echo ""Installing numpy with -no-binary=:all:. This will take a bit longer."". pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}"". else. pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}"". fi. ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:447,availability,sli,slightly,447,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:124,deployability,instal,installing,124,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:390,deployability,version,versions,390,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:465,deployability,version,version,465,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:711,deployability,build,build-prereq,711,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:95,energy efficiency,current,current,95,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:390,integrability,version,versions,390,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:465,integrability,version,version,465,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:390,modifiability,version,versions,390,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:465,modifiability,version,version,465,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:447,reliability,sli,slightly,447,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:456,security,modif,modified,456,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:611,security,modif,modifying,611,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:189,testability,understand,understand,189,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:343,testability,plan,planning,343,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:361,usability,support,support,361,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/395:51,energy efficiency,model,model,51,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:416,energy efficiency,model,model-case-study,416,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:61,modifiability,Pac,PacBio,61,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:96,modifiability,pac,package,96,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:156,modifiability,maintain,maintained,156,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:265,modifiability,Pac,PacBio,265,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:409,modifiability,pac,pacbio-model-case-study,409,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:156,safety,maintain,maintained,156,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:51,security,model,model,51,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:195,security,team,team,195,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:416,security,model,model-case-study,416,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:335,usability,help,helpful,335,"Hi @bitezhu . No, we don't recommend using the WGS model for PacBio data. The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. If you have trouble running DipAsm because of this, it might be worth asking them about it directly. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:66,energy efficiency,model,model,66,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:443,energy efficiency,model,model-case-study,443,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:76,modifiability,Pac,PacBio,76,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:117,modifiability,pac,package,117,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:177,modifiability,maintain,maintained,177,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:292,modifiability,Pac,PacBio,292,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:436,modifiability,pac,pacbio-model-case-study,436,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:177,safety,maintain,maintained,177,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:66,security,model,model,66,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:216,security,team,team,216,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:443,security,model,model-case-study,443,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:362,usability,help,helpful,362,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:746,usability,help,helpful,746,"> . > . > Hi @bitezhu. > . > No, we don't recommend using the WGS model for PacBio data. > . > The conda-deepvariant package is a community contribution by @chapmanb and is not maintained directly by the DeepVariant team -- we are not experts on Conda, unfortunately. > . > William Rowell of PacBio contributed a case study that uses Conda, so you may find this helpful: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md. > . > Otherwise we recommend using Docker or Singularity (or one of the external solutions) to run DeepVariant. > . > If you have trouble running DipAsm because of this, it might be worth asking them about it directly. > . > Maria. Hi, Maria, thanks for your quick reply, it's very helpful!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/396:5,availability,error,error,5,"This error might be caused by missing base quality scores in the BAM file, were you expecting this? DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:285,availability,reliab,reliable,285,"This error might be caused by missing base quality scores in the BAM file, were you expecting this? DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:5,performance,error,error,5,"This error might be caused by missing base quality scores in the BAM file, were you expecting this? DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:112,reliability,doe,does,112,"This error might be caused by missing base quality scores in the BAM file, were you expecting this? DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:285,reliability,reliab,reliable,285,"This error might be caused by missing base quality scores in the BAM file, were you expecting this? DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:5,safety,error,error,5,"This error might be caused by missing base quality scores in the BAM file, were you expecting this? DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:125,safety,valid,valid,125,"This error might be caused by missing base quality scores in the BAM file, were you expecting this? DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:5,usability,error,error,5,"This error might be caused by missing base quality scores in the BAM file, were you expecting this? DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:7,availability,error,error,7,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this? > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:289,availability,reliab,reliable,289,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this? > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:7,performance,error,error,7,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this? > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:116,reliability,doe,does,116,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this? > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:289,reliability,reliab,reliable,289,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this? > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:7,safety,error,error,7,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this? > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:129,safety,valid,valid,129,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this? > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:7,usability,error,error,7,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this? > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/398:31,testability,understand,understand,31,"Hi @aderzelle . If I correctly understand the question, this would be effectively equivalent to the setting EMIT_ALL_SITES options in GATK. Across an entire genome, emitting the data at all sites takes quite a bit longer and generates a gVCF file which is much larger. . It is likely more compact and faster to get coverage information from a method like mosdepth. . The option to emit all sites could make sense, but probably only for gene panels where the size of the output would not be unmanageable. Please let me know if I mis-understand your suggestions. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:315,testability,coverag,coverage,315,"Hi @aderzelle . If I correctly understand the question, this would be effectively equivalent to the setting EMIT_ALL_SITES options in GATK. Across an entire genome, emitting the data at all sites takes quite a bit longer and generates a gVCF file which is much larger. . It is likely more compact and faster to get coverage information from a method like mosdepth. . The option to emit all sites could make sense, but probably only for gene panels where the size of the output would not be unmanageable. Please let me know if I mis-understand your suggestions. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:532,testability,understand,understand,532,"Hi @aderzelle . If I correctly understand the question, this would be effectively equivalent to the setting EMIT_ALL_SITES options in GATK. Across an entire genome, emitting the data at all sites takes quite a bit longer and generates a gVCF file which is much larger. . It is likely more compact and faster to get coverage information from a method like mosdepth. . The option to emit all sites could make sense, but probably only for gene panels where the size of the output would not be unmanageable. Please let me know if I mis-understand your suggestions. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:70,usability,effectiv,effectively,70,"Hi @aderzelle . If I correctly understand the question, this would be effectively equivalent to the setting EMIT_ALL_SITES options in GATK. Across an entire genome, emitting the data at all sites takes quite a bit longer and generates a gVCF file which is much larger. . It is likely more compact and faster to get coverage information from a method like mosdepth. . The option to emit all sites could make sense, but probably only for gene panels where the size of the output would not be unmanageable. Please let me know if I mis-understand your suggestions. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:82,usability,close,close,82,Hello;. you perfectly understood my question and the answer makes sense. I do not close yet because there might be an additional reason the emit all sites option would have some interest (I am transferring the thread to a colleague). Thanks for your answer.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:232,security,triag,triage,232,"Hmm, I closed this but then noticed that @aderzelle said you're not closing on purpose. @aderzelle Is there anything else we can help here? You mentioned that you'll get a colleague to give more suggestions. Just to make it easy to triage for the next teammate on rotation, I'm going to close this for now. But please feel free to open again as needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:252,security,team,teammate,252,"Hmm, I closed this but then noticed that @aderzelle said you're not closing on purpose. @aderzelle Is there anything else we can help here? You mentioned that you'll get a colleague to give more suggestions. Just to make it easy to triage for the next teammate on rotation, I'm going to close this for now. But please feel free to open again as needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
